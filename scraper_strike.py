import time
import re
import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- è¨­å®šé …ç›® ---
MIN_REVENUE_THRESHOLD_REAL_ESTATE = 3  # ä¸å‹•ç”£æ¡ˆä»¶ã®å£²ä¸Šé«˜æœ€ä½ãƒ©ã‚¤ãƒ³ï¼ˆå˜ä½ï¼šå„„å††ï¼‰
MIN_REVENUE_THRESHOLD_OTHER = 5  # ä¸å‹•ç”£ä»¥å¤–ã®æ¡ˆä»¶ã®å£²ä¸Šé«˜æœ€ä½ãƒ©ã‚¤ãƒ³ï¼ˆå˜ä½ï¼šå„„å††ï¼‰
TARGET_URL = "https://www.strike.co.jp/smart/smart_search.html?issearch=on"
MAX_DEALS_TO_PROCESS = 100  # å‡¦ç†ã™ã‚‹æ¡ˆä»¶æ•°ã®ä¸Šé™
# ----------------

def parse_revenue(revenue_text, is_real_estate_deal=True):
    """ã€Œ3å„„å††ã€ã€Œ2å„„å††ï½5å„„å††ã€ç­‰ã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹"""
    if not revenue_text:
        return 0
    
    if "å„„å††" not in revenue_text:
        return 0
    
    if "ï½" in revenue_text:
        parts = revenue_text.split("ï½")
        if len(parts) == 2:
            upper_match = re.search(r'([\d\.]+)', parts[1])
            upper_value = float(upper_match.group(1)) if upper_match else 0
            if not is_real_estate_deal and upper_value == 5.0:
                return upper_value
            return upper_value
    
    match = re.search(r'([\d\.]+)', revenue_text)
    if match:
        return float(match.group(1))
    
    return 0

def is_real_estate_deal(title):
    return "ä¸å‹•ç”£" in title

def get_revenue_threshold(title):
    if is_real_estate_deal(title):
        return MIN_REVENUE_THRESHOLD_REAL_ESTATE
    else:
        return MIN_REVENUE_THRESHOLD_OTHER

def meets_revenue_criteria(revenue_text, title):
    is_real_estate = is_real_estate_deal(title)
    threshold = get_revenue_threshold(title)
    revenue_value = parse_revenue(revenue_text, is_real_estate)
    
    if is_real_estate:
        return revenue_value >= threshold
    else:
        return revenue_value > threshold

def extract_deal_info_from_strike(soup, max_deals):
    deals = []
    ss_elements = soup.find_all(string=re.compile(r'SS\d{6}'))
    
    for ss_element in ss_elements[:max_deals]:
        try:
            ss_number = ss_element.strip()
            parent = ss_element.parent
            if not parent:
                continue
            
            search_area = parent.parent if parent.parent else parent
            title = ""
            title_candidates = search_area.find_all(string=True)
            for i, candidate in enumerate(title_candidates):
                if ss_number in candidate:
                    for j in range(i + 1, min(i + 5, len(title_candidates))):
                        next_text = title_candidates[j].strip()
                        if next_text and not re.match(r'^[ï¼‹ï¼\s]*$', next_text) and 'æ‰€åœ¨åœ°' not in next_text and 'å£²ä¸Šé«˜' not in next_text:
                            title = next_text
                            break
                    break
            
            revenue_text = ""
            all_text = search_area.get_text()
            revenue_patterns = [
                r'å£²ä¸Šé«˜[^\d]*([^\n]+)',
                r'- å£²ä¸Šé«˜([^\n]+)',
                r'å£²ä¸Šé«˜ï¼š([^\n]+)'
            ]
            
            for pattern in revenue_patterns:
                revenue_match = re.search(pattern, all_text)
                if revenue_match:
                    revenue_text = revenue_match.group(1).strip()
                    break
            
            link = ""
            link_element = search_area.find('a', href=re.compile(r'sell_details\.html.*code=' + ss_number))
            if link_element:
                link = "https://www.strike.co.jp" + link_element.get('href')
            
            if title and revenue_text:
                deals.append({
                    'ss_number': ss_number,
                    'title': title,
                    'revenue': revenue_text,
                    'link': link
                })
        except Exception as e:
            print(f"    [è­¦å‘Š] {ss_number} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    return deals

def main():
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    deals_found = []
    print(f"ğŸ¯ ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ï¼ˆSMARTï¼‰ã®æ¡ˆä»¶ã‚’è§£æä¸­...ï¼ˆä¸Šé™: {MAX_DEALS_TO_PROCESS}ä»¶ï¼‰")
    try:
        driver.get(TARGET_URL)
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        all_deals = extract_deal_info_from_strike(soup, MAX_DEALS_TO_PROCESS)
        for i, deal in enumerate(all_deals, 1):
            if meets_revenue_criteria(deal['revenue'], deal['title']):
                deals_found.append(deal)
    finally:
        driver.quit()

    headers = ['æ¡ˆä»¶ç•ªå·', 'ã‚¿ã‚¤ãƒˆãƒ«', 'åˆ†é¡', 'å£²ä¸Šé«˜', 'ãƒªãƒ³ã‚¯']
    data_as_list = []
    for deal in deals_found:
        deal_type = "ä¸å‹•ç”£æ¡ˆä»¶" if is_real_estate_deal(deal['title']) else "ãã®ä»–æ¡ˆä»¶"
        data_as_list.append([
            deal.get('ss_number', ''),
            deal.get('title', ''),
            deal_type,
            deal.get('revenue', ''),
            deal.get('link', '')
        ])
    return {"headers": headers, "data": data_as_list}

if __name__ == "__main__":
    results = main()
    if results:
        print("\n" + "="*60)
        print(f"ğŸ‰ è§£æå®Œäº†ï¼æ¡ä»¶ã«åˆè‡´ã™ã‚‹ {len(results['data'])} ä»¶ã®æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        print("="*60)
        for deal in results['data']:
            print(f"æ¡ˆä»¶ç•ªå·: {deal[0]}")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {deal[1]}")
            print(f"åˆ†é¡: {deal[2]}")
            print(f"å£²ä¸Šé«˜: {deal[3]}")
            print(f"ãƒªãƒ³ã‚¯: {deal[4]}")
            print("-" * 40)