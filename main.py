# main29.py (å®Œå…¨ç‰ˆ)
import httpx
from bs4 import BeautifulSoup, Tag
import gspread
from google.oauth2.service_account import Credentials
import datetime
import hashlib
import traceback
import logging
import yaml
import time
import os
import re
import random
from functools import wraps
from typing import Optional, Dict, List, Set, Any
from dataclasses import dataclass, fields
from enum import Enum

# Seleniumé–¢é€£
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
CONFIG: Dict[str, Any] = {}

# --- å®šæ•°ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ ---
class Constants:
    FIELD_EXTRACTION_TIME = "extraction_time"
    FIELD_SITE_NAME = "site_name"
    FIELD_DEAL_ID = "deal_id"
    FIELD_TITLE = "title"
    FIELD_LOCATION = "location"
    FIELD_REVENUE = "revenue"
    FIELD_PROFIT = "profit"
    FIELD_PRICE = "price"
    FIELD_FEATURES = "features"
    FIELD_LINK = "link"
    FIELD_UNIQUE_ID = "unique_id"
    
    # çµ±ä¸€ã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
    JAPANESE_TO_ENGLISH_FIELDS: Dict[str, str] = {
        "æ¡ˆä»¶ID": FIELD_DEAL_ID,
        "æ¡ˆä»¶ç•ªå·": FIELD_DEAL_ID,
        "æ¡ˆä»¶No": FIELD_DEAL_ID,
        "ã‚¿ã‚¤ãƒˆãƒ«": FIELD_TITLE,
        "æ‰€åœ¨åœ°": FIELD_LOCATION,
        "æ‰€åœ¨åœ°åŸŸ": FIELD_LOCATION,
        "ã‚¨ãƒªã‚¢": FIELD_LOCATION,
        "åœ°åŸŸ": FIELD_LOCATION,
        "æ¥­ç¨®": FIELD_FEATURES,
        "æ¥­ç•Œ": FIELD_FEATURES,
        "å£²ä¸Šé«˜": FIELD_REVENUE,
        "æ¦‚ç®—å£²ä¸Š": FIELD_REVENUE,
        "å–¶æ¥­åˆ©ç›Š": FIELD_PROFIT,
        "å¸Œæœ›é‡‘é¡": FIELD_PRICE,
        "è­²æ¸¡å¸Œæœ›ä¾¡æ ¼": FIELD_PRICE,
        "ç‰¹è‰²": FIELD_FEATURES,
        "äº‹æ¥­å†…å®¹": FIELD_FEATURES,
        "äº‹æ¥­æ¦‚è¦": FIELD_FEATURES,
        "æ¦‚è¦": FIELD_FEATURES,
        "ç‰¹å¾´ãƒ»å¼·ã¿": FIELD_FEATURES,
        "ãƒªãƒ³ã‚¯": FIELD_LINK,
    }

class ScrapingStatus(Enum):
    SUCCESS = "success"
    FAILED = "failed"
    BLOCKED = "blocked"

@dataclass
class RawDealData:
    site_name: str
    title: str
    deal_id: str
    link: str
    revenue_text: str = ""
    profit_text: str = ""
    location_text: str = ""
    price_text: str = ""
    features_text: str = ""

@dataclass
class FormattedDealData:
    extraction_time: str
    site_name: str
    deal_id: str
    title: str
    features: str
    location: str
    revenue: str
    profit: str
    price: str
    link: str
    unique_id: str

# --- å°‚é–€å®¶ã‚¯ãƒ©ã‚¹ ---
class DataConverter:
    @staticmethod
    def parse_financial_value(text: str) -> int:
        """è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã«å¤‰æ›"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "é»’å­—ãªã—", "æç›Šãªã—"]):
            return 0
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')).replace(',', '')
        target_text = re.split(r'[ã€œï½-]', text)[-1]
        match = re.search(r'([\d\.]+)', target_text)
        if not match:
            return 0
        try:
            value = float(match.group(1))
        except ValueError:
            return 0
        multipliers = {'å„„': 100_000_000, 'åƒä¸‡': 10_000_000, 'ç™¾ä¸‡': 1_000_000, 'ä¸‡': 10_000}
        for unit, multiplier in multipliers.items():
            value *= multiplier
            break
        return int(value)
    
    @staticmethod
    def format_financial_text(text: str) -> str:
        """è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’ç™¾ä¸‡å††å˜ä½ã«çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "é»’å­—ãªã—", "æç›Šãªã—"]):
            return text or "N/A"
        
        def _to_million_format(text_part: str) -> str:
            clean_text = text_part.replace(',', '').strip()
            match = re.search(r'([\d\.]+)', clean_text)
            if not match:
                return text_part
            try:
                value = float(match.group(1))
            except ValueError:
                return text_part
            
            million_value = 0
            if 'å„„' in text_part:
                million_value = value * 100
            elif 'åƒä¸‡' in text_part:
                million_value = value * 10
            elif 'ç™¾ä¸‡å††' in text_part:
                return text_part
            elif 'ä¸‡' in text_part and 'ç™¾ä¸‡' not in text_part:
                million_value = value / 100
            else:
                return text_part
            
            if million_value >= 1:
                return f"{int(million_value):,}ç™¾ä¸‡å††" if million_value == int(million_value) else f"{million_value:.1f}ç™¾ä¸‡å††"
            else:
                return text_part
        
        separator = "ï½" if "ï½" in text else "ã€œ"
        if separator in text:
            parts = text.split(separator)
            if len(parts) == 2:
                return "ï½".join([_to_million_format(p.strip()) for p in parts])
        
        return _to_million_format(text)

    @staticmethod
    def convert_strike_revenue_to_million(revenue_text: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’ç™¾ä¸‡å††å˜ä½ã«å¤‰æ›"""
        if not revenue_text:
            return "-"
        
        conversion_map = {
            "5ï½10å„„å††": "500ï½1,000ç™¾ä¸‡å††",
            "10ï½50å„„å††": "1,000ï½5,000ç™¾ä¸‡å††", 
            "50ï½100å„„å††": "5,000ï½10,000ç™¾ä¸‡å††",
            "100å„„å††è¶…": "10,000ç™¾ä¸‡å††è¶…"
        }
        
        return conversion_map.get(revenue_text, revenue_text)

class AntiBlockingManager:
    """403ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.blocked_detected = False
        self.retry_count = 0
        self.max_retries = 1
        
    def get_human_like_delay(self, base_min: int = 3, base_max: int = 8) -> float:
        """äººé–“ã‚‰ã—ã„ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ã‚’ç”Ÿæˆ"""
        return random.uniform(base_min, base_max)
    
    def get_recovery_delay(self) -> float:
        """ãƒ–ãƒ­ãƒƒã‚¯å¾Œã®å›å¾©å¾…æ©Ÿæ™‚é–“ã‚’ç”Ÿæˆ"""
        return random.uniform(15, 30)
    
    def is_blocked_response(self, html_content: str) -> bool:
        """403ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if not html_content:
            return False
            
        blocked_indicators = [
            "403 ERROR",
            "The request could not be satisfied",
            "Request blocked",
            "cloudfront",
            "Access Denied",
            "Forbidden"
        ]
        
        # HTMLã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦æ¤œç´¢
        content_lower = html_content.lower()
        
        for indicator in blocked_indicators:
            if indicator.lower() in content_lower:
                return True
        
        # titleã‚¿ã‚°ã®ç¢ºèª
        soup = BeautifulSoup(html_content, 'lxml')
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().lower()
            if 'error' in title_text or 'blocked' in title_text or 'denied' in title_text:
                return True
        
        return False
    
    def get_human_like_headers(self, referer_url: str = None) -> Dict[str, str]:
        """äººé–“ã‚‰ã—ã„HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        }
        
        if referer_url:
            headers['Referer'] = referer_url
        
        return headers

class WebDriverManager:
    """WebDriverã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    def __init__(self, headless: bool = True, anti_blocking: AntiBlockingManager = None):
        self.headless = headless
        self.driver: Optional[webdriver.Chrome] = None
        self.anti_blocking = anti_blocking or AntiBlockingManager()

    def __enter__(self) -> webdriver.Chrome:
        logging.info("Initializing Selenium WebDriver with anti-blocking measures...")
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        # ã‚ˆã‚Šäººé–“ã‚‰ã—ã„ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920x1080")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agentã‚’è¨­å®š
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"--user-agent={user_agent}")
        
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        
        # WebDriverã®è‡ªå‹•åŒ–æ¤œå‡ºã‚’å›é¿
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        logging.info("âœ… WebDriver initialized successfully with anti-blocking measures.")
        return self.driver

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()
            logging.info("WebDriver has been closed.")

class DetailPageScraper:
    """è©³ç´°ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å°‚é–€ã«è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    def __init__(self, driver: webdriver.Chrome, anti_blocking: AntiBlockingManager):
        self.driver = driver
        self.anti_blocking = anti_blocking

    def _format_features_text(self, raw_text_block: str) -> str:
        """ç‰¹è‰²ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢å‡¦ç†"""
        text_with_header_breaks = re.sub(r'(?<!^)(ã€[^ã€ã€‘]+ã€‘)', r'\n\1', raw_text_block)
        
        inline_markers_pattern = r'(?<!^)([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ])'
        if re.search(inline_markers_pattern, text_with_header_breaks):
            text_with_all_breaks = re.sub(inline_markers_pattern, r'\n\1', text_with_header_breaks)
            lines = text_with_all_breaks.splitlines()
        else:
            lines = text_with_header_breaks.splitlines()
        
        final_lines = []
        for line in lines:
            stripped_line = line.strip()
            if stripped_line:
                cleaned_line = re.sub(r'^([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ã€€â˜†â˜…â€»â–²â–½])[\sã€€]+', r'\1', stripped_line)
                final_lines.append(cleaned_line)
        
        return "\n".join(final_lines)

    def fetch_features_with_blocking_protection(self, detail_url: str, selectors: Dict[str, Any], referer_url: str = None) -> str:
        """403ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ä»˜ãã®æ±ç”¨çš„ãªç‰¹è‰²æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
        if not detail_url or detail_url == 'N/A':
            return "-"
        
        try:
            logging.info(f"    -> Accessing detail page: {detail_url}")
            
            # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
            delay = self.anti_blocking.get_human_like_delay()
            logging.info(f"    -> Waiting {delay:.1f} seconds before access...")
            time.sleep(delay)
            
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            self.driver.get(detail_url)
            time.sleep(2.5)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            html_content = self.driver.page_source
            
            # 403ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
            if self.anti_blocking.is_blocked_response(html_content):
                logging.warning(f"    -> ğŸš« 403 BLOCK DETECTED for URL: {detail_url}")
                
                if not self.anti_blocking.blocked_detected:
                    self.anti_blocking.blocked_detected = True
                    logging.warning("    -> First block detected. Attempting recovery...")
                    
                    # å›å¾©å¾…æ©Ÿæ™‚é–“
                    recovery_delay = self.anti_blocking.get_recovery_delay()
                    logging.info(f"    -> Recovery wait: {recovery_delay:.1f} seconds...")
                    time.sleep(recovery_delay)
                    
                    # ãƒªãƒˆãƒ©ã‚¤
                    logging.info("    -> Retrying access...")
                    self.driver.get(detail_url)
                    time.sleep(3)
                    
                    retry_html = self.driver.page_source
                    
                    if self.anti_blocking.is_blocked_response(retry_html):
                        logging.error("    -> âŒ Still blocked after retry. Skipping this deal.")
                        return "-"
                    else:
                        logging.info("    -> âœ… Recovery successful!")
                        html_content = retry_html
                        self.anti_blocking.blocked_detected = False
                else:
                    logging.error("    -> âŒ Already in blocked state. Skipping this deal.")
                    return "-"
            
            detail_soup = BeautifulSoup(html_content, 'lxml')
            
            # M&Aç·åˆç ”ç©¶æ‰€ã®ç‰¹åˆ¥å‡¦ç†
            if 'masouken.com' in detail_url:
                return self._fetch_masouken_features(detail_soup)
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
            if 'strike.co.jp' in detail_url:
                return self._fetch_strike_features_enhanced(detail_soup, detail_url)
            
            # æ¨™æº–çš„ãªç‰¹è‰²æŠ½å‡ºå‡¦ç†
            return self._fetch_standard_features(detail_soup, selectors)
            
        except WebDriverException as e:
            logging.error(f"    -> WebDriver error on detail page: {detail_url} - {e}")
            return "-"
        except Exception as e:
            logging.error(f"    -> Error parsing detail page: {e}")
            logging.debug(traceback.format_exc())
            return "-"

    def fetch_features(self, detail_url: str, selectors: Dict[str, Any]) -> str:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®å¾“æ¥ãƒ¡ã‚½ãƒƒãƒ‰"""
        return self.fetch_features_with_blocking_protection(detail_url, selectors)

    def _fetch_strike_features_enhanced(self, detail_soup: BeautifulSoup, detail_url: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã®ç‰¹è‰²æŠ½å‡ºï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            deal_id = detail_url.split('code=')[-1] if 'code=' in detail_url else 'unknown'
            debug_file = f"debug_strike_detail_{deal_id}_{timestamp}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(str(detail_soup))
            logging.info(f"Debug: Strike detail HTML saved to {debug_file}")
        
        features_sections = []
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªul.detail__listã‚’æ¢ã™
        detail_list = detail_soup.find('ul', class_='detail__list')
        if detail_list:
            logging.info("    -> Found ul.detail__list using standard approach")
            
            # äº‹æ¥­æ¦‚è¦ã®æŠ½å‡º
            business_overview = self._extract_strike_list_item_enhanced(detail_list, 'äº‹æ¥­æ¦‚è¦')
            if business_overview:
                features_sections.append(f"ã€äº‹æ¥­æ¦‚è¦ã€‘\n{business_overview}")
                logging.info(f"    -> Found business overview: {business_overview[:50]}...")
            
            # ç‰¹å¾´ãƒ»å¼·ã¿ã®æŠ½å‡º
            strengths = self._extract_strike_list_item_enhanced(detail_list, 'ç‰¹å¾´ãƒ»å¼·ã¿')
            if strengths:
                features_sections.append(f"ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘\n{strengths}")
                logging.info(f"    -> Found strengths: {strengths[:50]}...")
        else:
            logging.warning("    -> ul.detail__list not found, trying alternative approaches")
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
            features_sections = self._extract_strike_features_from_text(detail_soup)
        
        if features_sections:
            result = "\n\n".join(features_sections)
            logging.info(f"    -> Successfully extracted features ({len(result)} chars)")
            return result
        else:
            logging.warning("    -> No features found with any approach")
            return "-"

    def _extract_strike_list_item_enhanced(self, detail_list: Tag, label_text: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ãƒªã‚¹ãƒˆé …ç›®ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
        li_items = detail_list.find_all('li')
        
        for li in li_items:
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: span.labelã‚’æ¢ã™
            label_span = li.find('span', class_='label')
            if label_span and label_text in label_span.get_text(strip=True):
                
                # æ–¹æ³•1: find_next_siblingã§spanã‚¿ã‚°ã‚’ç›´æ¥æ¢ã™
                value_span = label_span.find_next_sibling('span')
                if value_span:
                    for br in value_span.find_all('br'):
                        br.replace_with('\n')
                    value_text = value_span.get_text(strip=True)
                    if value_text and len(value_text) > 2:
                        logging.info(f"    -> Found via next_sibling span: {value_text[:30]}...")
                        return value_text
                
                # æ–¹æ³•2: liè¦ç´ å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                li_text = li.get_text(separator=' ', strip=True)
                label_full_text = label_span.get_text(strip=True)
                if label_full_text in li_text:
                    remaining_text = li_text.replace(label_full_text, '', 1).strip()
                    if remaining_text and len(remaining_text) > 2:
                        logging.info(f"    -> Found via text extraction: {remaining_text[:30]}...")
                        return remaining_text
                
                # æ–¹æ³•3: è¤‡æ•°ã®å…„å¼Ÿè¦ç´ ã‚’é †æ¬¡ãƒã‚§ãƒƒã‚¯
                current_sibling = label_span.next_sibling
                for attempt in range(5):
                    if current_sibling is None:
                        break
                    
                    if isinstance(current_sibling, str):
                        text_content = current_sibling.strip()
                        if text_content and len(text_content) > 2:
                            logging.info(f"    -> Found via sibling text: {text_content[:30]}...")
                            return text_content
                    elif hasattr(current_sibling, 'get_text'):
                        tag_content = current_sibling.get_text(strip=True)
                        if tag_content and len(tag_content) > 2:
                            logging.info(f"    -> Found via sibling tag: {tag_content[:30]}...")
                            return tag_content
                    
                    current_sibling = current_sibling.next_sibling
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒ©ãƒ™ãƒ«ãŒç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            li_text = li.get_text(strip=True)
            if label_text in li_text:
                # ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®å¾Œã®éƒ¨åˆ†ã‚’æŠ½å‡º
                parts = li_text.split(label_text, 1)
                if len(parts) == 2:
                    remaining = parts[1].strip()
                    # å…ˆé ­ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’é™¤å»
                    remaining = re.sub(r'^[ï¼š:\sã€€]+', '', remaining)
                    if remaining and len(remaining) > 2:
                        logging.info(f"    -> Found via text split: {remaining[:30]}...")
                        return remaining
        
        logging.warning(f"    -> No content found for label: {label_text}")
        return ""

    def _extract_strike_features_from_text(self, detail_soup: BeautifulSoup) -> List[str]:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹è‰²ã‚’ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        features_sections = []
        full_text = detail_soup.get_text()
        
        # äº‹æ¥­æ¦‚è¦ã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        business_patterns = [
            r'äº‹æ¥­æ¦‚è¦[ï¼š:\s]*([^\n]{20,200})',
            r'ã€äº‹æ¥­æ¦‚è¦ã€‘([^ã€]{20,200})',
            r'äº‹æ¥­å†…å®¹[ï¼š:\s]*([^\n]{20,200})',
        ]
        
        for pattern in business_patterns:
            matches = re.findall(pattern, full_text, re.DOTALL)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 20:
                    features_sections.append(f"ã€äº‹æ¥­æ¦‚è¦ã€‘\n{clean_match}")
                    break
            if features_sections:
                break
        
        # ç‰¹å¾´ãƒ»å¼·ã¿ã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        strength_patterns = [
            r'ç‰¹å¾´ãƒ»å¼·ã¿[ï¼š:\s]*([^\n]{20,200})',
            r'ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘([^ã€]{20,200})',
            r'å¼·ã¿[ï¼š:\s]*([^\n]{20,200})',
        ]
        
        for pattern in strength_patterns:
            matches = re.findall(pattern, full_text, re.DOTALL)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 20:
                    features_sections.append(f"ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘\n{clean_match}")
                    break
            if len(features_sections) == 2:  # æ—¢ã«äº‹æ¥­æ¦‚è¦ã‚‚ã‚ã‚‹å ´åˆ
                break
        
        return features_sections

    def _fetch_masouken_features(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€å°‚ç”¨ã®äº‹æ¥­è©³ç´°ã¨å¼·ã¿æŠ½å‡º"""
        features_sections = []
        
        business_details = self._extract_masouken_business_details(detail_soup)
        if business_details:
            features_sections.append(f"ã€äº‹æ¥­è©³ç´°ã€‘\n{business_details}")
        
        strengths = self._extract_masouken_strengths(detail_soup)
        if strengths:
            features_sections.append(f"ã€å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã€‘\n{strengths}")
        
        if features_sections:
            return "\n\n".join(features_sections)
        else:
            return "-"

    def _extract_masouken_business_details(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®äº‹æ¥­è©³ç´°æŠ½å‡º"""
        business_keywords = ['äº‹æ¥­è©³ç´°', 'äº‹æ¥­å†…å®¹', 'äº‹æ¥­æ¦‚è¦', 'æ¦‚è¦', 'ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«']
        
        for keyword in business_keywords:
            dt_elements = detail_soup.find_all('dt', string=re.compile(keyword))
            for dt in dt_elements:
                dd = dt.find_next_sibling('dd')
                if dd:
                    content = dd.get_text(strip=True)
                    if len(content) > 20:
                        return self._format_masouken_text(content)
        
        for keyword in business_keywords:
            headers = detail_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], 
                                         string=re.compile(keyword))
            
            for header in headers:
                content_elements = []
                current = header
                
                for _ in range(10):
                    current = current.find_next_sibling()
                    if not current or not isinstance(current, Tag):
                        break
                    
                    if current.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and current.get_text().strip():
                        break
                    
                    text = current.get_text(strip=True)
                    if len(text) > 15:
                        content_elements.append(current)
                
                if content_elements:
                    result = self._format_masouken_elements(content_elements)
                    if result and len(result) > 20:
                        return result
        
        return ""

    def _extract_masouken_strengths(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º"""
        strength_keywords = ['å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ', 'å¼·ã¿', 'å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ', 'ç‰¹å¾´', 'ç«¶åˆå„ªä½æ€§', 'å„ªä½æ€§']
        
        for keyword in strength_keywords:
            dt_elements = detail_soup.find_all('dt', string=re.compile(keyword))
            for dt in dt_elements:
                dd = dt.find_next_sibling('dd')
                if dd:
                    content = dd.get_text(strip=True)
                    if len(content) > 20:
                        return self._format_masouken_text(content)
        
        for keyword in strength_keywords:
            headers = detail_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], 
                                         string=re.compile(keyword))
            
            for header in headers:
                content_elements = []
                current = header
                
                for _ in range(10):
                    current = current.find_next_sibling()
                    if not current or not isinstance(current, Tag):
                        break
                    
                    if current.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and current.get_text().strip():
                        break
                    
                    text = current.get_text(strip=True)
                    if len(text) > 15:
                        content_elements.append(current)
                
                if content_elements:
                    result = self._format_masouken_elements(content_elements)
                    if result and len(result) > 20:
                        return result
        
        return ""

    def _fetch_standard_features(self, detail_soup: BeautifulSoup, selectors: Dict[str, Any]) -> str:
        """æ¨™æº–çš„ãªç‰¹è‰²æŠ½å‡ºå‡¦ç†"""
        selector_config = selectors.get('features')
        if not selector_config:
            return "-"
        
        start_keywords = selector_config.get('start_keywords', [])
        end_tag = selector_config.get('end_tag')
        target_tags = selector_config.get('target_tags', [])
        
        start_element = None
        for keyword in start_keywords:
            found = detail_soup.find(['h1', 'h2', 'h3', 'h4', 'h5', 'dt'], string=re.compile(keyword))
            if found:
                start_element = found
                break
        
        if not start_element:
            return "-"
        
        collected_elements = []
        for sibling in start_element.find_next_siblings():
            if not isinstance(sibling, Tag):
                continue
            if sibling.name == end_tag:
                break
            if sibling.name in target_tags:
                collected_elements.append(sibling)
            else:
                child_tags = sibling.find_all(target_tags)
                if child_tags:
                    collected_elements.extend(child_tags)
        
        if not collected_elements:
            return "-"
        
        raw_text_block = "\n".join([elem.get_text(strip=True) for elem in collected_elements if elem.get_text(strip=True)])
        return self._format_features_text(raw_text_block) if raw_text_block else "-"

    def _format_masouken_elements(self, elements: List[Tag]) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®è¦ç´ ã‚’æ•´å½¢"""
        formatted_items = []
        
        for element in elements:
            text = element.get_text(strip=True)
            
            if element.name in ['ul', 'ol']:
                li_items = element.find_all('li')
                for li in li_items:
                    item_text = li.get_text(strip=True)
                    if len(item_text) > 10:
                        formatted_items.append(f"ãƒ»{item_text}")
            elif any(marker in text for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']):
                for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']:
                    if marker in text:
                        parts = text.split(marker)
                        for part in parts[1:]:
                            part = part.strip()
                            if len(part) > 10:
                                formatted_items.append(f"ãƒ»{part}")
                        break
            elif len(text) > 15:
                sentences = re.split(r'[ã€‚ï¼]', text)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 15:
                        if not sentence.endswith('ã€‚'):
                            sentence += 'ã€‚'
                        formatted_items.append(f"ãƒ»{sentence}")
        
        return "\n".join(formatted_items[:5])

    def _format_masouken_text(self, text: str) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢"""
        if not text or len(text) < 20:
            return ""
        
        cleaned_text = text.strip()
        
        if len(cleaned_text) > 500:
            sentences = re.split(r'[ã€‚ï¼]', cleaned_text)
            truncated_sentences = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) > 500:
                    break
                truncated_sentences.append(sentence.strip())
                current_length += len(sentence)
            
            cleaned_text = 'ã€‚'.join([s for s in truncated_sentences if s])
            if cleaned_text and not cleaned_text.endswith('ã€‚'):
                cleaned_text += 'ã€‚'
        
        if 'ãƒ»' in cleaned_text or 'â—†' in cleaned_text:
            return cleaned_text
        else:
            sentences = re.split(r'[ã€‚ï¼]', cleaned_text)
            bullet_points = []
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 10:
                    bullet_points.append(f"ãƒ»{sentence}ã€‚")
            
            return "\n".join(bullet_points[:3])

class UniversalParser:
    """çµ±ä¸€ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def parse_list_page(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """æ±ç”¨çš„ãªä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚µãƒ¼"""
        site_name = site_config['name']
        parser_type = site_config.get('parser_type', 'standard')
        
        # M&Aç·åˆç ”ç©¶æ‰€ã®ç‰¹åˆ¥å‡¦ç†
        if parser_type == 'text_based' or site_name == "M&Aç·åˆç ”ç©¶æ‰€":
            return UniversalParser._parse_masouken_text_based(site_config, html_content)
        
        # M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®ç‰¹åˆ¥å‡¦ç†
        if site_name == "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º":
            return UniversalParser._parse_ma_capital_partners(site_config, html_content)
        
        # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹åˆ¥å‡¦ç†
        if site_name == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
            return UniversalParser._parse_strike(site_config, html_content)
        
        # æ¨™æº–çš„ãªHTMLã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å‡¦ç†
        return UniversalParser._parse_selector_based(site_config, html_content)
    
    @staticmethod
    def _parse_strike(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆå‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = f"debug_strike_{timestamp}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º
        items = soup.select('div.search-result__item')
        
        if not items:
            logging.info("    -> No items found with selector")
            return []
        
        logging.info(f"    -> Found {len(items)} Strike items")
        
        # ä¸Šã‹ã‚‰52ä»¶ã‚’å¯¾è±¡ï¼ˆ14è¡ŒÃ—3æ¡ˆä»¶/è¡Œï¼‰
        items_to_process = items[:52]
        
        for i, item in enumerate(items_to_process):
            try:
                logging.info(f"    -> Processing item {i+1}/{len(items_to_process)}")
                
                # æ¡ˆä»¶ç•ªå·ã®æŠ½å‡º
                deal_id = UniversalParser._extract_strike_deal_id(item)
                if not deal_id:
                    logging.info(f"    -> No deal ID found in item {i+1}, skipping")
                    continue
                
                logging.info(f"    -> Found deal ID: {deal_id}")
                
                # å£²ä¸Šé«˜ã®æŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                revenue_text = UniversalParser._extract_strike_revenue(item)
                
                # å£²ä¸Šé«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šæŒ‡å®šã•ã‚ŒãŸ4ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿è©³ç´°ãƒšãƒ¼ã‚¸ã«é€²ã‚€
                valid_revenues = ["5ï½10å„„å††", "10ï½50å„„å††", "50ï½100å„„å††", "100å„„å††è¶…"]
                if revenue_text not in valid_revenues:
                    logging.info(f"    -> Skipping deal {deal_id}: Revenue '{revenue_text}' doesn't meet criteria")
                    continue
                
                logging.info(f"    -> Revenue meets criteria: {revenue_text}")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä»®ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                title = f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal_id}"
                
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’æ§‹ç¯‰
                link = f"https://www.strike.co.jp/smart/sell_details.html?code={deal_id}"
                
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆè©³ç´°æƒ…å ±ã¯è©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—ï¼‰
                deal_data = RawDealData(
                    site_name=site_config['name'],
                    deal_id=deal_id,
                    title=title,
                    link=link,
                    location_text="",  # è©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—
                    revenue_text=revenue_text,
                    profit_text="-",  # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã¯å¸¸ã«"-"
                    price_text="-",   # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã¯å¸¸ã«"-"
                    features_text=""  # è©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—
                )
                
                results.append(deal_data)
                logging.info(f"    -> Successfully extracted deal: {deal_id}")
                
            except Exception as e:
                logging.error(f"    -> Error parsing ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ item {i+1}: {e}")
                logging.debug(traceback.format_exc())
                continue
        
        logging.info(f"    -> ã‚¹ãƒˆãƒ©ã‚¤ã‚¯: Successfully extracted {len(results)} deals meeting criteria")
        return results

    @staticmethod
    def _extract_strike_deal_id(item: Tag) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®æ¡ˆä»¶IDã‚’æŠ½å‡º"""
        # p.search-result__numã‹ã‚‰æŠ½å‡º
        num_element = item.select_one('p.search-result__num')
        if num_element:
            text = num_element.get_text(strip=True)
            # SSç•ªå·ã‚’æŠ½å‡º
            ss_match = re.search(r'(SS\d+)', text)
            if ss_match:
                return ss_match.group(1)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰SSç•ªå·ã‚’æ¤œç´¢
        item_text = item.get_text()
        ss_match = re.search(r'(SS\d+)', item_text)
        if ss_match:
            return ss_match.group(1)
        
        return ""

    @staticmethod
    def _extract_strike_revenue(item: Tag) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’æŠ½å‡º"""
        # span.sales-amountã‹ã‚‰æŠ½å‡º
        sales_element = item.select_one('span.sales-amount')
        if sales_element:
            return sales_element.get_text(strip=True)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰å£²ä¸Šé«˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        item_text = item.get_text()
        revenue_patterns = [
            r'5ï½10å„„å††', r'5ã€œ10å„„å††',
            r'10ï½50å„„å††', r'10ã€œ50å„„å††',
            r'50ï½100å„„å††', r'50ã€œ100å„„å††',
            r'100å„„å††è¶…'
        ]
        
        for pattern in revenue_patterns:
            if re.search(pattern, item_text):
                # æ­£è¦åŒ–ã—ã¦è¿”ã™
                normalized = pattern.replace('ã€œ', 'ï½')
                return normalized
        
        return ""
    
    @staticmethod
    def _parse_ma_capital_partners(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºå°‚ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = f"debug_ma_capital_{timestamp}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # æ¡ˆä»¶ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
        items = soup.select('article.c-filter-project')
        
        if not items:
            logging.info("    -> No items found with standard selector")
            return []
        
        logging.info(f"    -> Found {len(items)} items")
        
        for i, item in enumerate(items):
            try:
                logging.info(f"    -> Processing item {i+1}/{len(items)}")
                
                # æ¡ˆä»¶ç•ªå·ã®æŠ½å‡º
                deal_no_element = item.select_one('.c-filter-project__no')
                if not deal_no_element:
                    logging.info(f"    -> No deal number found in item {i+1}, skipping")
                    continue
                
                deal_no_text = deal_no_element.get_text(strip=True)
                deal_match = re.search(r'æ¡ˆä»¶No[ï¼š:\s]*([A-Z0-9-]+)', deal_no_text)
                if not deal_match:
                    logging.info(f"    -> Could not extract deal ID from: {deal_no_text}")
                    continue
                
                deal_id = deal_match.group(1)
                logging.info(f"    -> Found deal ID: {deal_id}")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
                title_element = item.select_one('.c-filter-project__ttl')
                title = title_element.get_text(strip=True) if title_element else f"M&Aæ¡ˆä»¶_{deal_id}"
                logging.info(f"    -> Found title: {title[:50]}...")
                
                # ãƒªãƒ³ã‚¯ã®æŠ½å‡º
                link_element = item.select_one('a.c-cta.c-button--arrow')
                if link_element:
                    href = link_element.get('href', '')
                    link = href if href.startswith('http') else f"https://www.ma-cp.com{href}"
                else:
                    link = f"https://www.ma-cp.com/deal/{deal_id}/"
                
                # è²¡å‹™æƒ…å ±ã®æŠ½å‡º
                revenue_text = UniversalParser._extract_ma_capital_dl_data(item, 'æ¦‚ç®—å£²ä¸Š')
                profit_text = UniversalParser._extract_ma_capital_dl_data(item, 'å–¶æ¥­åˆ©ç›Š')
                location_text = UniversalParser._extract_ma_capital_dl_data(item, 'æ‰€åœ¨åœ°')
                price_text = UniversalParser._extract_ma_capital_dl_data(item, 'å¸Œæœ›é‡‘é¡')
                
                logging.info(f"    -> Revenue: {revenue_text}, Profit: {profit_text}")
                
                # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                revenue_value = DataConverter.parse_financial_value(revenue_text)
                profit_value = DataConverter.parse_financial_value(profit_text)
                
                min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                
                if revenue_value < min_revenue or profit_value < min_profit:
                    logging.info(f"    -> Skipping deal {deal_id}: doesn't meet financial criteria")
                    continue
                
                # äº‹æ¥­å†…å®¹ã®æŠ½å‡ºï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
                features_text = UniversalParser._extract_ma_capital_business_content(item)
                
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                deal_data = RawDealData(
                    site_name=site_config['name'],
                    deal_id=deal_id,
                    title=title,
                    link=link,
                    location_text=location_text,
                    revenue_text=revenue_text,
                    profit_text=profit_text,
                    price_text=price_text,
                    features_text=features_text
                )
                
                results.append(deal_data)
                logging.info(f"    -> Successfully extracted deal: {deal_id} - {title[:50]}")
                
            except Exception as e:
                logging.error(f"    -> Error parsing M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º item {i+1}: {e}")
                logging.debug(traceback.format_exc())
                continue
        
        logging.info(f"    -> M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º: Successfully extracted {len(results)} deals meeting criteria")
        return results
    
    @staticmethod
    def _extract_ma_capital_dl_data(item: Tag, field_name: str) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®dlè¦ç´ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        dl_elements = item.select('dl.c-filter-project__dataList')
        for dl in dl_elements:
            dt = dl.select_one('dt.c-filter-project__dataList__ttl')
            dd = dl.select_one('dd.c-filter-project__dataList__data')
            
            if dt and dd:
                dt_text = dt.get_text(strip=True)
                if dt_text == field_name:
                    return dd.get_text(strip=True)
        
        return ""
    
    @staticmethod
    def _extract_ma_capital_business_content(item: Tag) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®äº‹æ¥­å†…å®¹ã‚’æŠ½å‡ºï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        features_sections = []
        
        # äº‹æ¥­å†…å®¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        business_blocks = item.select('div.c-filter-project__listBlock')
        
        for block in business_blocks:
            # ãƒ©ãƒ™ãƒ«ã‚’ç¢ºèª
            label_element = block.select_one('h5.c-filter-project__label')
            if not label_element:
                continue
            
            label_text = label_element.get_text(strip=True)
            
            # äº‹æ¥­å†…å®¹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’å‡¦ç†ï¼ˆæ¥­ç¨®ã¯é™¤å¤–ï¼‰
            if label_text == 'äº‹æ¥­å†…å®¹':
                lists_element = block.select_one('div.c-filter-project__lists')
                if lists_element:
                    content = lists_element.get_text(strip=True)
                    
                    # HTMLã®<br>ã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                    html_content = str(lists_element)
                    content_with_breaks = re.sub(r'<br\s*/?>', '\n', html_content)
                    clean_soup = BeautifulSoup(content_with_breaks, 'html.parser')
                    formatted_content = clean_soup.get_text(strip=True)
                    
                    if formatted_content:
                        # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ç®‡æ¡æ›¸ãå½¢å¼ã«æ•´å½¢
                        lines = [line.strip() for line in formatted_content.split('\n') if line.strip()]
                        
                        if len(lines) == 1 and not lines[0].startswith('ãƒ»'):
                            # å˜ä¸€è¡Œã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                            features_sections.append(f"ã€{label_text}ã€‘\n{lines[0]}")
                        else:
                            # è¤‡æ•°è¡Œã®å ´åˆã¯ç®‡æ¡æ›¸ãã¨ã—ã¦æ•´å½¢
                            formatted_lines = []
                            for line in lines:
                                if not line.startswith('ãƒ»'):
                                    line = f"ãƒ»{line}"
                                formatted_lines.append(line)
                            
                            if formatted_lines:
                                features_sections.append(f"ã€{label_text}ã€‘\n" + '\n'.join(formatted_lines))
        
        return '\n\n'.join(features_sections) if features_sections else ""
    
    @staticmethod
    def _parse_selector_based(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¨™æº–ãƒ‘ãƒ¼ã‚µãƒ¼"""
        soup = BeautifulSoup(html_content, 'lxml')
        items = soup.select(site_config.get('item_selector', ''))
        results = []
        
        for item in items:
            data = {'site_name': site_config['name']}
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            for jp_key, selector in site_config.get('data_selectors', {}).items():
                en_key = Constants.JAPANESE_TO_ENGLISH_FIELDS.get(jp_key)
                if not en_key:
                    continue
                
                element = item.select_one(selector)
                text_content = element.get_text(strip=True) if element else ""
                
                if en_key == Constants.FIELD_LINK and element:
                    href = element.get('href', '')
                    base_url = '/'.join(site_config['url'].split('/')[:3])
                    data[en_key] = href if href.startswith('http') else f"{base_url}{href}"
                elif en_key in [Constants.FIELD_REVENUE, Constants.FIELD_PROFIT, Constants.FIELD_LOCATION, Constants.FIELD_PRICE, Constants.FIELD_FEATURES]:
                    data[f"{en_key}_text"] = text_content
                else:
                    data[en_key] = text_content
            
            # è¿½åŠ ã®DLè¦ç´ å‡¦ç†ï¼ˆM&Aãƒ­ã‚¤ãƒ¤ãƒ«ç”¨ï¼‰
            if site_config['name'] == "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼":
                UniversalParser._extract_dl_elements(item, data)
                # ç‰¹è‰²ã®è©³ç´°æŠ½å‡º
                enhanced_features = UniversalParser._extract_enhanced_features(item)
                if enhanced_features and enhanced_features != "-":
                    data['features_text'] = enhanced_features
            
            if data.get('deal_id') and data.get('title') and data.get('link'):
                results.append(RawDealData(**{k: v for k, v in data.items() if k in {f.name for f in fields(RawDealData)}}))
        
        return results
    
    @staticmethod
    def _parse_masouken_text_based(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aç·åˆç ”ç©¶æ‰€å°‚ç”¨ã®æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ¼ã‚µãƒ¼"""
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = f"debug_masouken_{timestamp}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ä¸€è¦§ãƒšãƒ¼ã‚¸ã®æ¡ˆä»¶ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
        deal_items = soup.find_all('li', class_='p-projects-index__item')
        
        if deal_items:
            logging.info(f"    -> Found {len(deal_items)} deal items using HTML selector")
            
            for item in deal_items:
                try:
                    # æ¡ˆä»¶IDã‚’æŠ½å‡º
                    id_element = item.find('span', class_='c-projects-feed__number')
                    if not id_element:
                        continue
                    
                    id_text = id_element.get_text(strip=True)
                    id_match = re.search(r'æ¡ˆä»¶ID[ï¼š:\s]*(\d+)', id_text)
                    if not id_match:
                        continue
                    
                    deal_id = id_match.group(1)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    title_element = item.find('h2')
                    title = title_element.get_text(strip=True) if title_element else f"M&Aæ¡ˆä»¶_{deal_id}"
                    
                    # å£²ä¸Šé«˜ã¨å–¶æ¥­åˆ©ç›Šã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    revenue_text = ""
                    profit_text = ""
                    
                    detail_elements = item.find_all('li', class_='c-projects-feed__detail')
                    for detail in detail_elements:
                        heading = detail.find('p', class_='heading')
                        if heading:
                            heading_text = heading.get_text(strip=True)
                            value_p = heading.find_next_sibling('p')
                            if value_p:
                                value_text = value_p.get_text(strip=True)
                                
                                if heading_text == 'å£²ä¸Šé«˜':
                                    revenue_text = value_text
                                elif heading_text == 'å–¶æ¥­åˆ©ç›Š':
                                    profit_text = value_text
                    
                    # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    revenue_value = DataConverter.parse_financial_value(revenue_text)
                    profit_value = DataConverter.parse_financial_value(profit_text)
                    
                    min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                    min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                    
                    if revenue_value < min_revenue or profit_value < min_profit:
                        logging.info(f"    -> Skipping deal {deal_id}: Revenue={revenue_value:,}, Profit={profit_value:,}")
                        continue
                    
                    # ãã®ä»–ã®æƒ…å ±ã‚’æŠ½å‡º
                    location_text = ""
                    price_text = ""
                    
                    for detail in detail_elements:
                        heading = detail.find('p', class_='heading')
                        if heading:
                            heading_text = heading.get_text(strip=True)
                            value_p = heading.find_next_sibling('p')
                            if value_p:
                                value_text = value_p.get_text(strip=True)
                                
                                if heading_text == 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼':
                                    price_text = value_text
                    
                    # åœ°åŸŸæƒ…å ±ã‚’æŠ½å‡º
                    info_element = item.find('div', class_='c-projects-feed__info')
                    if info_element:
                        info_text = info_element.get_text(strip=True)
                        # "æ¥­ç¨®ï¼åœ°åŸŸ"ã®å½¢å¼ã‹ã‚‰åœ°åŸŸéƒ¨åˆ†ã‚’æŠ½å‡º
                        if 'ï¼' in info_text:
                            location_text = info_text.split('ï¼')[-1]
                    
                    # ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
                    link = f"https://masouken.com/list/{deal_id}"
                    
                    # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                    deal_data = RawDealData(
                        site_name=site_config['name'],
                        deal_id=deal_id,
                        title=title,
                        link=link,
                        location_text=location_text,
                        revenue_text=revenue_text,
                        profit_text=profit_text,
                        price_text=price_text,
                        features_text=""  # è©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—
                    )
                    
                    results.append(deal_data)
                    logging.info(f"    -> Successfully extracted deal: {deal_id} - {title[:50]}")
                    
                except Exception as e:
                    logging.error(f"    -> Error parsing deal item: {e}")
                    continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹æŠ½å‡º
        if not results:
            logging.info("    -> Trying text-based extraction as fallback")
            results = UniversalParser._parse_masouken_text_fallback(site_config, html_content)
        
        logging.info(f"    -> M&Aç·åˆç ”ç©¶æ‰€: Successfully extracted {len(results)} deals meeting criteria")
        return results
    
    @staticmethod
    def _parse_masouken_text_fallback(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        results = []
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¡ˆä»¶IDãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        content_text = soup.get_text()
        logging.info(f"    -> Total content length: {len(content_text)} characters")
        
        # æ¡ˆä»¶IDãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        deal_id_pattern = r'æ¡ˆä»¶ID[ï¼š:\s]*(\d+)'
        deal_matches = list(re.finditer(deal_id_pattern, content_text))
        logging.info(f"    -> Found {len(deal_matches)} deal ID matches")
        
        if deal_matches:
            for i, match in enumerate(deal_matches):
                try:
                    deal_id = match.group(1)
                    start_pos = match.start()
                    
                    # æ¬¡ã®æ¡ˆä»¶IDã¾ã§ã®ç¯„å›²ã‚’å–å¾—
                    if i + 1 < len(deal_matches):
                        end_pos = deal_matches[i + 1].start()
                        content = content_text[start_pos:end_pos]
                    else:
                        content = content_text[start_pos:start_pos + 2000]  # æœ€å¾Œã®æ¡ˆä»¶ã¯2000æ–‡å­—ã¾ã§
                    
                    logging.info(f"    -> Processing deal ID: {deal_id}, content length: {len(content)}")
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã®æ”¹å–„
                    title = UniversalParser._extract_masouken_title(content, deal_id)
                    
                    # æ‰€åœ¨åœ°æŠ½å‡ºã®æ”¹å–„
                    location_text = UniversalParser._extract_masouken_location(content)
                    
                    # è²¡å‹™æƒ…å ±æŠ½å‡º
                    revenue_text = UniversalParser._extract_financial_info(content, 'å£²ä¸Šé«˜')
                    profit_text = UniversalParser._extract_financial_info(content, 'å–¶æ¥­åˆ©ç›Š')
                    price_text = UniversalParser._extract_financial_info(content, 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼')
                    
                    # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    revenue_value = DataConverter.parse_financial_value(revenue_text)
                    profit_value = DataConverter.parse_financial_value(profit_text)
                    
                    min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                    min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                    
                    if revenue_value < min_revenue or profit_value < min_profit:
                        logging.info(f"    -> Skipping deal {deal_id}: doesn't meet financial criteria")
                        continue
                    
                    # ãƒªãƒ³ã‚¯ç”Ÿæˆ
                    link = f"https://masouken.com/list/{deal_id}"
                    
                    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
                    if deal_id and title:
                        deal_data = RawDealData(
                            site_name=site_config['name'],
                            deal_id=deal_id,
                            title=title,
                            link=link,
                            location_text=location_text,
                            revenue_text=revenue_text,
                            profit_text=profit_text,
                            price_text=price_text,
                            features_text=""  # è©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—
                        )
                        results.append(deal_data)
                        logging.info(f"    -> Successfully extracted deal: {deal_id} - {title[:50]}")
                    else:
                        logging.debug(f"    -> Skipped incomplete deal: ID={deal_id}, Title={title[:30] if title else 'None'}")
                        
                except Exception as e:
                    logging.error(f"    -> Error parsing deal {i + 1}: {e}")
                    continue
        
        return results
    
    @staticmethod
    def _extract_masouken_title(content: str, deal_id: str) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚’æ”¹å–„"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€ã€‘ã§å›²ã¾ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        bracket_patterns = [
            r'ã€([^ã€‘]{5,100})ã€‘',  # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'æ¡ˆä»¶ID[ï¼š:\s]*' + deal_id + r'[^\n]*\n[^\n]*ã€([^ã€‘]{5,100})ã€‘',  # æ¡ˆä»¶IDå¾Œã®ã€ã€‘
        ]
        
        exclude_keywords = [
            'æ¡ˆä»¶ID', 'å£²ä¸Šé«˜', 'å–¶æ¥­åˆ©ç›Š', 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'URL', 'http',
            'ç™¾ä¸‡å††', 'ä¸‡å††', 'æ‰€åœ¨åœ°', 'ã‚¨ãƒªã‚¢', 'åœ°åŸŸ', 'é¸æŠã—ã¦ãã ã•ã„'
        ]
        
        business_keywords = [
            'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½é€ ', 'è²©å£²', 'å·¥äº‹', 'å»ºè¨­', 'æ¥­', 'ä¼šç¤¾', 'ä¼æ¥­',
            'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'åº—èˆ—', 'åºƒå‘Š', 'IT', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³',
            'åŒ–ç²§å“', 'å¡—è£…', 'è¨­è¨ˆ', 'é–‹ç™º', 'é‹å–¶', 'ç®¡ç†', 'å•†ç¤¾', 'è²¿æ˜“', 'è¼¸å…¥',
            'è¼¸å‡º', 'å¸å£²', 'å°å£²', 'åŒ»ç™‚', 'ä»‹è­·', 'æ•™è‚²', 'å­¦ç¿’', 'å¡¾', 'ã‚¹ã‚¯ãƒ¼ãƒ«',
            'ä¸å‹•ç”£', 'å»ºç¯‰', 'åœŸæœ¨', 'é›»æ°—', 'æ©Ÿæ¢°', 'è‡ªå‹•è»Š', 'éƒ¨å“', 'ææ–™',
            'ã‚³ãƒ³ã‚µãƒ«', 'Web', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆ', 'é€šè²©', 'EC', 'é…é€', 'ç‰©æµ'
        ]
        
        for pattern in bracket_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                title_candidate = match.strip()
                
                # é•·ã•ãƒã‚§ãƒƒã‚¯
                if not (5 <= len(title_candidate) <= 100):
                    continue
                
                # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if any(keyword in title_candidate for keyword in exclude_keywords):
                    continue
                
                # æ•°å­—ã®ã¿ã¯é™¤å¤–
                if title_candidate.isdigit():
                    continue
                
                # äº‹æ¥­é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if any(keyword in title_candidate for keyword in business_keywords):
                    logging.info(f"Found title for deal {deal_id}: {title_candidate}")
                    return f"ã€{title_candidate}ã€‘"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ±ç”¨çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
        logging.warning(f"Could not extract proper title for deal {deal_id}, using fallback")
        return f"M&Aæ¡ˆä»¶_{deal_id}"
    
    @staticmethod
    def _extract_masouken_location(content: str) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®æ‰€åœ¨åœ°æŠ½å‡ºã‚’æ”¹å–„"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç›´æ¥çš„ãªæ‰€åœ¨åœ°è¡¨è¨˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¼·åŒ–ï¼‰
        direct_patterns = [
            r'æ‰€åœ¨åœ°[ï¼š:\s]*([^ï¼š:\n]{2,20})',
            r'ã‚¨ãƒªã‚¢[ï¼š:\s]*([^ï¼š:\n]{2,20})',
            r'åœ°åŸŸ[ï¼š:\s]*([^ï¼š:\n]{2,20})',
            r'ãƒ»æ‰€åœ¨åœ°[ï¼š:\s]*([^ï¼š:\n]{2,20})',
            r'ãƒ»ã‚¨ãƒªã‚¢[ï¼š:\s]*([^ï¼š:\n]{2,20})'
        ]
        
        for pattern in direct_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                location_candidate = match.strip()
                
                # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
                location_candidate = re.sub(r'^[ï¼š:\sãƒ»â—†â– â–¼]+', '', location_candidate)
                location_candidate = re.sub(r'[ï¼š:\sãƒ»â—†â– â–¼]+$', '', location_candidate)
                
                # ã€Œé¸æŠã—ã¦ãã ã•ã„ã€ãªã©ã®ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
                unwanted_phrases = [
                    'é¸æŠã—ã¦ãã ã•ã„', 'select', 'option', 'value',
                    'æ¡ˆä»¶ID', 'å£²ä¸Š', 'åˆ©ç›Š', 'ä¾¡æ ¼', 'äº‹æ¥­', 'ä¼šç¤¾', 'æ ªå¼'
                ]
                
                # ä¸è¦ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å»ã‚’è©¦è¡Œ
                for phrase in unwanted_phrases:
                    if phrase in location_candidate:
                        location_candidate = location_candidate.replace(phrase, '')
                        location_candidate = re.sub(r'^[^\w]+', '', location_candidate)
                        location_candidate = re.sub(r'[^\w]+$', '', location_candidate)
                
                # é•·ã•ãƒã‚§ãƒƒã‚¯
                if not (2 <= len(location_candidate) <= 15):
                    continue
                
                # æœ‰åŠ¹ãªåœ°åŸŸåã‹ãƒã‚§ãƒƒã‚¯
                if UniversalParser._is_valid_location(location_candidate):
                    logging.info(f"Found location (direct): {location_candidate}")
                    return location_candidate
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: éƒ½é“åºœçœŒåã®ç›´æ¥æ¤œç´¢
        prefecture_pattern = r'(åŒ—æµ·é“|é’æ£®çœŒ?|å²©æ‰‹çœŒ?|å®®åŸçœŒ?|ç§‹ç”°çœŒ?|å±±å½¢çœŒ?|ç¦å³¶çœŒ?|èŒ¨åŸçœŒ?|æ ƒæœ¨çœŒ?|ç¾¤é¦¬çœŒ?|åŸ¼ç‰çœŒ?|åƒè‘‰çœŒ?|æ±äº¬éƒ½?|ç¥å¥ˆå·çœŒ?|æ–°æ½ŸçœŒ?|å¯Œå±±çœŒ?|çŸ³å·çœŒ?|ç¦äº•çœŒ?|å±±æ¢¨çœŒ?|é•·é‡çœŒ?|å²é˜œçœŒ?|é™å²¡çœŒ?|æ„›çŸ¥çœŒ?|ä¸‰é‡çœŒ?|æ»‹è³€çœŒ?|äº¬éƒ½åºœ?|å¤§é˜ªåºœ?|å…µåº«çœŒ?|å¥ˆè‰¯çœŒ?|å’Œæ­Œå±±çœŒ?|é³¥å–çœŒ?|å³¶æ ¹çœŒ?|å²¡å±±çœŒ?|åºƒå³¶çœŒ?|å±±å£çœŒ?|å¾³å³¶çœŒ?|é¦™å·çœŒ?|æ„›åª›çœŒ?|é«˜çŸ¥çœŒ?|ç¦å²¡çœŒ?|ä½è³€çœŒ?|é•·å´çœŒ?|ç†Šæœ¬çœŒ?|å¤§åˆ†çœŒ?|å®®å´çœŒ?|é¹¿å…å³¶çœŒ?|æ²–ç¸„çœŒ?)'
        
        prefecture_matches = re.findall(prefecture_pattern, content)
        if prefecture_matches:
            result = prefecture_matches[0]
            logging.info(f"Found location (prefecture): {result}")
            return result
        
        logging.info("No location found")
        return ""

    @staticmethod
    def _is_valid_location(location_text: str) -> bool:
        """åœ°åŸŸåã¨ã—ã¦æœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
        
        # éƒ½é“åºœçœŒåãƒªã‚¹ãƒˆ
        prefectures = [
            'åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 'å±±å½¢', 'ç¦å³¶',
            'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·',
            'æ–°æ½Ÿ', 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'é•·é‡', 'å²é˜œ',
            'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 'å¤§é˜ª', 'å…µåº«',
            'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£',
            'å¾³å³¶', 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 'é•·å´',
            'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 'æ²–ç¸„'
        ]
        
        # åœ°æ–¹åãƒªã‚¹ãƒˆ
        regions = [
            'åŒ—æµ·é“', 'æ±åŒ—', 'é–¢æ±', 'ç”²ä¿¡è¶Š', 'ä¸­éƒ¨', 'åŒ—é™¸',
            'è¿‘ç•¿', 'é–¢è¥¿', 'ä¸­å›½', 'å››å›½', 'ä¹å·', 'æ²–ç¸„',
            'é¦–éƒ½åœ', 'é–¢æ±åœ', 'é–¢è¥¿åœ', 'ä¸­äº¬åœ'
        ]
        
        # æµ·å¤–åœ°åŸŸ
        international = [
            'æµ·å¤–', 'å›½å†…', 'å…¨å›½', 'ã‚¢ã‚¸ã‚¢', 'æ±å—ã‚¢ã‚¸ã‚¢',
            'ã‚¿ã‚¤', 'ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«', 'ãƒ™ãƒˆãƒŠãƒ ', 'ãƒãƒ¬ãƒ¼ã‚·ã‚¢', 'ä¸­å›½', 'éŸ“å›½'
        ]
        
        # ã„ãšã‚Œã‹ã®ãƒªã‚¹ãƒˆã«éƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        all_locations = prefectures + regions + international
        
        for valid_location in all_locations:
            if valid_location in location_text:
                return True
        
        # ã€ŒçœŒã€ã€Œåºœã€ã€Œéƒ½ã€ã€Œé“ã€ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°æœ‰åŠ¹ã¨ã¿ãªã™
        if any(suffix in location_text for suffix in ['çœŒ', 'åºœ', 'éƒ½', 'é“']):
            return True
        
        return False

    @staticmethod
    def _extract_dl_elements(item: Tag, data: Dict[str, str]) -> None:
        """DLè¦ç´ ã‹ã‚‰ã®è©³ç´°æƒ…å ±æŠ½å‡º"""
        dl_tags = item.select("dl.p-case__dl")
        for dl in dl_tags:
            dts = dl.find_all("dt")
            dds = dl.find_all("dd")
            for dt, dd in zip(dts, dds):
                jp_key = dt.get_text(strip=True)
                en_key = Constants.JAPANESE_TO_ENGLISH_FIELDS.get(jp_key)
                if en_key and en_key in [Constants.FIELD_REVENUE, Constants.FIELD_PROFIT, Constants.FIELD_LOCATION, Constants.FIELD_PRICE]:
                    data[f"{en_key}_text"] = dd.get_text(strip=True)
    
    @staticmethod
    def _extract_enhanced_features(item_element: Tag) -> str:
        """M&Aãƒ­ã‚¤ãƒ¤ãƒ«ç”¨ã®æ‹¡å¼µç‰¹è‰²æŠ½å‡º"""
        try:
            all_text = item_element.get_text()
            
            # ç‰¹å¾´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
            feature_patterns = [
                r'ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘(.+?)(?=ã€|â– |â—†|$)',
                r'ã€ç‰¹è‰²ã€‘(.+?)(?=ã€|â– |â—†|$)',
                r'ã€äº‹æ¥­å†…å®¹ã€‘(.+?)(?=ã€|â– |â—†|$)',
            ]
            
            for pattern in feature_patterns:
                match = re.search(pattern, all_text, re.DOTALL)
                if match and len(match.group(1).strip()) > 20:
                    feature_content = match.group(1).strip()
                    
                    # ç®‡æ¡æ›¸ããƒãƒ¼ã‚«ãƒ¼ã§æŠ½å‡º
                    bullet_patterns = [
                        (r'âœ“([^âœ“\n]+)', 'âœ“'),
                        (r'ãƒ»([^ãƒ»\n]+)', 'ãƒ»'),
                        (r'â—†([^â—†\n]+)', 'â—†'),
                    ]
                    
                    for bullet_pattern, marker in bullet_patterns:
                        matches = re.findall(bullet_pattern, feature_content)
                        if matches:
                            business_keywords = ['å–å¼•', 'å®Ÿç¸¾', 'æŠ€è¡“', 'å“è³ª', 'é¡§å®¢', 'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½å“', 'å¼·ã¿']
                            extracted_items = []
                            
                            for match in matches:
                                cleaned_item = match.strip()
                                if (len(cleaned_item) > 10 and 
                                    any(keyword in cleaned_item for keyword in business_keywords)):
                                    extracted_items.append(f"{marker}{cleaned_item.strip()}")
                                    if len(extracted_items) >= 5:
                                        break
                            
                            if extracted_items:
                                return "\n".join(extracted_items)
            
            return "-"
            
        except Exception as e:
            logging.error(f"    -> Error extracting enhanced features: {e}")
            return "-"
    
    @staticmethod
    def _extract_financial_info(content: str, field_pattern: str) -> str:
        """è²¡å‹™æƒ…å ±æŠ½å‡ºã®ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        patterns = [
            rf'{field_pattern}[ï¼š:\s]*([^\n]+)',
            rf'ãƒ»{field_pattern}[ï¼š:\s]*([^\n]+)',
            rf'{field_pattern}\s*([^\n]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                value = match.group(1).strip()
                # ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
                value = re.sub(r'^[ï¼š:\s]+', '', value)
                value = re.sub(r'[ãƒ»â—†â–¼]+$', '', value)
                # æ¬¡ã®é …ç›®åãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã“ã§åˆ‡ã‚‹
                next_items = ['å–¶æ¥­åˆ©ç›Š', 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'æ‰€åœ¨åœ°', 'æ¥­ç•Œ', 'æ¡ˆä»¶ID']
                for next_item in next_items:
                    if next_item in value and next_item != field_pattern:
                        value = value.split(next_item)[0].strip()
                        break
                if len(value) > 0 and value != field_pattern:
                    return value
        
        return ""

class GSheetConnector:
    """Google Sheetsæ¥ç¶šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    def __init__(self, config: Dict):
        self.config = config['google_sheets']
        self.worksheet = self._connect()

    def _connect(self):
        logging.info("Connecting to Google Sheets...")
        try:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
            if not creds_path:
                raise ValueError("GOOGLE_APPLICATION_CREDENTIALS environment variable is not set.")

            scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
            creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
            client = gspread.authorize(creds)
            spreadsheet = client.open_by_key(self.config['spreadsheet_id'])
            worksheet = spreadsheet.worksheet(self.config['sheet_name'])
            logging.info("âœ… Successfully connected to Google Sheets.")
            return worksheet
        except Exception as e:
            logging.critical(f"âŒ Google Sheets connection error: {e}")
            return None

    def get_existing_ids(self) -> Set[str]:
        """æ—¢å­˜ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’å–å¾—"""
        if not self.worksheet:
            return set()
        logging.info("Fetching existing deal IDs from the sheet...")
        try:
            all_data = self.worksheet.get_all_records()
            if all_data and Constants.FIELD_UNIQUE_ID in all_data[0]:
                ids = {row[Constants.FIELD_UNIQUE_ID] for row in all_data if row.get(Constants.FIELD_UNIQUE_ID)}
                logging.info(f"Found {len(ids)} existing IDs.")
                return ids
            logging.info("No existing data or 'unique_id' column found.")
            return set()
        except Exception as e:
            logging.error(f"Error fetching existing IDs: {e}")
            return set()

    def write_deals(self, new_deals: List[FormattedDealData]) -> None:
        """æ–°ã—ã„æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿"""
        if not self.worksheet or not new_deals:
            return
        logging.info(f"Writing {len(new_deals)} new deals to the spreadsheet...")
        try:
            all_values = self.worksheet.get_all_values()
            headers = [f.name for f in fields(FormattedDealData)]
            if not all_values:
                self.worksheet.append_row(headers, value_input_option='USER_ENTERED')
            existing_headers = self.worksheet.row_values(1) if all_values else headers
            rows_to_append = [[getattr(deal, key, '') for key in existing_headers] for deal in new_deals]
            if rows_to_append:
                self.worksheet.append_rows(rows_to_append, value_input_option='USER_ENTERED')
            logging.info(f"âœ… Successfully appended {len(new_deals)} rows.")
        except Exception as e:
            logging.error(f"Error writing to spreadsheet: {e}")

def load_config(file_path: str = 'config.yaml') -> None:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    global CONFIG
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if 'GOOGLE_SHEETS_ID' in os.environ:
            config['google_sheets']['spreadsheet_id'] = os.environ['GOOGLE_SHEETS_ID']
        CONFIG = config
    except Exception as e:
        print(f"âŒ Config file read error: {e}")
        raise

def setup_logging(config: Dict) -> None:
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_config = config.get('logging', {})
    logging.basicConfig(
        level=getattr(logging, log_config.get('level', 'INFO').upper()),
        format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
        handlers=[
            logging.FileHandler(log_config.get('file_name', 'scraping.log'), encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def retry_on_failure(max_retries_key: str = 'max_retries', delay_key: str = 'retry_delay'):
    """ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            scraping_config = CONFIG.get('scraping', {})
            max_retries = scraping_config.get(max_retries_key, 3)
            delay = scraping_config.get(delay_key, 1)
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except httpx.RequestError as e:
                    logging.warning(f"Network error. Retrying {attempt + 1}/{max_retries}: {e}")
                    if attempt == max_retries - 1:
                        logging.error("Max retries reached.")
                        raise
                    time.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator

@retry_on_failure()
def fetch_html(url: str) -> Optional[str]:
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—"""
    timeout = CONFIG.get('scraping', {}).get('timeout', 15)
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        with httpx.Client(timeout=timeout, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            response.raise_for_status()
            return response.text
    except httpx.TimeoutException as e:
        raise httpx.RequestError(f"Timeout occurred: {e}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code >= 500:
            raise httpx.RequestError(f"Server error {e.response.status_code}, retrying...")
        else:
            logging.error(f"HTTP client error (no retry): {e.response.status_code} for url {url}")
            return None
    except Exception as e:
        logging.error(f"Unexpected error fetching {url}: {e}")
        return None

def format_deal_data(raw_deals: List[RawDealData], existing_ids: Set[str]) -> List[FormattedDealData]:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã€æ¡ä»¶ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†"""
    formatted_deals = []
    extraction_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    for raw_deal in raw_deals:
        try:
            unique_id = hashlib.md5(f"{raw_deal.site_name}_{raw_deal.deal_id}".encode()).hexdigest()[:12]
            
            if unique_id in existing_ids:
                logging.info(f"    -> Skipping duplicate deal: {raw_deal.deal_id}")
                continue
            
            # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã¯ä¸€è¦§ãƒšãƒ¼ã‚¸ã§æ—¢ã«ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰
            if raw_deal.site_name not in ["M&Aç·åˆç ”ç©¶æ‰€", "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯"]:
                revenue_value = DataConverter.parse_financial_value(raw_deal.revenue_text)
                profit_value = DataConverter.parse_financial_value(raw_deal.profit_text)
                
                min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                
                if revenue_value < min_revenue or profit_value < min_profit:
                    logging.info(f"    -> Skipping deal {raw_deal.deal_id}: doesn't meet financial criteria")
                    continue
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’ç™¾ä¸‡å††å˜ä½ã«å¤‰æ›
            if raw_deal.site_name == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                revenue_formatted = DataConverter.convert_strike_revenue_to_million(raw_deal.revenue_text)
            else:
                revenue_formatted = DataConverter.format_financial_text(raw_deal.revenue_text)
            
            formatted_deal = FormattedDealData(
                extraction_time=extraction_time,
                site_name=raw_deal.site_name,
                deal_id=raw_deal.deal_id,
                title=raw_deal.title,
                location=raw_deal.location_text or "-",
                revenue=revenue_formatted,
                profit=DataConverter.format_financial_text(raw_deal.profit_text),
                price=DataConverter.format_financial_text(raw_deal.price_text),
                features=raw_deal.features_text or "-",
                link=raw_deal.link,
                unique_id=unique_id
            )
            
            formatted_deals.append(formatted_deal)
            logging.info(f"    -> Formatted deal: {raw_deal.deal_id} - {raw_deal.title[:50]}...")
            
        except Exception as e:
            logging.error(f"    -> Error formatting deal {raw_deal.deal_id}: {e}")
            continue
    
    return formatted_deals

def scrape_site(site_config: Dict[str, Any]) -> List[RawDealData]:
    """å„ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    if not site_config.get('enabled', False):
        logging.info(f"Site {site_config['name']} is disabled. Skipping.")
        return []
    
    logging.info(f"ğŸ” Starting scraping for: {site_config['name']}")
    all_deals = []
    
    try:
        max_pages = site_config.get('max_pages', 1)
        base_url = site_config['url']
        
        for page_num in range(1, max_pages + 1):
            if site_config.get('pagination', {}).get('type') == 'query_param':
                param = site_config['pagination']['param']
                url = f"{base_url}?{param}={page_num}"
            elif site_config.get('pagination', {}).get('type') == 'path':
                path_template = site_config['pagination']['path']
                url = f"{base_url}{path_template.format(page_num=page_num)}"
            else:
                url = base_url
            
            logging.info(f"  ğŸ“„ Scraping page {page_num}: {url}")
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã‚µã‚¤ãƒˆã®å‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œ
            if site_config['name'] == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                html_content = scrape_strike_with_dynamic_loading(url)
            else:
                html_content = fetch_html(url)
            
            if not html_content:
                logging.error(f"  âŒ Failed to fetch page {page_num}")
                continue
            
            # çµ±ä¸€ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨
            deals = UniversalParser.parse_list_page(site_config, html_content)
            
            logging.info(f"  âœ… Found {len(deals)} deals on page {page_num}")
            all_deals.extend(deals)
            
            time.sleep(2)
            
            if max_pages == 1:
                break
    
    except Exception as e:
        logging.error(f"âŒ Error scraping {site_config['name']}: {e}")
        logging.debug(traceback.format_exc())
    
    logging.info(f"ğŸ¯ Total deals found from {site_config['name']}: {len(all_deals)}")
    return all_deals

def scrape_strike_with_dynamic_loading(url: str) -> Optional[str]:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã®å‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    try:
        anti_blocking = AntiBlockingManager()
        with WebDriverManager(headless=CONFIG.get('debug', {}).get('headless_mode', True), anti_blocking=anti_blocking) as driver:
            logging.info(f"  ğŸš€ Loading Strike page with dynamic loading support: {url}")
            driver.get(url)
            
            # 52ä»¶ã®æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§30ç§’ï¼‰
            wait = WebDriverWait(driver, 30)
            
            logging.info("  â³ Waiting for 52 deal items to load...")
            
            # æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ãŒ52ä»¶ä»¥ä¸Šèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            try:
                wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, 'div.search-result__item')) >= 52)
                logging.info("  âœ… 52+ deal items loaded successfully")
            except TimeoutException:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆã§ã‚‚ã€ç¾åœ¨èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ä»¶æ•°ã‚’ãƒã‚§ãƒƒã‚¯
                current_items = driver.find_elements(By.CSS_SELECTOR, 'div.search-result__item')
                logging.warning(f"  âš ï¸ Timeout waiting for 52 items. Currently loaded: {len(current_items)} items")
                
                if len(current_items) < 20:  # æœ€ä½é™ã®ä»¶æ•°ã‚‚ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                    logging.error("  âŒ Too few items loaded, aborting")
                    return None
                else:
                    logging.info(f"  âœ… Proceeding with {len(current_items)} loaded items")
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆJavaScriptã®å®Œå…¨ãªå®Ÿè¡Œå®Œäº†ã‚’ç¢ºä¿ï¼‰
            time.sleep(3)
            
            # æœ€çµ‚çš„ãªæ¡ˆä»¶æ•°ã‚’ç¢ºèª
            final_items = driver.find_elements(By.CSS_SELECTOR, 'div.search-result__item')
            logging.info(f"  ğŸ“Š Final item count: {len(final_items)}")
            
            return driver.page_source
            
    except Exception as e:
        logging.error(f"  âŒ Error in Strike dynamic loading: {e}")
        logging.debug(traceback.format_exc())
        return None

def enhance_deals_with_details(raw_deals: List[RawDealData], site_config: Dict[str, Any]) -> List[RawDealData]:
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰¹è‰²æƒ…å ±ã‚’å–å¾—ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    
    # ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ååˆ†ãªæƒ…å ±ãŒå–å¾—ã§ãã‚‹ã‚µã‚¤ãƒˆã¯è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—
    skip_detail_sites = ["M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼", "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º"]
    
    if site_config['name'] in skip_detail_sites:
        logging.info(f"  Skipping detail page scraping for {site_config['name']} (using list page features)")
        return raw_deals
    
    if not site_config.get('detail_page_selectors') and site_config['name'] not in ["M&Aç·åˆç ”ç©¶æ‰€", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯"]:
        logging.info(f"  No detail page selectors configured for {site_config['name']}")
        return raw_deals
    
    logging.info(f"ğŸ”— Fetching details for {len(raw_deals)} deals from {site_config['name']}")
    enhanced_deals = []
    
    try:
        anti_blocking = AntiBlockingManager()
        with WebDriverManager(headless=CONFIG.get('debug', {}).get('headless_mode', True), anti_blocking=anti_blocking) as driver:
            scraper = DetailPageScraper(driver, anti_blocking)
            
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’ãƒªãƒ•ã‚¡ãƒ©ãƒ¼ã¨ã—ã¦è¨­å®š
            referer_url = site_config['url']
            
            for i, deal in enumerate(raw_deals, 1):
                try:
                    logging.info(f"  ğŸ“– Processing deal {i}/{len(raw_deals)}: {deal.deal_id}")
                    
                    # 403ãƒ–ãƒ­ãƒƒã‚¯ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å‡¦ç†ã‚’åœæ­¢
                    if anti_blocking.blocked_detected:
                        logging.warning(f"  ğŸš« Blocked state detected. Skipping remaining {len(raw_deals) - i + 1} deals.")
                        # æ®‹ã‚Šã®æ¡ˆä»¶ã‚‚ãã®ã¾ã¾è¿½åŠ ï¼ˆè©³ç´°æƒ…å ±ãªã—ï¼‰
                        enhanced_deals.extend(raw_deals[i-1:])
                        break
                    
                    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®è©³ç´°ãƒšãƒ¼ã‚¸ã§è¿½åŠ æƒ…å ±ã‚’å–å¾—
                    if site_config['name'] == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                        enhanced_deal = enhance_strike_deal_with_details_protected(deal, scraper, anti_blocking, referer_url)
                        enhanced_deals.append(enhanced_deal)
                    else:
                        # ä»–ã®ã‚µã‚¤ãƒˆã®å‡¦ç†ï¼ˆ403å¯¾ç­–ä»˜ãï¼‰
                        features = scraper.fetch_features_with_blocking_protection(
                            deal.link, 
                            site_config.get('detail_page_selectors', {}),
                            referer_url
                        )
                        
                        if features and features != "-":
                            if deal.features_text:
                                deal.features_text = f"{deal.features_text}\n{features}"
                            else:
                                deal.features_text = features
                        
                        enhanced_deals.append(deal)
                    
                    # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“ï¼ˆã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å ´åˆã¯ã‚ˆã‚Šæ…é‡ã«ï¼‰
                    if site_config['name'] == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                        delay = anti_blocking.get_human_like_delay(4, 10)  # 4-10ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
                    else:
                        delay = anti_blocking.get_human_like_delay(2, 5)   # 2-5ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
                    
                    logging.info(f"    -> Waiting {delay:.1f} seconds before next request...")
                    time.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ Error processing deal {deal.deal_id}: {e}")
                    enhanced_deals.append(deal)
                    continue
    
    except Exception as e:
        logging.error(f"âŒ Error initializing WebDriver: {e}")
        return raw_deals
    
    logging.info(f"âœ… Enhanced {len(enhanced_deals)} deals with detail information")
    return enhanced_deals

def enhance_strike_deal_with_details_protected(deal: RawDealData, scraper: DetailPageScraper, anti_blocking: AntiBlockingManager, referer_url: str) -> RawDealData:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    try:
        logging.info(f"    -> Accessing Strike detail page: {deal.link}")
        
        # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
        delay = anti_blocking.get_human_like_delay(3, 8)
        logging.info(f"    -> Human-like delay: {delay:.1f} seconds...")
        time.sleep(delay)
        
        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        scraper.driver.get(deal.link)
        time.sleep(3)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
        
        html_content = scraper.driver.page_source
        
        # 403ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
        if anti_blocking.is_blocked_response(html_content):
            logging.warning(f"    -> ğŸš« 403 BLOCK DETECTED for deal: {deal.deal_id}")
            
            if not anti_blocking.blocked_detected:
                anti_blocking.blocked_detected = True
                logging.warning("    -> First Strike block detected. Attempting recovery...")
                
                # å›å¾©å¾…æ©Ÿæ™‚é–“ï¼ˆã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã§ã‚ˆã‚Šé•·ãï¼‰
                recovery_delay = anti_blocking.get_recovery_delay()
                logging.info(f"    -> Recovery wait: {recovery_delay:.1f} seconds...")
                time.sleep(recovery_delay)
                
                # ãƒªãƒˆãƒ©ã‚¤
                logging.info("    -> Retrying Strike access...")
                scraper.driver.get(deal.link)
                time.sleep(5)  # ã‚ˆã‚Šé•·ã„èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                
                retry_html = scraper.driver.page_source
                
                if anti_blocking.is_blocked_response(retry_html):
                    logging.error("    -> âŒ Still blocked after retry. Aborting Strike detail scraping.")
                    anti_blocking.blocked_detected = True
                    return deal
                else:
                    logging.info("    -> âœ… Strike recovery successful!")
                    html_content = retry_html
                    anti_blocking.blocked_detected = False
            else:
                logging.error("    -> âŒ Already in blocked state. Skipping Strike deal.")
                return deal
        
        detail_soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—ï¼ˆè¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å®Œå…¨å¯¾å¿œï¼‰
        title = extract_strike_title_enhanced(detail_soup, deal.deal_id)
        if title and title != f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal.deal_id}":
            deal.title = title
        
        # æ‰€åœ¨åœ°ã®å–å¾—ï¼ˆåœ°æ–¹å˜ä½è¡¨è¨˜ï¼‰
        location = extract_strike_location_enhanced(detail_soup)
        if location:
            deal.location_text = location
        
        # ç‰¹è‰²ã®å–å¾—ï¼ˆäº‹æ¥­æ¦‚è¦ + ç‰¹å¾´ãƒ»å¼·ã¿ï¼‰
        features = scraper._fetch_strike_features_enhanced(detail_soup, deal.link)
        if features and features != "-":
            deal.features_text = features
        
        logging.info(f"    -> Enhanced Strike deal: {deal.deal_id} - {deal.title[:50]}")
        return deal
        
    except Exception as e:
        logging.error(f"    -> Error enhancing Strike deal {deal.deal_id}: {e}")
        return deal

def extract_strike_title_enhanced(detail_soup: BeautifulSoup, deal_id: str) -> str:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
    title_selectors = [
        'h2.section-ttl span',
        'h2.section-ttl',
        'h1.section-ttl span',
        'h1.section-ttl',
        '.section-ttl span',
        '.section-ttl'
    ]
    
    for selector in title_selectors:
        title_element = detail_soup.select_one(selector)
        if title_element:
            title_text = title_element.get_text(strip=True)
            if title_text and len(title_text) > 5 and title_text != deal_id:
                logging.info(f"    -> Found title via selector {selector}: {title_text[:30]}...")
                return title_text
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: h1, h2ã‚¿ã‚°ã®å…¨æ¢ç´¢
    for header_tag in ['h1', 'h2', 'h3']:
        headers = detail_soup.find_all(header_tag)
        for header in headers:
            header_text = header.get_text(strip=True)
            # æ¡ˆä»¶IDã‚„ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’é™¤å¤–
            exclude_keywords = [
                'æ¡ˆä»¶æ¤œç´¢', 'ãƒ­ã‚°ã‚¤ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒŠãƒ“', 'ãƒˆãƒƒãƒ—',
                'ä¼šå“¡ç™»éŒ²', 'è©³ç´°æ¤œç´¢', 'STRIKE', deal_id, 'SS', 'å£²ä¸Šé«˜'
            ]
            
            if (header_text and len(header_text) > 5 and len(header_text) < 100 and
                not any(keyword in header_text for keyword in exclude_keywords)):
                
                # äº‹æ¥­é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                business_keywords = [
                    'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½é€ ', 'è²©å£²', 'å·¥äº‹', 'å»ºè¨­', 'æ¥­', 'ä¼šç¤¾',
                    'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'åº—èˆ—', 'åºƒå‘Š', 'IT', 'ã‚·ã‚¹ãƒ†ãƒ ',
                    'åŒ–ç²§å“', 'å¡—è£…', 'è¨­è¨ˆ', 'é–‹ç™º', 'é‹å–¶', 'ç®¡ç†', 'å•†ç¤¾'
                ]
                
                if any(keyword in header_text for keyword in business_keywords):
                    logging.info(f"    -> Found title via header search: {header_text[:30]}...")
                    return header_text
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æŠ½å‡º
    full_text = detail_soup.get_text()
    
    # æ¡ˆä»¶IDå‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
    if deal_id in full_text:
        # æ¡ˆä»¶IDã®å‰å¾Œ100æ–‡å­—ã‚’å–å¾—
        deal_id_pos = full_text.find(deal_id)
        surrounding_text = full_text[max(0, deal_id_pos-100):deal_id_pos+200]
        
        # ã€ã€‘ã§å›²ã¾ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
        bracket_matches = re.findall(r'ã€([^ã€‘]{5,80})ã€‘', surrounding_text)
        for match in bracket_matches:
            if deal_id not in match and len(match) > 5:
                logging.info(f"    -> Found title via bracket search: {match[:30]}...")
                return f"ã€{match}ã€‘"
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒˆãƒ«
    logging.warning(f"    -> Could not extract title for {deal_id}, using default")
    return f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal_id}"

def extract_strike_location_enhanced(detail_soup: BeautifulSoup) -> str:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®æ‰€åœ¨åœ°æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
    
    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®åœ°æ–¹è¡¨è¨˜ãƒ‘ã‚¿ãƒ¼ãƒ³
    valid_locations = [
        "æ±åŒ—åœ°æ–¹", "é–¢æ±åœ°æ–¹", "ä¸­éƒ¨ãƒ»åŒ—é™¸åœ°æ–¹", "é–¢è¥¿åœ°æ–¹", 
        "ä¸­å›½åœ°æ–¹", "å››å›½åœ°æ–¹", "ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹", "æ±æ—¥æœ¬", "è¥¿æ—¥æœ¬", "æµ·å¤–"
    ]
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: ul.detail__listå†…ã®æ‰€åœ¨åœ°ãƒ©ãƒ™ãƒ«ã‚’æ¢ã™
    detail_lists = detail_soup.find_all('ul', class_='detail__list')
    
    for ul in detail_lists:
        items = ul.find_all('li')
        
        for item in items:
            # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æ‰€åœ¨åœ°æƒ…å ±ã‚’æŠ½å‡º
            text_content = item.get_text(strip=True)
            
            # ã€Œæ‰€åœ¨åœ°ã€ã§å§‹ã¾ã‚‹è¡Œã‚’æ¢ã™
            if 'æ‰€åœ¨åœ°' in text_content:
                # æœ‰åŠ¹ãªåœ°æ–¹è¡¨è¨˜ã‹ãƒã‚§ãƒƒã‚¯
                for valid_location in valid_locations:
                    if valid_location in text_content:
                        logging.info(f"    -> Found location via detail list: {valid_location}")
                        return valid_location
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰åœ°æ–¹è¡¨è¨˜ã‚’æ¢ã™
    full_text = detail_soup.get_text()
    
    for valid_location in valid_locations:
        if valid_location in full_text:
            # æ‰€åœ¨åœ°ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯
            location_pos = full_text.find(valid_location)
            surrounding = full_text[max(0, location_pos-50):location_pos+50]
            
            if 'æ‰€åœ¨åœ°' in surrounding or 'ã‚¨ãƒªã‚¢' in surrounding or 'åœ°åŸŸ' in surrounding:
                logging.info(f"    -> Found location via full text search: {valid_location}")
                return valid_location
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: dtã‚¿ã‚°ã‹ã‚‰æ‰€åœ¨åœ°ã‚’æ¢ã™
    dt_elements = detail_soup.find_all('dt')
    for dt in dt_elements:
        dt_text = dt.get_text(strip=True)
        if 'æ‰€åœ¨åœ°' in dt_text:
            dd = dt.find_next_sibling('dd')
            if dd:
                dd_text = dd.get_text(strip=True)
                for valid_location in valid_locations:
                    if valid_location in dd_text:
                        logging.info(f"    -> Found location via dt/dd: {valid_location}")
                        return valid_location
    
    logging.warning("    -> Could not extract location")
    return ""

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    try:
        load_config()
        setup_logging(CONFIG)
        
        logging.info("ğŸš€ Starting M&A deal scraping process with anti-blocking measures")
        logging.info(f"ğŸ“Š Target criteria: Revenue â‰¥ {CONFIG.get('scraping', {}).get('min_revenue', 300000000):,} yen, Profit â‰¥ {CONFIG.get('scraping', {}).get('min_profit', 30000000):,} yen")
        
        sheet_connector = GSheetConnector(CONFIG)
        if not sheet_connector.worksheet:
            logging.critical("âŒ Cannot proceed without Google Sheets connection")
            return
        
        existing_ids = sheet_connector.get_existing_ids()
        logging.info(f"ğŸ“‹ Found {len(existing_ids)} existing deals in spreadsheet")
        
        all_new_deals = []
        enabled_sites = [site for site in CONFIG['sites'] if site.get('enabled', False)]
        
        for site_config in enabled_sites:
            try:
                raw_deals = scrape_site(site_config)
                
                if not raw_deals:
                    logging.info(f"  No deals found from {site_config['name']}")
                    continue
                
                enhanced_deals = enhance_deals_with_details(raw_deals, site_config)
                formatted_deals = format_deal_data(enhanced_deals, existing_ids)
                
                logging.info(f"âœ… {site_config['name']}: {len(formatted_deals)} new deals after filtering")
                all_new_deals.extend(formatted_deals)
                
            except Exception as e:
                logging.error(f"âŒ Failed to process {site_config['name']}: {e}")
                continue
        
        if all_new_deals:
            sheet_connector.write_deals(all_new_deals)
            logging.info(f"ğŸ‰ Successfully added {len(all_new_deals)} new deals to spreadsheet")
        else:
            logging.info("ğŸ“ No new deals to add")
        
        logging.info("âœ¨ Scraping process completed successfully with anti-blocking measures")
        
    except Exception as e:
        logging.critical(f"ğŸ’¥ Critical error in main process: {e}")
        logging.debug(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()