# main.py (å®Œå…¨ç‰ˆ - ä¿®æ­£æ¸ˆã¿)
import httpx
from bs4 import BeautifulSoup, Tag
import gspread
from google.oauth2.service_account import Credentials
import datetime
import hashlib
import traceback
import logging
import yaml
import time
import os
import re
import random
from functools import wraps
from typing import Optional, Dict, List, Set, Any
from dataclasses import dataclass, fields
from enum import Enum

# Seleniumé–¢é€£
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
CONFIG: Dict[str, Any] = {}

# --- å®šæ•°ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ ---
class Constants:
    FIELD_EXTRACTION_TIME = "extraction_time"
    FIELD_SITE_NAME = "site_name"
    FIELD_DEAL_ID = "deal_id"
    FIELD_TITLE = "title"
    FIELD_LOCATION = "location"
    FIELD_REVENUE = "revenue"
    FIELD_PROFIT = "profit"
    FIELD_PRICE = "price"
    FIELD_FEATURES = "features"
    FIELD_LINK = "link"
    FIELD_UNIQUE_ID = "unique_id"
    
    # çµ±ä¸€ã•ã‚ŒãŸæ—¥æœ¬èªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
    JAPANESE_TO_ENGLISH_FIELDS: Dict[str, str] = {
        "æ¡ˆä»¶ID": FIELD_DEAL_ID,
        "æ¡ˆä»¶ç•ªå·": FIELD_DEAL_ID,
        "æ¡ˆä»¶No": FIELD_DEAL_ID,
        "ã‚¿ã‚¤ãƒˆãƒ«": FIELD_TITLE,
        "æ‰€åœ¨åœ°": FIELD_LOCATION,
        "æ‰€åœ¨åœ°åŸŸ": FIELD_LOCATION,
        "ã‚¨ãƒªã‚¢": FIELD_LOCATION,
        "åœ°åŸŸ": FIELD_LOCATION,
        "æ¥­ç¨®": FIELD_FEATURES,
        "æ¥­ç•Œ": FIELD_FEATURES,
        "å£²ä¸Šé«˜": FIELD_REVENUE,
        "æ¦‚ç®—å£²ä¸Š": FIELD_REVENUE,
        "å–¶æ¥­åˆ©ç›Š": FIELD_PROFIT,
        "å¸Œæœ›é‡‘é¡": FIELD_PRICE,
        "è­²æ¸¡å¸Œæœ›ä¾¡æ ¼": FIELD_PRICE,
        "ç‰¹è‰²": FIELD_FEATURES,
        "äº‹æ¥­å†…å®¹": FIELD_FEATURES,
        "äº‹æ¥­æ¦‚è¦": FIELD_FEATURES,
        "æ¦‚è¦": FIELD_FEATURES,
        "ç‰¹å¾´ãƒ»å¼·ã¿": FIELD_FEATURES,
        "ãƒªãƒ³ã‚¯": FIELD_LINK,
    }

class ScrapingStatus(Enum):
    SUCCESS = "success"
    FAILED = "failed"
    BLOCKED = "blocked"

@dataclass
class RawDealData:
    site_name: str
    title: str
    deal_id: str
    link: str
    revenue_text: str = ""
    profit_text: str = ""
    location_text: str = ""
    price_text: str = ""
    features_text: str = ""

@dataclass
class FormattedDealData:
    extraction_time: str
    site_name: str
    deal_id: str
    title: str
    features: str
    location: str
    revenue: str
    profit: str
    price: str
    link: str
    unique_id: str

# --- å°‚é–€å®¶ã‚¯ãƒ©ã‚¹ ---
class DataConverter:
    @staticmethod
    def parse_financial_value(text: str) -> int:
        """è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã«å¤‰æ›"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "é»’å­—ãªã—", "æç›Šãªã—"]):
            return 0
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')).replace(',', '')
        target_text = re.split(r'[ã€œï½-]', text)[-1]
        match = re.search(r'([\d\.]+)', target_text)
        if not match:
            return 0
        try:
            value = float(match.group(1))
        except ValueError:
            return 0
        multipliers = {'å„„': 100_000_000, 'åƒä¸‡': 10_000_000, 'ç™¾ä¸‡': 1_000_000, 'ä¸‡': 10_000}
        for unit, multiplier in multipliers.items():
            if unit in text:
                value *= multiplier
                break
        return int(value)
    
    @staticmethod
    def format_financial_text(text: str) -> str:
        """è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’ç™¾ä¸‡å††å˜ä½ã«çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "é»’å­—ãªã—", "æç›Šãªã—"]):
            return text or "N/A"
        
        def _to_million_format(text_part: str) -> str:
            clean_text = text_part.replace(',', '').strip()
            match = re.search(r'([\d\.]+)', clean_text)
            if not match:
                return text_part
            try:
                value = float(match.group(1))
            except ValueError:
                return text_part
            
            million_value = 0
            if 'å„„' in text_part:
                million_value = value * 100
            elif 'åƒä¸‡' in text_part:
                million_value = value * 10
            elif 'ç™¾ä¸‡å††' in text_part:
                return text_part
            elif 'ä¸‡' in text_part and 'ç™¾ä¸‡' not in text_part:
                million_value = value / 100
            else:
                return text_part
            
            if million_value >= 1:
                return f"{int(million_value):,}ç™¾ä¸‡å††" if million_value == int(million_value) else f"{million_value:.1f}ç™¾ä¸‡å††"
            else:
                return text_part
        
        separator = "ï½" if "ï½" in text else "ã€œ"
        if separator in text:
            parts = text.split(separator)
            if len(parts) == 2:
                return "ï½".join([_to_million_format(p.strip()) for p in parts])
        
        return _to_million_format(text)

    @staticmethod
    def convert_strike_revenue_to_million(revenue_text: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’ç™¾ä¸‡å††å˜ä½ã«å¤‰æ›"""
        if not revenue_text:
            return "-"
        
        conversion_map = {
            "5ï½10å„„å††": "500ï½1,000ç™¾ä¸‡å††",
            "10ï½50å„„å††": "1,000ï½5,000ç™¾ä¸‡å††", 
            "50ï½100å„„å††": "5,000ï½10,000ç™¾ä¸‡å††",
            "100å„„å††è¶…": "10,000ç™¾ä¸‡å††è¶…"
        }
        
        return conversion_map.get(revenue_text, revenue_text)

class AntiBlockingManager:
    """403ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.blocked_detected = False
        self.retry_count = 0
        self.max_retries = 1
        
    def get_human_like_delay(self, base_min: int = 3, base_max: int = 8) -> float:
        """äººé–“ã‚‰ã—ã„ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿæ™‚é–“ã‚’ç”Ÿæˆ"""
        return random.uniform(base_min, base_max)
    
    def get_recovery_delay(self) -> float:
        """ãƒ–ãƒ­ãƒƒã‚¯å¾Œã®å›å¾©å¾…æ©Ÿæ™‚é–“ã‚’ç”Ÿæˆ"""
        return random.uniform(15, 30)
    
    def is_blocked_response(self, html_content: str) -> bool:
        """403ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if not html_content:
            return False
            
        blocked_indicators = [
            "403 ERROR",
            "The request could not be satisfied",
            "Request blocked",
            "cloudfront",
            "Access Denied",
            "Forbidden"
        ]
        
        # HTMLã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦æ¤œç´¢
        content_lower = html_content.lower()
        
        for indicator in blocked_indicators:
            if indicator.lower() in content_lower:
                return True
        
        # titleã‚¿ã‚°ã®ç¢ºèª
        soup = BeautifulSoup(html_content, 'lxml')
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().lower()
            if 'error' in title_text or 'blocked' in title_text or 'denied' in title_text:
                return True
        
        return False

class WebDriverManager:
    """WebDriverã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    def __init__(self, headless: bool = True, anti_blocking: AntiBlockingManager = None):
        self.headless = headless
        self.driver: Optional[webdriver.Chrome] = None
        self.anti_blocking = anti_blocking or AntiBlockingManager()

    def __enter__(self) -> webdriver.Chrome:
        logging.info("Initializing Selenium WebDriver with anti-blocking measures...")
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        # ã‚ˆã‚Šäººé–“ã‚‰ã—ã„ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920x1080")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agentã‚’è¨­å®š
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"--user-agent={user_agent}")
        
        try:
            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            
            # WebDriverã®è‡ªå‹•åŒ–æ¤œå‡ºã‚’å›é¿
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info("âœ… WebDriver initialized successfully with anti-blocking measures.")
            return self.driver
        except Exception as e:
            logging.error(f"Failed to initialize WebDriver: {e}")
            raise

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            try:
                self.driver.quit()
                logging.info("WebDriver has been closed.")
            except Exception as e:
                logging.error(f"Error closing WebDriver: {e}")

class DetailPageScraper:
    """è©³ç´°ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å°‚é–€ã«è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    def __init__(self, driver: webdriver.Chrome, anti_blocking: AntiBlockingManager):
        self.driver = driver
        self.anti_blocking = anti_blocking

    def _format_features_text(self, raw_text_block: str) -> str:
        """ç‰¹è‰²ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢å‡¦ç†"""
        text_with_header_breaks = re.sub(r'(?<!^)(ã€[^ã€ã€‘]+ã€‘)', r'\n\1', raw_text_block)
        
        # è¡Œé ­ã®ãƒãƒ¼ã‚«ãƒ¼ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€æ–‡ä¸­ã®ã€Œãƒ»ã€ã¯åˆ†å‰²ã—ãªã„
        line_start_marker_pattern = r'(?<!^)(?<=ã€‚\s*)([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ])|(?<!^)(?<=ã€\s*)([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ])|(?<!^)(?<=\n\s*)([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ])'
        
        # ã‚ˆã‚Šæ…é‡ãªåˆ†å‰²ï¼šæ–‡ã®çµ‚ã‚ã‚Šã‚„æ˜ç¢ºãªåŒºåˆ‡ã‚Šã®å¾Œã®ãƒãƒ¼ã‚«ãƒ¼ã®ã¿åˆ†å‰²
        if re.search(line_start_marker_pattern, text_with_header_breaks):
            text_with_all_breaks = re.sub(line_start_marker_pattern, r'\n\1\2\3', text_with_header_breaks)
            lines = text_with_all_breaks.splitlines()
        else:
            lines = text_with_header_breaks.splitlines()
        
        final_lines = []
        for line in lines:
            stripped_line = line.strip()
            if stripped_line:
                # ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                cleaned_line = re.sub(r'^([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– ã€€â˜†â˜…â€»â–²â–½])[\sã€€\t]+', r'\1', stripped_line)
                final_lines.append(cleaned_line)
        
        return "\n".join(final_lines)

    def fetch_features_with_blocking_protection(self, detail_url: str, selectors: Dict[str, Any], referer_url: str = None) -> str:
        """403ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ä»˜ãã®æ±ç”¨çš„ãªç‰¹è‰²æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰"""
        if not detail_url or detail_url == 'N/A':
            return "-"
        
        try:
            logging.info(f"    -> Accessing detail page: {detail_url}")
            
            # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
            delay = self.anti_blocking.get_human_like_delay()
            logging.info(f"    -> Waiting {delay:.1f} seconds before access...")
            time.sleep(delay)
            
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            self.driver.get(detail_url)
            time.sleep(2.5)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            html_content = self.driver.page_source
            
            # 403ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
            if self.anti_blocking.is_blocked_response(html_content):
                logging.warning(f"    -> ğŸš« 403 BLOCK DETECTED for URL: {detail_url}")
                
                if not self.anti_blocking.blocked_detected:
                    self.anti_blocking.blocked_detected = True
                    logging.warning("    -> First block detected. Attempting recovery...")
                    
                    # å›å¾©å¾…æ©Ÿæ™‚é–“
                    recovery_delay = self.anti_blocking.get_recovery_delay()
                    logging.info(f"    -> Recovery wait: {recovery_delay:.1f} seconds...")
                    time.sleep(recovery_delay)
                    
                    # ãƒªãƒˆãƒ©ã‚¤
                    logging.info("    -> Retrying access...")
                    self.driver.get(detail_url)
                    time.sleep(3)
                    
                    retry_html = self.driver.page_source
                    
                    if self.anti_blocking.is_blocked_response(retry_html):
                        logging.error("    -> âŒ Still blocked after retry. Skipping this deal.")
                        return "-"
                    else:
                        logging.info("    -> âœ… Recovery successful!")
                        html_content = retry_html
                        self.anti_blocking.blocked_detected = False
                else:
                    logging.error("    -> âŒ Already in blocked state. Skipping this deal.")
                    return "-"
            
            detail_soup = BeautifulSoup(html_content, 'lxml')
            
            # M&Aç·åˆç ”ç©¶æ‰€ã®ç‰¹åˆ¥å‡¦ç†
            if 'masouken.com' in detail_url:
                return self._fetch_masouken_features(detail_soup)
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹åˆ¥å‡¦ç†
            if 'strike.co.jp' in detail_url:
                return self._fetch_strike_features_enhanced(detail_soup, detail_url)
            
            # æ¨™æº–çš„ãªç‰¹è‰²æŠ½å‡ºå‡¦ç†
            return self._fetch_standard_features(detail_soup, selectors)
            
        except WebDriverException as e:
            logging.error(f"    -> WebDriver error on detail page: {detail_url} - {e}")
            return "-"
        except Exception as e:
            logging.error(f"    -> Error parsing detail page: {e}")
            logging.debug(traceback.format_exc())
            return "-"

    def _fetch_strike_features_enhanced(self, detail_soup: BeautifulSoup, detail_url: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã®ç‰¹è‰²æŠ½å‡ºï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            deal_id = detail_url.split('code=')[-1] if 'code=' in detail_url else 'unknown'
            debug_file = os.path.join("debug", f"debug_strike_detail_{deal_id}_{timestamp}.html")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(str(detail_soup))
            logging.info(f"Debug: Strike detail HTML saved to {debug_file}")
        
        features_sections = []
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªul.detail__listã‚’æ¢ã™
        detail_list = detail_soup.find('ul', class_='detail__list')
        if detail_list:
            logging.info("    -> Found ul.detail__list using standard approach")
            
            # äº‹æ¥­æ¦‚è¦ã®æŠ½å‡º
            business_overview = self._extract_strike_list_item_enhanced(detail_list, 'äº‹æ¥­æ¦‚è¦')
            if business_overview:
                features_sections.append(f"ã€äº‹æ¥­æ¦‚è¦ã€‘\n{business_overview}")
                logging.info(f"    -> Found business overview: {business_overview[:50]}...")
            
            # ç‰¹å¾´ãƒ»å¼·ã¿ã®æŠ½å‡º
            strengths = self._extract_strike_list_item_enhanced(detail_list, 'ç‰¹å¾´ãƒ»å¼·ã¿')
            if strengths:
                features_sections.append(f"ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘\n{strengths}")
                logging.info(f"    -> Found strengths: {strengths[:50]}...")
        else:
            logging.warning("    -> ul.detail__list not found, trying alternative approaches")
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
            features_sections = self._extract_strike_features_from_text(detail_soup)
        
        if features_sections:
            result = "\n\n".join(features_sections)
            logging.info(f"    -> Successfully extracted features ({len(result)} chars)")
            return result
        else:
            logging.warning("    -> No features found with any approach")
            return "-"

    def _extract_strike_list_item_enhanced(self, detail_list: Tag, label_text: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ãƒªã‚¹ãƒˆé …ç›®ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
        li_items = detail_list.find_all('li')
        
        for li in li_items:
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: span.labelã‚’æ¢ã™
            label_span = li.find('span', class_='label')
            if label_span and label_text in label_span.get_text(strip=True):
                
                # æ–¹æ³•1: find_next_siblingã§spanã‚¿ã‚°ã‚’ç›´æ¥æ¢ã™
                value_span = label_span.find_next_sibling('span')
                if value_span:
                    # ã€ä¿®æ­£ã€‘brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›ã—ã¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    value_span_copy = value_span.__copy__() if hasattr(value_span, '__copy__') else value_span
                    for br in value_span_copy.find_all('br'):
                        br.replace_with('\n')
                    value_text = value_span_copy.get_text(strip=True)
                    if value_text and len(value_text) > 2:
                        # ã€ä¿®æ­£ã€‘æ”¹è¡Œã‚’ä¿æŒã—ãªãŒã‚‰ãƒãƒ¼ã‚«ãƒ¼å‡¦ç†
                        formatted_value = self._format_strike_text_with_linebreaks(value_text)
                        logging.info(f"    -> Found via next_sibling span: {formatted_value[:30]}...")
                        return formatted_value
                
                # æ–¹æ³•2: liè¦ç´ å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                # ã€ä¿®æ­£ã€‘brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                li_copy = li.__copy__() if hasattr(li, '__copy__') else li
                for br in li_copy.find_all('br'):
                    br.replace_with('\n')
                li_text = li_copy.get_text(separator='\n', strip=True)
                label_full_text = label_span.get_text(strip=True)
                if label_full_text in li_text:
                    remaining_text = li_text.replace(label_full_text, '', 1).strip()
                    if remaining_text and len(remaining_text) > 2:
                        # ã€ä¿®æ­£ã€‘æ”¹è¡Œã‚’ä¿æŒã—ãªãŒã‚‰ãƒãƒ¼ã‚«ãƒ¼å‡¦ç†
                        formatted_remaining = self._format_strike_text_with_linebreaks(remaining_text)
                        logging.info(f"    -> Found via text extraction: {formatted_remaining[:30]}...")
                        return formatted_remaining
                
                # æ–¹æ³•3: è¤‡æ•°ã®å…„å¼Ÿè¦ç´ ã‚’é †æ¬¡ãƒã‚§ãƒƒã‚¯
                current_sibling = label_span.next_sibling
                collected_text = []
                for attempt in range(5):
                    if current_sibling is None:
                        break
                    
                    if isinstance(current_sibling, str):
                        text_content = current_sibling.strip()
                        if text_content and len(text_content) > 2:
                            collected_text.append(text_content)
                    elif hasattr(current_sibling, 'get_text'):
                        # ã€ä¿®æ­£ã€‘brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                        sibling_copy = current_sibling.__copy__() if hasattr(current_sibling, '__copy__') else current_sibling
                        for br in sibling_copy.find_all('br'):
                            br.replace_with('\n')
                        tag_content = sibling_copy.get_text(strip=True)
                        if tag_content and len(tag_content) > 2:
                            collected_text.append(tag_content)
                    
                    current_sibling = current_sibling.next_sibling
                
                if collected_text:
                    combined_text = '\n'.join(collected_text)
                    formatted_combined = self._format_strike_text_with_linebreaks(combined_text)
                    logging.info(f"    -> Found via sibling collection: {formatted_combined[:30]}...")
                    return formatted_combined
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒ©ãƒ™ãƒ«ãŒç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            # ã€ä¿®æ­£ã€‘brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            li_copy = li.__copy__() if hasattr(li, '__copy__') else li
            for br in li_copy.find_all('br'):
                br.replace_with('\n')
            li_text = li_copy.get_text(strip=True)
            if label_text in li_text:
                parts = li_text.split(label_text, 1)
                if len(parts) == 2:
                    remaining = parts[1].strip()
                    remaining = re.sub(r'^[ï¼š:\sã€€]+', '', remaining)
                    if remaining and len(remaining) > 2:
                        formatted_remaining = self._format_strike_text_with_linebreaks(remaining)
                        logging.info(f"    -> Found via text split: {formatted_remaining[:30]}...")
                        return formatted_remaining
        
        logging.warning(f"    -> No content found for label: {label_text}")
        return ""

    def _format_strike_text_with_linebreaks(self, text: str) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ”¹è¡Œã‚’ä¿æŒã—ãªãŒã‚‰æ•´å½¢"""
        if not text:
            return ""
        
        lines = []
        
        # æ—¢å­˜ã®æ”¹è¡Œã§åˆ†å‰²
        raw_lines = text.split('\n')
        
        for line in raw_lines:
            line = line.strip()
            if not line:
                continue
            
            # è¡Œé ­ã®ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
            line_start_markers = ['â– ', 'â—', 'â—†', 'â—‹', 'â–¼', 'â—']  # â†ã“ã“ã«ã€Œâ– ã€ã‚’è¿½åŠ 
            has_line_start_marker = any(line.startswith(marker) for marker in line_start_markers)
            
            if has_line_start_marker:
                # è¡Œé ­ã«ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾è¿½åŠ ï¼ˆãƒãƒ¼ã‚«ãƒ¼å¾Œã®ç©ºç™½ã¯å‰Šé™¤ï¼‰
                cleaned_line = re.sub(r'^([â– â—â—†â—‹â–¼â—])[\sã€€\t]+', r'\1', line)  # â†ã“ã“ã«ã‚‚ã€Œâ– ã€ã‚’è¿½åŠ 
                lines.append(cleaned_line)
            else:
                # è¡Œé ­ã«ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯ã€æ–‡ä¸­ã®ã€Œãƒ»ã€ã§ã¯åˆ†å‰²ã—ãªã„
                # ãŸã ã—ã€æ˜ã‚‰ã‹ã«ç®‡æ¡æ›¸ãã¨æ€ã‚ã‚Œã‚‹å ´åˆï¼ˆã€Œãƒ»ã€ã®å¾Œã«å¤§æ–‡å­—ã‚„æ”¹è¡ŒãŒã‚ã‚‹å ´åˆï¼‰ã®ã¿åˆ†å‰²
                bullet_pattern = r'([ãƒ»])(?=\s*[A-Zä¸€-é¾¯])'  # ã€Œãƒ»ã€ã®å¾Œã«æ–‡å­—ãŒç¶šãå ´åˆã®ã¿
                
                # æ–‡ä¸­ã®ã€Œãƒ»ã€ï¼ˆä¾‹ï¼šã€Œä¼ç”»ãƒ»é–‹ç™ºã€ï¼‰ã¯åˆ†å‰²ã—ãªã„
                # ç®‡æ¡æ›¸ãã®ã€Œãƒ»ã€ï¼ˆä¾‹ï¼šã€Œãƒ»é–¢è¥¿åœ°æ–¹ã‚’ä¸­å¿ƒã«ã€ï¼‰ã®ã¿åˆ†å‰²
                if 'ãƒ»' in line:
                    # ã€Œãƒ»ã€ãŒè¡Œé ­ã«ã‚ã‚‹ã€ã¾ãŸã¯ã€Œãƒ»ã€ã®å‰å¾Œã«ååˆ†ãªæ–‡è„ˆãŒã‚ã‚‹å ´åˆã®ã¿åˆ†å‰²
                    parts = re.split(r'(?<=ã€‚)\s*ãƒ»|(?<=ã€)\s*ãƒ»|^\s*ãƒ»', line)
                    
                    if len(parts) > 1:
                        # åˆ†å‰²ã•ã‚ŒãŸå ´åˆ
                        for i, part in enumerate(parts):
                            part = part.strip()
                            if part:
                                if i == 0 and not part.startswith('ãƒ»'):
                                    lines.append(part)
                                else:
                                    if not part.startswith('ãƒ»'):
                                        part = f'ãƒ»{part}'
                                    # ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
                                    cleaned_part = re.sub(r'^([ãƒ»])[\sã€€\t]+', r'\1', part)
                                    lines.append(cleaned_part)
                    else:
                        # åˆ†å‰²ã•ã‚Œãªã‹ã£ãŸå ´åˆï¼ˆæ–‡ä¸­ã®ã€Œãƒ»ã€ï¼‰ã¯ãã®ã¾ã¾è¿½åŠ 
                        lines.append(line)
                else:
                    # ã€Œãƒ»ã€ãŒãªã„å ´åˆã¯ã€Œâ– ã€ã§ã®åˆ†å‰²å‡¦ç†ã‚’è¿½åŠ 
                    if 'â– ' in line and not line.startswith('â– '):
                        # ã€Œâ– ã€ã§åˆ†å‰²ï¼ˆæ–‡æœ«ã®ã€Œã€‚ã€ã®å¾Œã®ã€Œâ– ã€ã®ã¿ï¼‰
                        parts = re.split(r'(?<=ã€‚)\s*â– ', line)
                        
                        if len(parts) > 1:
                            # æœ€åˆã®éƒ¨åˆ†ã‚’ãã®ã¾ã¾è¿½åŠ 
                            if parts[0].strip():
                                lines.append(parts[0].strip())
                            
                            # æ®‹ã‚Šã®éƒ¨åˆ†ã«ã€Œâ– ã€ã‚’ä»˜ã‘ã¦è¿½åŠ 
                            for part in parts[1:]:
                                part = part.strip()
                                if part:
                                    cleaned_part = re.sub(r'^[\sã€€\t]+', '', part)
                                    lines.append(f'â– {cleaned_part}')
                        else:
                            lines.append(line)
                    else:
                        # ã€Œâ– ã€ãŒãªã„å ´åˆã€ã¾ãŸã¯è¡Œé ­ãŒã€Œâ– ã€ã®å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
                        lines.append(line)
        
        # é‡è¤‡é™¤å»ã¨æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        final_lines = []
        seen = set()
        
        for line in lines:
            if line and len(line) > 2 and line not in seen:
                final_lines.append(line)
                seen.add(line)
        
        return '\n'.join(final_lines)

    def _extract_strike_features_from_text(self, detail_soup: BeautifulSoup) -> List[str]:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹è‰²ã‚’ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        features_sections = []
        full_text = detail_soup.get_text()
        
        # äº‹æ¥­æ¦‚è¦ã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        business_patterns = [
            r'äº‹æ¥­æ¦‚è¦[ï¼š:\s]*([^\n]{20,200})',
            r'ã€äº‹æ¥­æ¦‚è¦ã€‘([^ã€]{20,200})',
            r'äº‹æ¥­å†…å®¹[ï¼š:\s]*([^\n]{20,200})',
        ]
        
        for pattern in business_patterns:
            matches = re.findall(pattern, full_text, re.DOTALL)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 20:
                    features_sections.append(f"ã€äº‹æ¥­æ¦‚è¦ã€‘\n{clean_match}")
                    break
            if features_sections:
                break
        
        # ç‰¹å¾´ãƒ»å¼·ã¿ã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        strength_patterns = [
            r'ç‰¹å¾´ãƒ»å¼·ã¿[ï¼š:\s]*([^\n]{20,200})',
            r'ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘([^ã€]{20,200})',
            r'å¼·ã¿[ï¼š:\s]*([^\n]{20,200})',
        ]
        
        for pattern in strength_patterns:
            matches = re.findall(pattern, full_text, re.DOTALL)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 20:
                    features_sections.append(f"ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘\n{clean_match}")
                    break
            if len(features_sections) == 2:  # æ—¢ã«äº‹æ¥­æ¦‚è¦ã‚‚ã‚ã‚‹å ´åˆ
                break
        
        return features_sections

    def _fetch_masouken_features(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€å°‚ç”¨ã®äº‹æ¥­è©³ç´°ã¨å¼·ã¿æŠ½å‡º"""
        features_sections = []
        
        business_details = self._extract_masouken_business_details(detail_soup)
        if business_details:
            features_sections.append(f"ã€äº‹æ¥­è©³ç´°ã€‘\n{business_details}")
        
        strengths = self._extract_masouken_strengths(detail_soup)
        if strengths:
            features_sections.append(f"ã€å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã€‘\n{strengths}")
        
        if features_sections:
            return "\n\n".join(features_sections)
        else:
            return "-"

    def _extract_masouken_business_details(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®äº‹æ¥­è©³ç´°æŠ½å‡º"""
        business_keywords = ['äº‹æ¥­è©³ç´°', 'äº‹æ¥­å†…å®¹', 'äº‹æ¥­æ¦‚è¦', 'æ¦‚è¦', 'ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«']
        
        for keyword in business_keywords:
            dt_elements = detail_soup.find_all('dt', string=re.compile(keyword))
            for dt in dt_elements:
                dd = dt.find_next_sibling('dd')
                if dd:
                    content = dd.get_text(strip=True)
                    if len(content) > 20:
                        return self._format_masouken_text(content)
        
        for keyword in business_keywords:
            headers = detail_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], 
                                         string=re.compile(keyword))
            
            for header in headers:
                content_elements = []
                current = header
                
                for _ in range(10):
                    current = current.find_next_sibling()
                    if not current or not isinstance(current, Tag):
                        break
                    
                    if current.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and current.get_text().strip():
                        break
                    
                    text = current.get_text(strip=True)
                    if len(text) > 15:
                        content_elements.append(current)
                
                if content_elements:
                    result = self._format_masouken_elements(content_elements)
                    if result and len(result) > 20:
                        return result
        
        return ""

    def _extract_masouken_strengths(self, detail_soup: BeautifulSoup) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º"""
        strength_keywords = ['å¼·ã¿ãƒ»å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ', 'å¼·ã¿', 'å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ', 'ç‰¹å¾´', 'ç«¶åˆå„ªä½æ€§', 'å„ªä½æ€§']
        
        for keyword in strength_keywords:
            dt_elements = detail_soup.find_all('dt', string=re.compile(keyword))
            for dt in dt_elements:
                dd = dt.find_next_sibling('dd')
                if dd:
                    content = dd.get_text(strip=True)
                    if len(content) > 20:
                        return self._format_masouken_text(content)
        
        for keyword in strength_keywords:
            headers = detail_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], 
                                         string=re.compile(keyword))
            
            for header in headers:
                content_elements = []
                current = header
                
                for _ in range(10):
                    current = current.find_next_sibling()
                    if not current or not isinstance(current, Tag):
                        break
                    
                    if current.name in ['h1', 'h2', 'h3', 'h4', 'h5'] and current.get_text().strip():
                        break
                    
                    text = current.get_text(strip=True)
                    if len(text) > 15:
                        content_elements.append(current)
                
                if content_elements:
                    result = self._format_masouken_elements(content_elements)
                    if result and len(result) > 20:
                        return result
        
        return ""

    def _fetch_standard_features(self, detail_soup: BeautifulSoup, selectors: Dict[str, Any]) -> str:
        """æ¨™æº–çš„ãªç‰¹è‰²æŠ½å‡ºå‡¦ç†"""
        selector_config = selectors.get('features')
        if not selector_config:
            return "-"
        
        start_keywords = selector_config.get('start_keywords', [])
        end_tag = selector_config.get('end_tag')
        target_tags = selector_config.get('target_tags', [])
        
        start_element = None
        for keyword in start_keywords:
            found = detail_soup.find(['h1', 'h2', 'h3', 'h4', 'h5', 'dt'], string=re.compile(keyword))
            if found:
                start_element = found
                break
        
        if not start_element:
            return "-"
        
        collected_elements = []
        for sibling in start_element.find_next_siblings():
            if not isinstance(sibling, Tag):
                continue
            if sibling.name == end_tag:
                break
            if sibling.name in target_tags:
                collected_elements.append(sibling)
            else:
                child_tags = sibling.find_all(target_tags)
                if child_tags:
                    collected_elements.extend(child_tags)
        
        if not collected_elements:
            return "-"
        
        raw_text_block = "\n".join([elem.get_text(strip=True) for elem in collected_elements if elem.get_text(strip=True)])
        return self._format_features_text(raw_text_block) if raw_text_block else "-"

    def _format_masouken_elements(self, elements: List[Tag]) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®è¦ç´ ã‚’æ•´å½¢"""
        formatted_items = []
        
        for element in elements:
            text = element.get_text(strip=True)
            
            if element.name in ['ul', 'ol']:
                li_items = element.find_all('li')
                for li in li_items:
                    item_text = li.get_text(strip=True)
                    if len(item_text) > 10:
                        # ã€ä¿®æ­£ã€‘ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰è¿½åŠ 
                        cleaned_item = re.sub(r'([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– â—])[\sã€€\t]+', r'\1', item_text)
                        formatted_items.append(f"ãƒ»{cleaned_item}")
            elif any(marker in text for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']):
                for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']:
                    if marker in text:
                        parts = text.split(marker)
                        for part in parts[1:]:
                            part = part.strip()
                            if len(part) > 10:
                                # ã€ä¿®æ­£ã€‘ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã€æ”¹è¡Œã§åˆ†å‰²
                                cleaned_part = re.sub(r'^[\sã€€]+', '', part)
                                # æ–‡ã®åŒºåˆ‡ã‚Šã§æ”¹è¡Œã‚’è¿½åŠ 
                                sentences = re.split(r'(?<=[ã€‚ï¼])\s*(?=[ãƒ»â—‹â—†âœ“â—â—‰â–¼â– â—])|(?<=[ã€‚ï¼])\s*(?=\S)', cleaned_part)
                                for sentence in sentences:
                                    sentence = sentence.strip()
                                    if len(sentence) > 10:
                                        formatted_items.append(f"ãƒ»{sentence}")
                        break
            elif len(text) > 15:
                # ã€ä¿®æ­£ã€‘æ–‡ç« ã‚’è‡ªç„¶ãªåŒºåˆ‡ã‚Šã§åˆ†å‰²
                # ã¾ãšã€ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã§åˆ†å‰²
                if any(marker in text for marker in ['ãƒ»', 'â—‹', 'â—†', 'âœ“', 'â—', 'â—‰', 'â–¼', 'â– ', 'â—']):
                    marker_pattern = r'([ãƒ»â—‹â—†âœ“â—â—‰â–¼â– â—])'
                    parts = re.split(marker_pattern, text)
                    current_item = ""
                    for i, part in enumerate(parts):
                        if part in ['ãƒ»', 'â—‹', 'â—†', 'âœ“', 'â—', 'â—‰', 'â–¼', 'â– ', 'â—']:
                            if current_item.strip() and len(current_item.strip()) > 10:
                                formatted_items.append(f"ãƒ»{current_item.strip()}")
                            current_item = ""
                        else:
                            current_item += part
                    if current_item.strip() and len(current_item.strip()) > 10:
                        formatted_items.append(f"ãƒ»{current_item.strip()}")
                else:
                    # ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯å¥ç‚¹ã§åˆ†å‰²
                    sentences = re.split(r'[ã€‚ï¼]', text)
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if len(sentence) > 15:
                            if not sentence.endswith('ã€‚'):
                                sentence += 'ã€‚'
                            cleaned_sentence = re.sub(r'^[\sã€€]+', '', sentence)
                            formatted_items.append(f"ãƒ»{cleaned_sentence}")
        
        return "\n".join(formatted_items[:8])  # æœ€å¤§8é …ç›®ã«å¢—åŠ 

    def _format_masouken_text(self, text: str) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢"""
        if not text or len(text) < 20:
            return ""
        
        cleaned_text = text.strip()
        
        # ã€ä¿®æ­£ã€‘ãƒãƒ¼ã‚«ãƒ¼ãŒã‚ã‚‹å ´åˆã¯æ”¹è¡Œã‚’ä¿æŒ
        if any(marker in cleaned_text for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']):
            lines = []
            # ã¾ãšæ”¹è¡Œã§åˆ†å‰²
            text_lines = cleaned_text.split('\n')
            
            for line in text_lines:
                line = line.strip()
                if not line:
                    continue
                    
                # ãƒãƒ¼ã‚«ãƒ¼ã§åˆ†å‰²ã—ã¤ã¤æ”¹è¡Œã‚‚è€ƒæ…®
                found_marker = False
                for marker in ['ãƒ»', 'â—†', 'â–¼', 'â—‹', 'â—']:
                    if marker in line:
                        found_marker = True
                        # ãƒãƒ¼ã‚«ãƒ¼ã§åˆ†å‰²
                        parts = line.split(marker)
                        for part in parts[1:]:  # æœ€åˆã®ç©ºã®éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                            part = part.strip()
                            if len(part) > 10:
                                cleaned_part = re.sub(r'^[\sã€€]+', '', part)
                                lines.append(f"ãƒ»{cleaned_part}")
                        break
                
                if not found_marker and len(line) > 10:
                    lines.append(f"ãƒ»{line}")
            
            if lines:
                return '\n'.join(lines[:5])  # æœ€å¤§5é …ç›®
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(cleaned_text) > 500:
            sentences = re.split(r'[ã€‚ï¼]', cleaned_text)
            truncated_sentences = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) > 500:
                    break
                truncated_sentences.append(sentence.strip())
                current_length += len(sentence)
            
            cleaned_text = 'ã€‚'.join([s for s in truncated_sentences if s])
            if cleaned_text and not cleaned_text.endswith('ã€‚'):
                cleaned_text += 'ã€‚'
        
        # ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯æ–‡ã§åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼]', cleaned_text)
        bullet_points = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:
                bullet_points.append(f"ãƒ»{sentence}ã€‚")
        
        return "\n".join(bullet_points[:3])

class UniversalParser:
    """çµ±ä¸€ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def parse_list_page(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """æ±ç”¨çš„ãªä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚µãƒ¼"""
        site_name = site_config['name']
        parser_type = site_config.get('parser_type', 'standard')
        
        # M&Aç·åˆç ”ç©¶æ‰€ã®ç‰¹åˆ¥å‡¦ç†
        if parser_type == 'text_based' or site_name == "M&Aç·åˆç ”ç©¶æ‰€":
            return UniversalParser._parse_masouken_text_based(site_config, html_content)
        
        # M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®ç‰¹åˆ¥å‡¦ç†
        if site_name == "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º":
            return UniversalParser._parse_ma_capital_partners(site_config, html_content)
        
        # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ç‰¹åˆ¥å‡¦ç†
        if site_name == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
            return UniversalParser._parse_strike(site_config, html_content)
        
        # æ¨™æº–çš„ãªHTMLã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼å‡¦ç†
        return UniversalParser._parse_selector_based(site_config, html_content)
    
    @staticmethod
    def _parse_strike(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆå‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = os.path.join("debug", f"debug_strike_{timestamp}.html")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        selectors_to_try = [
            'div.search-result__item',
            'div[class*="search-result"]',
            'div[class*="item"]',
            'div[class*="case"]',
            'article',
            'li'
        ]
        
        items = []
        for selector in selectors_to_try:
            items = soup.select(selector)
            if items:
                logging.info(f"Found {len(items)} items using selector: {selector}")
                break
        
        if not items:
            logging.warning("No items found with any selector")
            return []
        
        # ä¸Šã‹ã‚‰52ä»¶ã‚’å¯¾è±¡ï¼ˆ14è¡ŒÃ—3æ¡ˆä»¶/è¡Œï¼‰
        items_to_process = items[:52]
        
        for i, item in enumerate(items_to_process):
            try:
                logging.info(f"Processing item {i+1}/{len(items_to_process)}")
                
                # æ¡ˆä»¶ç•ªå·ã®æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
                deal_id = UniversalParser._extract_strike_deal_id_flexible(item)
                if not deal_id:
                    logging.info(f"No deal ID found in item {i+1}, skipping")
                    continue
                
                logging.info(f"Found deal ID: {deal_id}")
                
                # å£²ä¸Šé«˜ã®æŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                revenue_text = UniversalParser._extract_strike_revenue_flexible(item)
                
                # å£²ä¸Šé«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šæŒ‡å®šã•ã‚ŒãŸ4ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿è©³ç´°ãƒšãƒ¼ã‚¸ã«é€²ã‚€
                valid_revenues = ["5ï½10å„„å††", "10ï½50å„„å††", "50ï½100å„„å††", "100å„„å††è¶…"]
                if revenue_text not in valid_revenues:
                    logging.info(f"Skipping deal {deal_id}: Revenue '{revenue_text}' doesn't meet criteria")
                    continue
                
                logging.info(f"Revenue meets criteria: {revenue_text}")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã§å–å¾—ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä»®ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                title = f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal_id}"
                
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’æ§‹ç¯‰
                link = f"https://www.strike.co.jp/smart/sell_details.html?code={deal_id}"
                
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                deal_data = RawDealData(
                    site_name=site_config['name'],
                    deal_id=deal_id,
                    title=title,
                    link=link,
                    location_text="",
                    revenue_text=revenue_text,
                    profit_text="-",
                    price_text="-",
                    features_text=""
                )
                
                results.append(deal_data)
                logging.info(f"Successfully extracted deal: {deal_id}")
                
            except Exception as e:
                logging.error(f"Error parsing ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ item {i+1}: {e}")
                continue
        
        logging.info(f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯: Successfully extracted {len(results)} deals meeting criteria")
        return results

    @staticmethod
    def _extract_strike_deal_id_flexible(item: Tag) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®æ¡ˆä»¶IDã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        selectors_to_try = [
            'p.search-result__num',
            '.search-result__num',
            '[class*="num"]',
            '[class*="id"]'
        ]
        
        for selector in selectors_to_try:
            num_element = item.select_one(selector)
            if num_element:
                text = num_element.get_text(strip=True)
                ss_match = re.search(r'(SS\d+)', text)
                if ss_match:
                    return ss_match.group(1)
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰SSç•ªå·ã‚’æ¤œç´¢
        item_text = item.get_text()
        ss_match = re.search(r'(SS\d+)', item_text)
        if ss_match:
            return ss_match.group(1)
        
        return ""

    @staticmethod
    def _extract_strike_revenue_flexible(item: Tag) -> str:
        """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        selectors_to_try = [
            'span.sales-amount',
            '.sales-amount',
            '[class*="sales"]',
            '[class*="amount"]'
        ]
        
        for selector in selectors_to_try:
            sales_element = item.select_one(selector)
            if sales_element:
                return sales_element.get_text(strip=True)
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰å£²ä¸Šé«˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        item_text = item.get_text()
        revenue_patterns = [
            r'5ï½10å„„å††', r'5ã€œ10å„„å††',
            r'10ï½50å„„å††', r'10ã€œ50å„„å††',
            r'50ï½100å„„å††', r'50ã€œ100å„„å††',
            r'100å„„å††è¶…'
        ]
        
        for pattern in revenue_patterns:
            if re.search(pattern, item_text):
                # æ­£è¦åŒ–ã—ã¦è¿”ã™
                normalized = pattern.replace('ã€œ', 'ï½')
                return normalized
        
        return ""
    
    @staticmethod
    def _parse_ma_capital_partners(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºå°‚ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆæŸ”è»Ÿæ€§å‘ä¸Šç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = os.path.join("debug", f"debug_ma_capital_{timestamp}.html")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # æ¡ˆä»¶ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼‰
        selectors_to_try = [
            'article.c-filter-project',
            'article[class*="project"]',
            'div[class*="project"]',
            'div[class*="case"]',
            'article',
            'li[class*="item"]'
        ]
        
        items = []
        for selector in selectors_to_try:
            items = soup.select(selector)
            if items:
                logging.info(f"Found {len(items)} items using selector: {selector}")
                break
        
        if not items:
            logging.warning("No items found with any selector")
            return []
        
        for i, item in enumerate(items):
            try:
                logging.info(f"Processing item {i+1}/{len(items)}")
                
                # æ¡ˆä»¶ç•ªå·ã®æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
                deal_id = UniversalParser._extract_ma_capital_deal_id_flexible(item)
                if not deal_id:
                    logging.info(f"No deal number found in item {i+1}, skipping")
                    continue
                
                logging.info(f"Found deal ID: {deal_id}")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
                title = UniversalParser._extract_ma_capital_title_flexible(item, deal_id)
                
                # ãƒªãƒ³ã‚¯ã®æŠ½å‡º
                link = UniversalParser._extract_ma_capital_link_flexible(item, deal_id)
                
                # è²¡å‹™æƒ…å ±ã®æŠ½å‡º
                revenue_text = UniversalParser._extract_ma_capital_dl_data_flexible(item, ['æ¦‚ç®—å£²ä¸Š', 'å£²ä¸Šé«˜', 'å£²ä¸Š'])
                profit_text = UniversalParser._extract_ma_capital_dl_data_flexible(item, ['å–¶æ¥­åˆ©ç›Š', 'åˆ©ç›Š'])
                location_text = UniversalParser._extract_ma_capital_dl_data_flexible(item, ['æ‰€åœ¨åœ°', 'ã‚¨ãƒªã‚¢', 'åœ°åŸŸ'])
                price_text = UniversalParser._extract_ma_capital_dl_data_flexible(item, ['å¸Œæœ›é‡‘é¡', 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'ä¾¡æ ¼'])
                
                logging.info(f"Revenue: {revenue_text}, Profit: {profit_text}")
                
                # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                revenue_value = DataConverter.parse_financial_value(revenue_text)
                profit_value = DataConverter.parse_financial_value(profit_text)
                
                min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                
                if revenue_value < min_revenue or profit_value < min_profit:
                    logging.info(f"Skipping deal {deal_id}: doesn't meet financial criteria")
                    continue
                
                # äº‹æ¥­å†…å®¹ã®æŠ½å‡º
                features_text = UniversalParser._extract_ma_capital_business_content_flexible(item)
                
                # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                deal_data = RawDealData(
                    site_name=site_config['name'],
                    deal_id=deal_id,
                    title=title,
                    link=link,
                    location_text=location_text,
                    revenue_text=revenue_text,
                    profit_text=profit_text,
                    price_text=price_text,
                    features_text=features_text
                )
                
                results.append(deal_data)
                logging.info(f"Successfully extracted deal: {deal_id} - {title[:50]}")
                
            except Exception as e:
                logging.error(f"Error parsing M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º item {i+1}: {e}")
                continue
        
        logging.info(f"M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º: Successfully extracted {len(results)} deals")
        return results
    
    @staticmethod
    def _extract_ma_capital_deal_id_flexible(item: Tag) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®æ¡ˆä»¶IDã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        selectors_to_try = [
            '.c-filter-project__no',
            '[class*="no"]',
            '[class*="id"]',
            '[class*="num"]'
        ]
        
        for selector in selectors_to_try:
            deal_no_element = item.select_one(selector)
            if deal_no_element:
                deal_no_text = deal_no_element.get_text(strip=True)
                deal_match = re.search(r'æ¡ˆä»¶No[ï¼š:\s]*([A-Z0-9-]+)', deal_no_text)
                if deal_match:
                    return deal_match.group(1)
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æ¡ˆä»¶ç•ªå·ã‚’æ¤œç´¢
        item_text = item.get_text()
        patterns = [
            r'æ¡ˆä»¶No[ï¼š:\s]*([A-Z0-9-]+)',
            r'æ¡ˆä»¶ç•ªå·[ï¼š:\s]*([A-Z0-9-]+)',
            r'No[ï¼š:\s]*([A-Z0-9-]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, item_text)
            if match:
                return match.group(1)
        
        return ""

    @staticmethod
    def _extract_ma_capital_title_flexible(item: Tag, deal_id: str) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        selectors_to_try = [
            '.c-filter-project__ttl',
            '[class*="ttl"]',
            '[class*="title"]',
            'h1', 'h2', 'h3'
        ]
        
        for selector in selectors_to_try:
            title_element = item.select_one(selector)
            if title_element:
                title_text = title_element.get_text(strip=True)
                if title_text and len(title_text) > 5:
                    return title_text
        
        return f"M&Aæ¡ˆä»¶_{deal_id}"

    @staticmethod
    def _extract_ma_capital_link_flexible(item: Tag, deal_id: str) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®ãƒªãƒ³ã‚¯ã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        selectors_to_try = [
            'a.c-cta.c-button--arrow',
            'a[class*="cta"]',
            'a[class*="button"]',
            'a[href*="deal"]',
            'a'
        ]
        
        for selector in selectors_to_try:
            link_element = item.select_one(selector)
            if link_element:
                href = link_element.get('href', '')
                if href:
                    link = href if href.startswith('http') else f"https://www.ma-cp.com{href}"
                    return link
        
        return f"https://www.ma-cp.com/deal/{deal_id}/"

    @staticmethod
    def _extract_ma_capital_dl_data_flexible(item: Tag, field_names: List[str]) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®dlè¦ç´ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªdlæ§‹é€ 
        dl_elements = item.select('dl.c-filter-project__dataList')
        for dl in dl_elements:
            dt = dl.select_one('dt')
            dd = dl.select_one('dd')
            
            if dt and dd:
                dt_text = dt.get_text(strip=True)
                for field_name in field_names:
                    if field_name in dt_text:
                        return dd.get_text(strip=True)
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: å…¨ã¦ã®dlè¦ç´ ã‚’æ¤œç´¢
        all_dl_elements = item.select('dl')
        for dl in all_dl_elements:
            dt = dl.select_one('dt')
            dd = dl.select_one('dd')
            
            if dt and dd:
                dt_text = dt.get_text(strip=True)
                for field_name in field_names:
                    if field_name in dt_text:
                        return dd.get_text(strip=True)
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æŠ½å‡º
        item_text = item.get_text()
        for field_name in field_names:
            pattern = rf'{field_name}[ï¼š:\s]*([^\n]+)'
            match = re.search(pattern, item_text)
            if match:
                return match.group(1).strip()
        
        return ""

    @staticmethod
    def _extract_ma_capital_business_content_flexible(item: Tag) -> str:
        """M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚ºã®äº‹æ¥­å†…å®¹ã‚’æŸ”è»Ÿã«æŠ½å‡º"""
        features_sections = []
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªãƒ–ãƒ­ãƒƒã‚¯æ§‹é€ 
        selectors_to_try = [
            'div.c-filter-project__listBlock',
            'div[class*="listBlock"]',
            'div[class*="content"]',
            'div[class*="detail"]'
        ]
        
        business_blocks = []
        for selector in selectors_to_try:
            business_blocks = item.select(selector)
            if business_blocks:
                break
        
        for block in business_blocks:
            # ãƒ©ãƒ™ãƒ«ã‚’ç¢ºèª
            label_selectors = [
                'h5.c-filter-project__label',
                'h5[class*="label"]',
                'h4', 'h5', 'h6',
                '[class*="label"]'
            ]
            
            label_element = None
            for label_selector in label_selectors:
                label_element = block.select_one(label_selector)
                if label_element:
                    break
            
            if not label_element:
                continue
            
            label_text = label_element.get_text(strip=True)
            
            # äº‹æ¥­å†…å®¹ã€äº‹æ¥­æ¦‚è¦ã€ç‰¹å¾´ãªã©ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†
            if any(keyword in label_text for keyword in ['äº‹æ¥­å†…å®¹', 'äº‹æ¥­æ¦‚è¦', 'ç‰¹å¾´', 'æ¦‚è¦']):
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                content_selectors = [
                    'div.c-filter-project__lists',
                    'div[class*="lists"]',
                    'div[class*="content"]',
                    'p', 'div'
                ]
                
                lists_element = None
                for content_selector in content_selectors:
                    lists_element = block.select_one(content_selector)
                    if lists_element:
                        break
                
                if lists_element:
                    # HTMLã®<br>ã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                    html_content = str(lists_element)
                    content_with_breaks = re.sub(r'<br\s*/?>', '\n', html_content)
                    clean_soup = BeautifulSoup(content_with_breaks, 'html.parser')
                    raw_content = clean_soup.get_text()
                    
                    if raw_content:
                        # æœ€çµ‚ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’é©ç”¨
                        cleaned_content = UniversalParser._clean_extracted_text_flexible(raw_content, label_text)
                        
                        if cleaned_content:
                            features_sections.append(cleaned_content)
        
        return '\n\n'.join(features_sections) if features_sections else ""

    @staticmethod
    def _clean_extracted_text_flexible(raw_text: str, label_text: str) -> str:
        """æŠ½å‡ºå¾Œãƒ†ã‚­ã‚¹ãƒˆã®æŸ”è»Ÿãªã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
        if not raw_text:
            return ""
        
        # 1. å…¨ä½“ã®å…ˆé ­ã¨æœ«å°¾ã®ç©ºç™½ã‚’å®Œå…¨é™¤å»
        cleaned_text = raw_text.strip()
        
        # 2. å„è¡Œã®å…ˆé ­ã¨æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ã€ç©ºè¡Œã‚’é™¤å»
        lines = []
        for line in cleaned_text.split('\n'):
            stripped_line = line.strip()
            if stripped_line:  # ç©ºè¡Œã¯é™¤å¤–
                lines.append(stripped_line)
        
        if not lines:
            return ""
        
        # 3. ç„¡é–¢ä¿‚ãªãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        filtered_lines = []
        
        for line in lines:
            # æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
            unwanted_patterns = [
                r'æ‰€åœ¨åœ°[ï¼š:]', r'æ¥­ç¨®[ï¼š:]', r'å¾“æ¥­å“¡æ•°[ï¼š:]', r'è¨­ç«‹[ï¼š:]', r'è³‡æœ¬é‡‘[ï¼š:]',
                r'å£²ä¸Šé«˜[ï¼š:]', r'å–¶æ¥­åˆ©ç›Š[ï¼š:]', r'å¸Œæœ›é‡‘é¡[ï¼š:]', r'æ¡ˆä»¶No[ï¼š:]'
            ]
            
            skip_line = False
            for pattern in unwanted_patterns:
                if re.search(pattern, line):
                    skip_line = True
                    break
            
            if not skip_line:
                filtered_lines.append(line)
        
        # 4. ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã€ç‰¹å¾´ã€‘ãªã©ï¼‰ã®å‡¦ç†
        final_lines = []
        has_subsections = any('ã€' in line and 'ã€‘' in line for line in filtered_lines)
        
        if has_subsections:
            # ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾ä¿æŒ
            final_lines = filtered_lines
        else:
            # ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸
            if label_text and filtered_lines:
                final_lines = [f"ã€{label_text}ã€‘"] + filtered_lines
        
        # 5. ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ç©ºç™½ã‚’å®Œå…¨å‰Šé™¤
        cleaned_final_lines = []
        for line in final_lines:
            # ãƒãƒ¼ã‚«ãƒ¼å¾Œã®å…¨ã¦ã®ç©ºç™½æ–‡å­—ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã€ã‚¿ãƒ–ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ï¼‰ã‚’å‰Šé™¤
            cleaned_line = re.sub(r'([â—â—‹â—†âœ“â—â—‰â–¼â– ãƒ»â–²â–½â˜†â˜…â€»])\s+', r'\1', line)
            cleaned_final_lines.append(cleaned_line)
        
        # 6. æœ€çµ‚çš„ãªæ•´å½¢
        if cleaned_final_lines:
            result = '\n'.join(cleaned_final_lines)
            
            # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚’å˜ä¸€åŒ–ï¼ˆ3ã¤ä»¥ä¸Šã®æ”¹è¡Œã‚’2ã¤ã«ï¼‰
            result = re.sub(r'\n{3,}', '\n\n', result)
            
            # æœ€çµ‚çš„ãªå‰å¾Œã®ç©ºç™½é™¤å»
            result = result.strip()
            
            return result
        
        return ""
        
    @staticmethod
    def _parse_selector_based(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¨™æº–ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆæŸ”è»Ÿæ€§å‘ä¸Šç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        item_selectors = [
            site_config.get('item_selector', ''),
            'article',
            'div[class*="item"]',
            'li[class*="item"]',
            'div[class*="case"]',
            'div[class*="project"]',
            'tr'
        ]
        
        items = []
        for selector in item_selectors:
            if selector:
                items = soup.select(selector)
                if items:
                    logging.info(f"Found {len(items)} items using selector: {selector}")
                    break
        
        if not items:
            logging.warning("No items found with any selector")
            return []
        
        results = []
        
        for item in items:
            data = {'site_name': site_config['name']}
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
            for jp_key, selector in site_config.get('data_selectors', {}).items():
                en_key = Constants.JAPANESE_TO_ENGLISH_FIELDS.get(jp_key)
                if not en_key:
                    continue
                
                # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
                element = None
                selectors_to_try = [selector]
                
                # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç”Ÿæˆ
                if '[class*=' in selector:
                    # ã‚¯ãƒ©ã‚¹åã®ä¸€éƒ¨ãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦è¡Œ
                    class_part = re.search(r'\[class\*="([^"]+)"\]', selector)
                    if class_part:
                        alt_selector = f'[class*="{class_part.group(1)}"]'
                        selectors_to_try.append(alt_selector)
                
                for try_selector in selectors_to_try:
                    element = item.select_one(try_selector)
                    if element:
                        break
                
                text_content = element.get_text(strip=True) if element else ""
                
                if en_key == Constants.FIELD_LINK and element:
                    href = element.get('href', '')
                    base_url = '/'.join(site_config['url'].split('/')[:3])
                    data[en_key] = href if href.startswith('http') else f"{base_url}{href}"
                elif en_key in [Constants.FIELD_REVENUE, Constants.FIELD_PROFIT, Constants.FIELD_LOCATION, Constants.FIELD_PRICE, Constants.FIELD_FEATURES]:
                    data[f"{en_key}_text"] = text_content
                else:
                    data[en_key] = text_content
            
            # è¿½åŠ ã®DLè¦ç´ å‡¦ç†ï¼ˆM&Aãƒ­ã‚¤ãƒ¤ãƒ«ç”¨ï¼‰
            if site_config['name'] == "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼":
                UniversalParser._extract_dl_elements_flexible(item, data)
                # ç‰¹è‰²ã®è©³ç´°æŠ½å‡º
                enhanced_features = UniversalParser._extract_enhanced_features_flexible(item)
                if enhanced_features and enhanced_features != "-":
                    data['features_text'] = enhanced_features
            
            if data.get('deal_id') and data.get('title') and data.get('link'):
                results.append(RawDealData(**{k: v for k, v in data.items() if k in {f.name for f in fields(RawDealData)}}))
        
        return results

    @staticmethod
    def _parse_masouken_text_based(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aç·åˆç ”ç©¶æ‰€å°‚ç”¨ã®æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆæŸ”è»Ÿæ€§å‘ä¸Šç‰ˆï¼‰"""
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = os.path.join("debug", f"debug_masouken_{timestamp}.html")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚ˆã‚ŠæŸ”è»Ÿãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
        deal_selectors = [
            'li.p-projects-index__item',
            'li[class*="project"]',
            'div[class*="project"]',
            'div[class*="case"]',
            'article',
            'li[class*="item"]',
            'div[class*="item"]'
        ]
        
        deal_items = []
        for selector in deal_selectors:
            deal_items = soup.select(selector)
            if deal_items:
                logging.info(f"Found {len(deal_items)} items using selector: {selector}")
                break
        
        if deal_items:
            for item in deal_items:
                try:
                    # ã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ˆä»¶IDæŠ½å‡º
                    id_patterns = [
                        r'æ¡ˆä»¶ID[ï¼š:\s]*(\d+)',
                        r'æ¡ˆä»¶ç•ªå·[ï¼š:\s]*(\d+)',
                        r'ID[ï¼š:\s]*(\d+)',
                        r'No[ï¼š:\s]*(\d+)'
                    ]
                    
                    deal_id = None
                    item_text = item.get_text()
                    
                    for pattern in id_patterns:
                        match = re.search(pattern, item_text)
                        if match:
                            deal_id = match.group(1)
                            break
                    
                    if not deal_id:
                        continue
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã®æ”¹å–„
                    title_selectors = ['h1', 'h2', 'h3', '.title', '[class*="title"]', '[class*="ttl"]']
                    title = f"M&Aæ¡ˆä»¶_{deal_id}"
                    
                    for selector in title_selectors:
                        title_elem = item.select_one(selector)
                        if title_elem:
                            title_text = title_elem.get_text(strip=True)
                            if title_text and len(title_text) > 5:
                                title = title_text
                                break
                    
                    # è²¡å‹™æƒ…å ±ã®æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
                    revenue_text = UniversalParser._extract_financial_info_flexible(item_text, ['å£²ä¸Šé«˜', 'å£²ä¸Š'])
                    profit_text = UniversalParser._extract_financial_info_flexible(item_text, ['å–¶æ¥­åˆ©ç›Š', 'åˆ©ç›Š'])
                    
                    # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    revenue_value = DataConverter.parse_financial_value(revenue_text)
                    profit_value = DataConverter.parse_financial_value(profit_text)
                    
                    min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                    min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                    
                    if revenue_value < min_revenue or profit_value < min_profit:
                        continue
                    
                    # ãã®ä»–ã®æƒ…å ±
                    location_text = UniversalParser._extract_location_flexible(item_text)
                    price_text = UniversalParser._extract_financial_info_flexible(item_text, ['è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'å¸Œæœ›ä¾¡æ ¼', 'ä¾¡æ ¼'])
                    
                    link = f"https://masouken.com/list/{deal_id}"
                    
                    deal_data = RawDealData(
                        site_name=site_config['name'],
                        deal_id=deal_id,
                        title=title,
                        link=link,
                        location_text=location_text,
                        revenue_text=revenue_text,
                        profit_text=profit_text,
                        price_text=price_text,
                        features_text=""
                    )
                    
                    results.append(deal_data)
                    
                except Exception as e:
                    logging.error(f"Error parsing deal item: {e}")
                    continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚æ”¹å–„
        if not results:
            results = UniversalParser._parse_masouken_text_fallback_improved(site_config, html_content)
        
        logging.info(f"M&Aç·åˆç ”ç©¶æ‰€: Successfully extracted {len(results)} deals")
        return results

    @staticmethod
    def _parse_masouken_text_fallback_improved(site_config: Dict[str, Any], html_content: str) -> List[RawDealData]:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        results = []
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¡ˆä»¶IDãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        content_text = soup.get_text()
        logging.info(f"Total content length: {len(content_text)} characters")
        
        # æ¡ˆä»¶IDãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        deal_id_pattern = r'æ¡ˆä»¶ID[ï¼š:\s]*(\d+)'
        deal_matches = list(re.finditer(deal_id_pattern, content_text))
        logging.info(f"Found {len(deal_matches)} deal ID matches")
        
        if deal_matches:
            for i, match in enumerate(deal_matches):
                try:
                    deal_id = match.group(1)
                    start_pos = match.start()
                    
                    # æ¬¡ã®æ¡ˆä»¶IDã¾ã§ã®ç¯„å›²ã‚’å–å¾—
                    if i + 1 < len(deal_matches):
                        end_pos = deal_matches[i + 1].start()
                        content = content_text[start_pos:end_pos]
                    else:
                        content = content_text[start_pos:start_pos + 2000]  # æœ€å¾Œã®æ¡ˆä»¶ã¯2000æ–‡å­—ã¾ã§
                    
                    logging.info(f"Processing deal ID: {deal_id}, content length: {len(content)}")
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã®æ”¹å–„
                    title = UniversalParser._extract_masouken_title_flexible(content, deal_id)
                    
                    # æ‰€åœ¨åœ°æŠ½å‡ºã®æ”¹å–„
                    location_text = UniversalParser._extract_location_flexible(content)
                    
                    # è²¡å‹™æƒ…å ±æŠ½å‡º
                    revenue_text = UniversalParser._extract_financial_info_flexible(content, ['å£²ä¸Šé«˜', 'å£²ä¸Š'])
                    profit_text = UniversalParser._extract_financial_info_flexible(content, ['å–¶æ¥­åˆ©ç›Š', 'åˆ©ç›Š'])
                    price_text = UniversalParser._extract_financial_info_flexible(content, ['è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'å¸Œæœ›ä¾¡æ ¼'])
                    
                    # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    revenue_value = DataConverter.parse_financial_value(revenue_text)
                    profit_value = DataConverter.parse_financial_value(profit_text)
                    
                    min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                    min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                    
                    if revenue_value < min_revenue or profit_value < min_profit:
                        logging.info(f"Skipping deal {deal_id}: doesn't meet financial criteria")
                        continue
                    
                    # ãƒªãƒ³ã‚¯ç”Ÿæˆ
                    link = f"https://masouken.com/list/{deal_id}"
                    
                    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¡ä»¶ï¼‰
                    if deal_id and title:
                        deal_data = RawDealData(
                            site_name=site_config['name'],
                            deal_id=deal_id,
                            title=title,
                            link=link,
                            location_text=location_text,
                            revenue_text=revenue_text,
                            profit_text=profit_text,
                            price_text=price_text,
                            features_text=""
                        )
                        results.append(deal_data)
                        logging.info(f"Successfully extracted deal: {deal_id} - {title[:50]}")
                    else:
                        logging.debug(f"Skipped incomplete deal: ID={deal_id}, Title={title[:30] if title else 'None'}")
                        
                except Exception as e:
                    logging.error(f"Error parsing deal {i + 1}: {e}")
                    continue
        
        return results

    @staticmethod
    def _extract_financial_info_flexible(text: str, keywords: List[str]) -> str:
        """ã‚ˆã‚ŠæŸ”è»Ÿãªè²¡å‹™æƒ…å ±æŠ½å‡º"""
        for keyword in keywords:
            patterns = [
                rf'{keyword}[ï¼š:\s]*([^\n]+)',
                rf'{keyword}\s*([^\n]+)',
                rf'ãƒ»{keyword}[ï¼š:\s]*([^\n]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    value = match.group(1).strip()
                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    value = re.sub(r'^[ï¼š:\sãƒ»]+', '', value)
                    # æ¬¡ã®é …ç›®ã§åˆ‡ã‚‹
                    for stop_word in ['å–¶æ¥­åˆ©ç›Š', 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'æ‰€åœ¨åœ°', 'æ¥­ç•Œ']:
                        if stop_word in value and stop_word != keyword:
                            value = value.split(stop_word)[0].strip()
                            break
                    if value and len(value) > 0:
                        return value
        return ""

    @staticmethod
    def _extract_location_flexible(text: str) -> str:
        """ã‚ˆã‚ŠæŸ”è»Ÿãªæ‰€åœ¨åœ°æŠ½å‡º"""
        location_keywords = ['æ‰€åœ¨åœ°', 'ã‚¨ãƒªã‚¢', 'åœ°åŸŸ', 'æ‰€åœ¨']
        
        for keyword in location_keywords:
            pattern = rf'{keyword}[ï¼š:\s]*([^\nï¼š]+)'
            match = re.search(pattern, text)
            if match:
                location = match.group(1).strip()
                # éƒ½é“åºœçœŒåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                prefectures = ['åŒ—æµ·é“', 'é’æ£®', 'å²©æ‰‹', 'å®®åŸ', 'ç§‹ç”°', 'å±±å½¢', 'ç¦å³¶', 'èŒ¨åŸ', 'æ ƒæœ¨', 'ç¾¤é¦¬', 'åŸ¼ç‰', 'åƒè‘‰', 'æ±äº¬', 'ç¥å¥ˆå·', 'æ–°æ½Ÿ', 'å¯Œå±±', 'çŸ³å·', 'ç¦äº•', 'å±±æ¢¨', 'é•·é‡', 'å²é˜œ', 'é™å²¡', 'æ„›çŸ¥', 'ä¸‰é‡', 'æ»‹è³€', 'äº¬éƒ½', 'å¤§é˜ª', 'å…µåº«', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'é³¥å–', 'å³¶æ ¹', 'å²¡å±±', 'åºƒå³¶', 'å±±å£', 'å¾³å³¶', 'é¦™å·', 'æ„›åª›', 'é«˜çŸ¥', 'ç¦å²¡', 'ä½è³€', 'é•·å´', 'ç†Šæœ¬', 'å¤§åˆ†', 'å®®å´', 'é¹¿å…å³¶', 'æ²–ç¸„']
                
                for pref in prefectures:
                    if pref in location:
                        return location
        
        return ""

    @staticmethod
    def _extract_masouken_title_flexible(content: str, deal_id: str) -> str:
        """M&Aç·åˆç ”ç©¶æ‰€ã®ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚’æ”¹å–„ï¼ˆæŸ”è»Ÿç‰ˆï¼‰"""
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€ã€‘ã§å›²ã¾ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
        bracket_patterns = [
            r'ã€([^ã€‘]{5,100})ã€‘',
            r'æ¡ˆä»¶ID[ï¼š:\s]*' + deal_id + r'[^\n]*\n[^\n]*ã€([^ã€‘]{5,100})ã€‘',
        ]
        
        exclude_keywords = [
            'æ¡ˆä»¶ID', 'å£²ä¸Šé«˜', 'å–¶æ¥­åˆ©ç›Š', 'è­²æ¸¡å¸Œæœ›ä¾¡æ ¼', 'URL', 'http',
            'ç™¾ä¸‡å††', 'ä¸‡å††', 'æ‰€åœ¨åœ°', 'ã‚¨ãƒªã‚¢', 'åœ°åŸŸ', 'é¸æŠã—ã¦ãã ã•ã„'
        ]
        
        business_keywords = [
            'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½é€ ', 'è²©å£²', 'å·¥äº‹', 'å»ºè¨­', 'æ¥­', 'ä¼šç¤¾', 'ä¼æ¥­',
            'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'åº—èˆ—', 'åºƒå‘Š', 'IT', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³',
            'åŒ–ç²§å“', 'å¡—è£…', 'è¨­è¨ˆ', 'é–‹ç™º', 'é‹å–¶', 'ç®¡ç†', 'å•†ç¤¾', 'è²¿æ˜“', 'è¼¸å…¥',
            'è¼¸å‡º', 'å¸å£²', 'å°å£²', 'åŒ»ç™‚', 'ä»‹è­·', 'æ•™è‚²', 'å­¦ç¿’', 'å¡¾', 'ã‚¹ã‚¯ãƒ¼ãƒ«',
            'ä¸å‹•ç”£', 'å»ºç¯‰', 'åœŸæœ¨', 'é›»æ°—', 'æ©Ÿæ¢°', 'è‡ªå‹•è»Š', 'éƒ¨å“', 'ææ–™',
            'ã‚³ãƒ³ã‚µãƒ«', 'Web', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆ', 'é€šè²©', 'EC', 'é…é€', 'ç‰©æµ'
        ]
        
        for pattern in bracket_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for match in matches:
                title_candidate = match.strip()
                
                # é•·ã•ãƒã‚§ãƒƒã‚¯
                if not (5 <= len(title_candidate) <= 100):
                    continue
                
                # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if any(keyword in title_candidate for keyword in exclude_keywords):
                    continue
                
                # æ•°å­—ã®ã¿ã¯é™¤å¤–
                if title_candidate.isdigit():
                    continue
                
                # äº‹æ¥­é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if any(keyword in title_candidate for keyword in business_keywords):
                    logging.info(f"Found title for deal {deal_id}: {title_candidate}")
                    return f"ã€{title_candidate}ã€‘"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ±ç”¨çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
        logging.warning(f"Could not extract proper title for deal {deal_id}, using fallback")
        return f"M&Aæ¡ˆä»¶_{deal_id}"

    @staticmethod
    def _extract_dl_elements_flexible(item: Tag, data: Dict[str, str]) -> None:
        """DLè¦ç´ ã‹ã‚‰ã®è©³ç´°æƒ…å ±æŠ½å‡ºï¼ˆæŸ”è»Ÿç‰ˆï¼‰"""
        # ã‚ˆã‚Šå¤šãã®dlæ§‹é€ ã‚’è©¦è¡Œ
        dl_selectors = [
            "dl.p-case__dl",
            "dl[class*='case']",
            "div dl",
            "dl"
        ]
        
        dl_tags = []
        for selector in dl_selectors:
            dl_tags = item.select(selector)
            if dl_tags:
                break
        
        for dl in dl_tags:
            dt = dl.find("dt")
            dd = dl.find("dd")
            
            if dt and dd:
                jp_key = dt.get_text(strip=True)
                en_key = Constants.JAPANESE_TO_ENGLISH_FIELDS.get(jp_key)
                
                if en_key and en_key in [Constants.FIELD_REVENUE, Constants.FIELD_PROFIT, 
                                        Constants.FIELD_LOCATION, Constants.FIELD_PRICE]:
                    data[f"{en_key}_text"] = dd.get_text(strip=True)

    @staticmethod
    def _extract_enhanced_features_flexible(item_element: Tag) -> str:
        """M&Aãƒ­ã‚¤ãƒ¤ãƒ«ç”¨ã®æ‹¡å¼µç‰¹è‰²æŠ½å‡ºï¼ˆæŸ”è»Ÿç‰ˆï¼‰"""
        try:
            all_text = item_element.get_text()
            
            # ç‰¹å¾´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            feature_patterns = [
                r'ã€ç‰¹å¾´ãƒ»å¼·ã¿ã€‘(.+?)(?=ã€|â– |â—†|$)',
                r'ã€ç‰¹è‰²ã€‘(.+?)(?=ã€|â– |â—†|$)',
                r'ã€äº‹æ¥­å†…å®¹ã€‘(.+?)(?=ã€|â– |â—†|$)',
                r'ç‰¹å¾´ãƒ»å¼·ã¿[ï¼š:\s]*(.+?)(?=ã€|â– |â—†|$)',
                r'ç‰¹è‰²[ï¼š:\s]*(.+?)(?=ã€|â– |â—†|$)',
                r'äº‹æ¥­å†…å®¹[ï¼š:\s]*(.+?)(?=ã€|â– |â—†|$)',
            ]
            
            for pattern in feature_patterns:
                match = re.search(pattern, all_text, re.DOTALL)
                if match and len(match.group(1).strip()) > 20:
                    feature_content = match.group(1).strip()
                    
                    # ç®‡æ¡æ›¸ããƒãƒ¼ã‚«ãƒ¼ã§æŠ½å‡º
                    bullet_patterns = [
                        (r'âœ“([^âœ“\n]+)', 'âœ“'),
                        (r'ãƒ»([^ãƒ»\n]+)', 'ãƒ»'),
                        (r'â—†([^â—†\n]+)', 'â—†'),
                        (r'â—‹([^â—‹\n]+)', 'â—‹'),
                        (r'â—([^â—\n]+)', 'â—'),
                    ]
                    
                    for bullet_pattern, marker in bullet_patterns:
                        matches = re.findall(bullet_pattern, feature_content)
                        if matches:
                            business_keywords = ['å–å¼•', 'å®Ÿç¸¾', 'æŠ€è¡“', 'å“è³ª', 'é¡§å®¢', 'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½å“', 'å¼·ã¿']
                            extracted_items = []
                            
                            for match in matches:
                                cleaned_item = match.strip()
                                # ã€ä¿®æ­£ç®‡æ‰€ã€‘ãƒãƒ¼ã‚«ãƒ¼å¾Œã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                                cleaned_item = re.sub(r'^[\sã€€]+', '', cleaned_item)
                                if (len(cleaned_item) > 10 and 
                                    any(keyword in cleaned_item for keyword in business_keywords)):
                                    extracted_items.append(f"{marker}{cleaned_item}")
                                    if len(extracted_items) >= 5:
                                        break
                            
                            if extracted_items:
                                return "\n".join(extracted_items)
            
            return "-"
            
        except Exception as e:
            logging.error(f"Error extracting enhanced features: {e}")
            return "-"

class GSheetConnector:
    """Google Sheetsæ¥ç¶šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    def __init__(self, config: Dict):
        self.config = config['google_sheets']
        self.worksheet = self._connect()

    def _connect(self):
        logging.info("Connecting to Google Sheets...")
        try:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
            if not creds_path:
                raise ValueError("GOOGLE_APPLICATION_CREDENTIALS environment variable is not set.")

            scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
            creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
            client = gspread.authorize(creds)
            spreadsheet = client.open_by_key(self.config['spreadsheet_id'])
            worksheet = spreadsheet.worksheet(self.config['sheet_name'])
            logging.info("âœ… Successfully connected to Google Sheets.")
            return worksheet
        except Exception as e:
            logging.critical(f"âŒ Google Sheets connection error: {e}")
            return None

    def get_existing_ids(self) -> Set[str]:
        """æ—¢å­˜ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’å–å¾—"""
        if not self.worksheet:
            return set()
        logging.info("Fetching existing deal IDs from the sheet...")
        try:
            all_data = self.worksheet.get_all_records()
            if all_data and Constants.FIELD_UNIQUE_ID in all_data[0]:
                ids = {row[Constants.FIELD_UNIQUE_ID] for row in all_data if row.get(Constants.FIELD_UNIQUE_ID)}
                logging.info(f"Found {len(ids)} existing IDs.")
                return ids
            logging.info("No existing data or 'unique_id' column found.")
            return set()
        except Exception as e:
            logging.error(f"Error fetching existing IDs: {e}")
            return set()

    def write_deals(self, new_deals: List[FormattedDealData]) -> None:
        """æ–°ã—ã„æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿"""
        if not self.worksheet or not new_deals:
            return
        logging.info(f"Writing {len(new_deals)} new deals to the spreadsheet...")
        try:
            all_values = self.worksheet.get_all_values()
            headers = [f.name for f in fields(FormattedDealData)]
            if not all_values:
                self.worksheet.append_row(headers, value_input_option='USER_ENTERED')
            existing_headers = self.worksheet.row_values(1) if all_values else headers
            rows_to_append = [[getattr(deal, key, '') for key in existing_headers] for deal in new_deals]
            if rows_to_append:
                self.worksheet.append_rows(rows_to_append, value_input_option='USER_ENTERED')
            logging.info(f"âœ… Successfully appended {len(new_deals)} rows.")
        except Exception as e:
            logging.error(f"Error writing to spreadsheet: {e}")

def load_config(file_path: str = 'config.yaml') -> None:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    global CONFIG
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if 'GOOGLE_SHEETS_ID' in os.environ:
            config['google_sheets']['spreadsheet_id'] = os.environ['GOOGLE_SHEETS_ID']
        CONFIG = config
    except Exception as e:
        print(f"âŒ Config file read error: {e}")
        raise

def setup_logging(config: Dict) -> None:
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_config = config.get('logging', {})
    logging.basicConfig(
        level=getattr(logging, log_config.get('level', 'INFO').upper()),
        format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
        handlers=[
            logging.FileHandler(log_config.get('file_name', 'scraping.log'), encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def retry_on_failure(max_retries_key: str = 'max_retries', delay_key: str = 'retry_delay'):
    """ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            scraping_config = CONFIG.get('scraping', {})
            max_retries = scraping_config.get(max_retries_key, 3)
            delay = scraping_config.get(delay_key, 1)
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except httpx.RequestError as e:
                    logging.warning(f"Network error. Retrying {attempt + 1}/{max_retries}: {e}")
                    if attempt == max_retries - 1:
                        logging.error("Max retries reached.")
                        raise
                    time.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator

@retry_on_failure()
def fetch_html(url: str) -> Optional[str]:
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—"""
    timeout = CONFIG.get('scraping', {}).get('timeout', 15)
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        with httpx.Client(timeout=timeout, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            response.raise_for_status()
            return response.text
    except httpx.TimeoutException as e:
        raise httpx.RequestError(f"Timeout occurred: {e}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code >= 500:
            raise httpx.RequestError(f"Server error {e.response.status_code}, retrying...")
        else:
            logging.error(f"HTTP client error (no retry): {e.response.status_code} for url {url}")
            return None
    except Exception as e:
        logging.error(f"Unexpected error fetching {url}: {e}")
        return None

def diagnose_site_structure(site_config: Dict[str, Any], html_content: str) -> None:
    """ã‚µã‚¤ãƒˆæ§‹é€ ã®è¨ºæ–­æ©Ÿèƒ½"""
    soup = BeautifulSoup(html_content, 'lxml')
    site_name = site_config['name']
    
    logging.info(f"ğŸ” Diagnosing {site_name} structure...")
    
    # HTMLã®åŸºæœ¬æƒ…å ±
    logging.info(f"  HTML length: {len(html_content)} characters")
    logging.info(f"  Title: {soup.title.string if soup.title else 'No title'}")
    
    # è¨­å®šã•ã‚ŒãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã®æ¤œè¨¼
    if 'item_selector' in site_config:
        items = soup.select(site_config['item_selector'])
        logging.info(f"  Items found with '{site_config['item_selector']}': {len(items)}")
        
        if len(items) == 0:
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
            alternative_selectors = [
                'article', 'div[class*="item"]', 'li[class*="item"]',
                'div[class*="case"]', 'div[class*="project"]', 
                'tr', 'div[class*="deal"]'
            ]
            
            for alt_selector in alternative_selectors:
                alt_items = soup.select(alt_selector)
                if len(alt_items) > 0:
                    logging.warning(f"  ğŸ”„ Alternative selector '{alt_selector}' found {len(alt_items)} items")
    
    # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®æ¤œå‡º
    error_indicators = ['404', 'error', 'not found', 'blocked', 'forbidden']
    page_text = soup.get_text().lower()
    
    for indicator in error_indicators:
        if indicator in page_text:
            logging.warning(f"  âš ï¸ Possible error page detected: '{indicator}' found in content")

def format_deal_data(raw_deals: List[RawDealData], existing_ids: Set[str]) -> List[FormattedDealData]:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã€æ¡ä»¶ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†"""
    formatted_deals = []
    extraction_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    for raw_deal in raw_deals:
        try:
            unique_id = hashlib.md5(f"{raw_deal.site_name}_{raw_deal.deal_id}".encode()).hexdigest()[:12]
            
            if unique_id in existing_ids:
                logging.info(f"    -> Skipping duplicate deal: {raw_deal.deal_id}")
                continue
            
            # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã¯ä¸€è¦§ãƒšãƒ¼ã‚¸ã§æ—¢ã«ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰
            if raw_deal.site_name not in ["M&Aç·åˆç ”ç©¶æ‰€", "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯"]:
                revenue_value = DataConverter.parse_financial_value(raw_deal.revenue_text)
                profit_value = DataConverter.parse_financial_value(raw_deal.profit_text)
                
                min_revenue = CONFIG.get('scraping', {}).get('min_revenue', 300000000)
                min_profit = CONFIG.get('scraping', {}).get('min_profit', 30000000)
                
                if revenue_value < min_revenue or profit_value < min_profit:
                    logging.info(f"    -> Skipping deal {raw_deal.deal_id}: doesn't meet financial criteria")
                    continue
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®å£²ä¸Šé«˜ã‚’ç™¾ä¸‡å††å˜ä½ã«å¤‰æ›
            if raw_deal.site_name == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                revenue_formatted = DataConverter.convert_strike_revenue_to_million(raw_deal.revenue_text)
            else:
                revenue_formatted = DataConverter.format_financial_text(raw_deal.revenue_text)
            
            formatted_deal = FormattedDealData(
                extraction_time=extraction_time,
                site_name=raw_deal.site_name,
                deal_id=raw_deal.deal_id,
                title=raw_deal.title,
                location=raw_deal.location_text or "-",
                revenue=revenue_formatted,
                profit=DataConverter.format_financial_text(raw_deal.profit_text),
                price=DataConverter.format_financial_text(raw_deal.price_text),
                features=raw_deal.features_text or "-",
                link=raw_deal.link,
                unique_id=unique_id
            )
            
            formatted_deals.append(formatted_deal)
            logging.info(f"    -> Formatted deal: {raw_deal.deal_id} - {raw_deal.title[:50]}...")
            
        except Exception as e:
            logging.error(f"    -> Error formatting deal {raw_deal.deal_id}: {e}")
            continue
    
    return formatted_deals

def scrape_site(site_config: Dict[str, Any]) -> List[RawDealData]:
    """å„ã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆè¨ºæ–­æ©Ÿèƒ½ä»˜ãï¼‰"""
    if not site_config.get('enabled', False):
        logging.info(f"Site {site_config['name']} is disabled. Skipping.")
        return []
    
    logging.info(f"ğŸ” Starting scraping for: {site_config['name']}")
    all_deals = []
    
    try:
        max_pages = site_config.get('max_pages', 1)
        base_url = site_config['url']
        
        for page_num in range(1, max_pages + 1):
            if site_config.get('pagination', {}).get('type') == 'query_param':
                param = site_config['pagination']['param']
                url = f"{base_url}?{param}={page_num}"
            elif site_config.get('pagination', {}).get('type') == 'path':
                path_template = site_config['pagination']['path']
                url = f"{base_url}{path_template.format(page_num=page_num)}"
            else:
                url = base_url
            
            logging.info(f"  ğŸ“„ Scraping page {page_num}: {url}")
            
            # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã‚µã‚¤ãƒˆã®å‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œ
            if site_config['name'] == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                html_content = scrape_strike_with_dynamic_loading(url)
            else:
                html_content = fetch_html(url)
            
            if not html_content:
                logging.error(f"  âŒ Failed to fetch page {page_num}")
                continue
            
            # è¨ºæ–­å®Ÿè¡Œ
            diagnose_site_structure(site_config, html_content)
            
            # çµ±ä¸€ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨
            deals = UniversalParser.parse_list_page(site_config, html_content)
            all_deals.extend(deals)
            
            # è¨ºæ–­æ©Ÿèƒ½ï¼š1ãƒšãƒ¼ã‚¸ç›®ã§æ¡ˆä»¶ãŒ0ä»¶ã®å ´åˆã¯è­¦å‘Š
            if page_num == 1 and len(deals) == 0:
                logging.critical(f"ğŸš¨ CRITICAL - {site_config['name']}ã®1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰æ¡ˆä»¶ãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logging.critical(f"   ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                logging.critical(f"   config.yamlã®CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
                logging.critical(f"   ç¾åœ¨ã®item_selector: {site_config.get('item_selector')}")
            
            time.sleep(2)
            
            if max_pages == 1:
                break
    
    except Exception as e:
        logging.error(f"âŒ Error scraping {site_config['name']}: {e}")
        logging.debug(traceback.format_exc())
    
    logging.info(f"ğŸ¯ Total deals found from {site_config['name']}: {len(all_deals)}")
    return all_deals

def scrape_strike_with_dynamic_loading(url: str) -> Optional[str]:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã®å‹•çš„èª­ã¿è¾¼ã¿å¯¾å¿œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    try:
        anti_blocking = AntiBlockingManager()
        with WebDriverManager(headless=CONFIG.get('debug', {}).get('headless_mode', True), anti_blocking=anti_blocking) as driver:
            logging.info(f"  ğŸš€ Loading Strike page with dynamic loading support: {url}")
            driver.get(url)
            
            # 52ä»¶ã®æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§30ç§’ï¼‰
            wait = WebDriverWait(driver, 30)
            
            logging.info("  â³ Waiting for deal items to load...")
            
            # æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
            try:
                # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
                selectors_to_wait = [
                    'div.search-result__item',
                    'div[class*="search-result"]',
                    'div[class*="item"]'
                ]
                
                items_found = False
                for selector in selectors_to_wait:
                    try:
                        wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, selector)) >= 10)
                        current_items = driver.find_elements(By.CSS_SELECTOR, selector)
                        logging.info(f"  âœ… {len(current_items)} items loaded with selector: {selector}")
                        items_found = True
                        break
                    except TimeoutException:
                        continue
                
                if not items_found:
                    logging.warning("  âš ï¸ Timeout waiting for items with all selectors")
                    
            except TimeoutException:
                logging.warning(f"  âš ï¸ Timeout waiting for items")
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆJavaScriptã®å®Œå…¨ãªå®Ÿè¡Œå®Œäº†ã‚’ç¢ºä¿ï¼‰
            time.sleep(3)
            
            return driver.page_source
            
    except Exception as e:
        logging.error(f"  âŒ Error in Strike dynamic loading: {e}")
        logging.debug(traceback.format_exc())
        return None

def enhance_deals_with_details(raw_deals: List[RawDealData], site_config: Dict[str, Any]) -> List[RawDealData]:
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰¹è‰²æƒ…å ±ã‚’å–å¾—ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    
    # ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ååˆ†ãªæƒ…å ±ãŒå–å¾—ã§ãã‚‹ã‚µã‚¤ãƒˆã¯è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—
    skip_detail_sites = ["M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º", "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼"]
    
    if site_config['name'] in skip_detail_sites:
        logging.info(f"  Skipping detail page scraping for {site_config['name']} (using list page features)")
        return raw_deals
    
    if not site_config.get('detail_page_selectors') and site_config['name'] not in ["M&Aç·åˆç ”ç©¶æ‰€", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯", "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼"]:
        logging.info(f"  No detail page selectors configured for {site_config['name']}")
        return raw_deals
    
    logging.info(f"ğŸ”— Fetching details for {len(raw_deals)} deals from {site_config['name']}")
    enhanced_deals = []
    
    try:
        anti_blocking = AntiBlockingManager()
        with WebDriverManager(headless=CONFIG.get('debug', {}).get('headless_mode', True), anti_blocking=anti_blocking) as driver:
            scraper = DetailPageScraper(driver, anti_blocking)
            
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã‚’ãƒªãƒ•ã‚¡ãƒ©ãƒ¼ã¨ã—ã¦è¨­å®š
            referer_url = site_config['url']
            
            for i, deal in enumerate(raw_deals, 1):
                try:
                    logging.info(f"  ğŸ“– Processing deal {i}/{len(raw_deals)}: {deal.deal_id}")
                    
                    # 403ãƒ–ãƒ­ãƒƒã‚¯ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å‡¦ç†ã‚’åœæ­¢
                    if anti_blocking.blocked_detected:
                        logging.warning(f"  ğŸš« Blocked state detected. Skipping remaining {len(raw_deals) - i + 1} deals.")
                        # æ®‹ã‚Šã®æ¡ˆä»¶ã‚‚ãã®ã¾ã¾è¿½åŠ ï¼ˆè©³ç´°æƒ…å ±ãªã—ï¼‰
                        enhanced_deals.extend(raw_deals[i-1:])
                        break
                    
                    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®è©³ç´°ãƒšãƒ¼ã‚¸ã§è¿½åŠ æƒ…å ±ã‚’å–å¾—
                    elif site_config['name'] == "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯":
                        enhanced_deal = enhance_strike_deal_with_details_protected(deal, scraper, anti_blocking, referer_url)
                        enhanced_deals.append(enhanced_deal)
                    else:
                        # ä»–ã®ã‚µã‚¤ãƒˆã®å‡¦ç†ï¼ˆ403å¯¾ç­–ä»˜ãï¼‰
                        features = scraper.fetch_features_with_blocking_protection(
                            deal.link, 
                            site_config.get('detail_page_selectors', {}),
                            referer_url
                        )
                        
                        if features and features != "-":
                            if deal.features_text:
                                deal.features_text = f"{deal.features_text}\n{features}"
                            else:
                                deal.features_text = features
                        
                        enhanced_deals.append(deal)
                    
                    # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
                    if site_config['name'] in ["ã‚¹ãƒˆãƒ©ã‚¤ã‚¯", "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼"]:
                        delay = anti_blocking.get_human_like_delay(4, 6)
                    else:
                        delay = anti_blocking.get_human_like_delay(2, 4)
                    
                    logging.info(f"    -> Waiting {delay:.1f} seconds before next request...")
                    time.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ Error processing deal {deal.deal_id}: {e}")
                    enhanced_deals.append(deal)
                    continue
    
    except Exception as e:
        logging.error(f"âŒ Error initializing WebDriver: {e}")
        return raw_deals
    
    logging.info(f"âœ… Enhanced {len(enhanced_deals)} deals with detail information")
    return enhanced_deals

def enhance_maroyal_deal_with_features(deal: RawDealData, scraper: DetailPageScraper, 
    anti_blocking: AntiBlockingManager, referer_url: str) -> RawDealData:
    """M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼ã®æ¡ˆä»¶ã«ç‰¹å¾´ãƒ»å¼·ã¿æƒ…å ±ã‚’è¿½åŠ """
    try:
        logging.info(f"    ğŸ” Fetching features from M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼ detail page: {deal.link}")
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ã®ç‰¹è‰²ã‚’å–å¾—
        features_text = scraper.fetch_features_with_blocking_protection(
            deal.link, 
            {},  # M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã¯ç‹¬è‡ªã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            referer_url
        )
        
        if features_text and features_text != "-":
            deal.features_text = features_text
            logging.info(f"    âœ… Extracted features: {features_text[:100]}...")
        else:
            logging.warning(f"    âš ï¸ No features found for deal {deal.deal_id}")
            
    except Exception as e:
        logging.error(f"    âŒ Error fetching features for deal {deal.deal_id}: {e}")
    
    return deal

def enhance_strike_deal_with_details_protected(deal: RawDealData, scraper: DetailPageScraper, anti_blocking: AntiBlockingManager, referer_url: str) -> RawDealData:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—ï¼ˆ403å¯¾ç­–å¼·åŒ–ç‰ˆï¼‰"""
    try:
        logging.info(f"    -> Accessing Strike detail page: {deal.link}")
        
        # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
        delay = anti_blocking.get_human_like_delay(3, 8)
        logging.info(f"    -> Human-like delay: {delay:.1f} seconds...")
        time.sleep(delay)
        
        # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        scraper.driver.get(deal.link)
        time.sleep(3)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
        
        html_content = scraper.driver.page_source
        
        # 403ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
        if anti_blocking.is_blocked_response(html_content):
            logging.warning(f"    -> ğŸš« 403 BLOCK DETECTED for deal: {deal.deal_id}")
            
            if not anti_blocking.blocked_detected:
                anti_blocking.blocked_detected = True
                logging.warning("    -> First Strike block detected. Attempting recovery...")
                
                # å›å¾©å¾…æ©Ÿæ™‚é–“ï¼ˆã‚¹ãƒˆãƒ©ã‚¤ã‚¯å°‚ç”¨ã§ã‚ˆã‚Šé•·ãï¼‰
                recovery_delay = anti_blocking.get_recovery_delay()
                logging.info(f"    -> Recovery wait: {recovery_delay:.1f} seconds...")
                time.sleep(recovery_delay)
                
                # ãƒªãƒˆãƒ©ã‚¤
                logging.info("    -> Retrying Strike access...")
                scraper.driver.get(deal.link)
                time.sleep(5)  # ã‚ˆã‚Šé•·ã„èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                
                retry_html = scraper.driver.page_source
                
                if anti_blocking.is_blocked_response(retry_html):
                    logging.error("    -> âŒ Still blocked after retry. Aborting Strike detail scraping.")
                    anti_blocking.blocked_detected = True
                    return deal
                else:
                    logging.info("    -> âœ… Strike recovery successful!")
                    html_content = retry_html
                    anti_blocking.blocked_detected = False
            else:
                logging.error("    -> âŒ Already in blocked state. Skipping Strike deal.")
                return deal
        
        detail_soup = BeautifulSoup(html_content, 'lxml')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
        title = extract_strike_title_enhanced(detail_soup, deal.deal_id)
        if title and title != f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal.deal_id}":
            deal.title = title
        
        # æ‰€åœ¨åœ°ã®å–å¾—
        location = extract_strike_location_enhanced(detail_soup)
        if location:
            deal.location_text = location
        
        # ç‰¹è‰²ã®å–å¾—
        features = scraper._fetch_strike_features_enhanced(detail_soup, deal.link)
        if features and features != "-":
            deal.features_text = features
        
        logging.info(f"    -> Enhanced Strike deal: {deal.deal_id} - {deal.title[:50]}")
        return deal
        
    except Exception as e:
        logging.error(f"    -> Error enhancing Strike deal {deal.deal_id}: {e}")
        return deal

def extract_strike_title_enhanced(detail_soup: BeautifulSoup, deal_id: str) -> str:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: æ¨™æº–çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
    title_selectors = [
        'h2.section-ttl span',
        'h2.section-ttl',
        'h1.section-ttl span',
        'h1.section-ttl',
        '.section-ttl span',
        '.section-ttl'
    ]
    
    for selector in title_selectors:
        title_element = detail_soup.select_one(selector)
        if title_element:
            title_text = title_element.get_text(strip=True)
            if title_text and len(title_text) > 5 and title_text != deal_id:
                logging.info(f"    -> Found title via selector {selector}: {title_text[:30]}...")
                return title_text
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: h1, h2ã‚¿ã‚°ã®å…¨æ¢ç´¢
    for header_tag in ['h1', 'h2', 'h3']:
        headers = detail_soup.find_all(header_tag)
        for header in headers:
            header_text = header.get_text(strip=True)
            # æ¡ˆä»¶IDã‚„ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’é™¤å¤–
            exclude_keywords = [
                'æ¡ˆä»¶æ¤œç´¢', 'ãƒ­ã‚°ã‚¤ãƒ³', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒŠãƒ“', 'ãƒˆãƒƒãƒ—',
                'ä¼šå“¡ç™»éŒ²', 'è©³ç´°æ¤œç´¢', 'STRIKE', deal_id, 'SS', 'å£²ä¸Šé«˜'
            ]
            
            if (header_text and len(header_text) > 5 and len(header_text) < 100 and
                not any(keyword in header_text for keyword in exclude_keywords)):
                
                # äº‹æ¥­é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                business_keywords = [
                    'äº‹æ¥­', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½é€ ', 'è²©å£²', 'å·¥äº‹', 'å»ºè¨­', 'æ¥­', 'ä¼šç¤¾',
                    'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'åº—èˆ—', 'åºƒå‘Š', 'IT', 'ã‚·ã‚¹ãƒ†ãƒ ',
                    'åŒ–ç²§å“', 'å¡—è£…', 'è¨­è¨ˆ', 'é–‹ç™º', 'é‹å–¶', 'ç®¡ç†', 'å•†ç¤¾'
                ]
                
                if any(keyword in header_text for keyword in business_keywords):
                    logging.info(f"    -> Found title via header search: {header_text[:30]}...")
                    return header_text
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒˆãƒ«
    logging.warning(f"    -> Could not extract title for {deal_id}, using default")
    return f"ã‚¹ãƒˆãƒ©ã‚¤ã‚¯æ¡ˆä»¶_{deal_id}"

def extract_strike_location_enhanced(detail_soup: BeautifulSoup) -> str:
    """ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®æ‰€åœ¨åœ°æŠ½å‡ºï¼ˆå®Œå…¨å¯¾å¿œç‰ˆï¼‰"""
    
    # ã‚¹ãƒˆãƒ©ã‚¤ã‚¯ã®åœ°æ–¹è¡¨è¨˜ãƒ‘ã‚¿ãƒ¼ãƒ³
    valid_locations = [
        "æ±åŒ—åœ°æ–¹", "é–¢æ±åœ°æ–¹", "ä¸­éƒ¨ãƒ»åŒ—é™¸åœ°æ–¹", "é–¢è¥¿åœ°æ–¹", 
        "ä¸­å›½åœ°æ–¹", "å››å›½åœ°æ–¹", "ä¹å·ãƒ»æ²–ç¸„åœ°æ–¹", "æ±æ—¥æœ¬", "è¥¿æ—¥æœ¬", "æµ·å¤–"
    ]
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: ul.detail__listå†…ã®æ‰€åœ¨åœ°ãƒ©ãƒ™ãƒ«ã‚’æ¢ã™
    detail_lists = detail_soup.find_all('ul', class_='detail__list')
    
    for ul in detail_lists:
        items = ul.find_all('li')
        
        for item in items:
            # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æ‰€åœ¨åœ°æƒ…å ±ã‚’æŠ½å‡º
            text_content = item.get_text(strip=True)
            
            # ã€Œæ‰€åœ¨åœ°ã€ã§å§‹ã¾ã‚‹è¡Œã‚’æ¢ã™
            if 'æ‰€åœ¨åœ°' in text_content:
                # æœ‰åŠ¹ãªåœ°æ–¹è¡¨è¨˜ã‹ãƒã‚§ãƒƒã‚¯
                for valid_location in valid_locations:
                    if valid_location in text_content:
                        logging.info(f"    -> Found location via detail list: {valid_location}")
                        return valid_location
    
    # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰åœ°æ–¹è¡¨è¨˜ã‚’æ¢ã™
    full_text = detail_soup.get_text()
    
    for valid_location in valid_locations:
        if valid_location in full_text:
            # æ‰€åœ¨åœ°ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯
            location_pos = full_text.find(valid_location)
            surrounding = full_text[max(0, location_pos-50):location_pos+50]
            
            if 'æ‰€åœ¨åœ°' in surrounding or 'ã‚¨ãƒªã‚¢' in surrounding or 'åœ°åŸŸ' in surrounding:
                logging.info(f"    -> Found location via full text search: {valid_location}")
                return valid_location
    
    logging.warning("    -> Could not extract location")
    return ""

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆè¨ºæ–­æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        load_config()
        setup_logging(CONFIG)
        
        logging.info("ğŸš€ Starting M&A deal scraping with diagnostics and anti-blocking measures")
        logging.info(f"ğŸ“Š Target criteria: Revenue â‰¥ {CONFIG.get('scraping', {}).get('min_revenue', 300000000):,} yen, Profit â‰¥ {CONFIG.get('scraping', {}).get('min_profit', 30000000):,} yen")
        
        sheet_connector = GSheetConnector(CONFIG)
        if not sheet_connector.worksheet:
            logging.critical("âŒ Cannot proceed without Google Sheets connection")
            return
        
        existing_ids = sheet_connector.get_existing_ids()
        logging.info(f"ğŸ“‹ Found {len(existing_ids)} existing deals in spreadsheet")
        
        all_new_deals = []
        target_sites = ["M&Aç·åˆç ”ç©¶æ‰€", "M&Aã‚­ãƒ£ãƒ”ã‚¿ãƒ«ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚º", "M&Aãƒ­ã‚¤ãƒ¤ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ¼", "ã‚¹ãƒˆãƒ©ã‚¤ã‚¯"]
        enabled_sites = [site for site in CONFIG['sites'] 
                        if site.get('enabled', False) and site['name'] in target_sites]
        
        for site_config in enabled_sites:
            try:
                logging.info(f"ğŸ” Processing {site_config['name']}")
                
                raw_deals = scrape_site(site_config)
                
                if not raw_deals:
                    logging.warning(f"âš ï¸ {site_config['name']}: No deals extracted")
                    continue
                
                enhanced_deals = enhance_deals_with_details(raw_deals, site_config)
                formatted_deals = format_deal_data(enhanced_deals, existing_ids)
                
                logging.info(f"âœ… {site_config['name']}: {len(formatted_deals)} new deals after filtering")
                all_new_deals.extend(formatted_deals)
                
            except Exception as e:
                logging.error(f"âŒ Failed to process {site_config['name']}: {e}")
                continue
        
        if all_new_deals:
            sheet_connector.write_deals(all_new_deals)
            logging.info(f"ğŸ‰ Successfully added {len(all_new_deals)} new deals to spreadsheet")
        else:
            logging.warning("ğŸ“ No new deals found across all sites")
        
        logging.info("âœ¨ Scraping process completed successfully with diagnostics and anti-blocking measures")
        
    except Exception as e:
        logging.critical(f"ğŸ’¥ Critical error in main process: {e}")
        logging.debug(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()