# main3.py - ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰
import httpx
from bs4 import BeautifulSoup, Tag
import gspread
from google.oauth2.service_account import Credentials
import datetime
import hashlib
import traceback
import logging
import yaml
import time
import os
import re
import random
from functools import wraps
from typing import Optional, Dict, List, Set, Any
from dataclasses import dataclass, fields
from enum import Enum

# Seleniumé–¢é€£
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
CONFIG: Dict[str, Any] = {}

# --- å®šæ•°ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ ---
class Constants:
    FIELD_EXTRACTION_TIME = "extraction_time"
    FIELD_SITE_NAME = "site_name"
    FIELD_DEAL_ID = "deal_id"
    FIELD_TITLE = "title"
    FIELD_LOCATION = "location"
    FIELD_REVENUE = "revenue"
    FIELD_PROFIT = "profit"
    FIELD_PRICE = "price"
    FIELD_FEATURES = "features"
    FIELD_LINK = "link"
    FIELD_UNIQUE_ID = "unique_id"

class ScrapingStatus(Enum):
    SUCCESS = "success"
    FAILED = "failed"
    BLOCKED = "blocked"

@dataclass
class RawDealData:
    site_name: str
    title: str
    deal_id: str
    link: str
    revenue_text: str = ""
    profit_text: str = ""
    location_text: str = ""
    price_text: str = ""
    features_text: str = ""

@dataclass
class FormattedDealData:
    extraction_time: str
    site_name: str
    deal_id: str
    title: str
    features: str
    location: str
    revenue: str
    profit: str
    price: str
    link: str
    unique_id: str

# --- ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¯ãƒ©ã‚¹ ---
class SpeedMADataConverter:
    @staticmethod
    def parse_financial_value(text: str) -> int:
        """ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aã®è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã«å¤‰æ›"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "**"]):
            return 0
        
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›ã€ã‚«ãƒ³ãƒé™¤å»
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')).replace(',', '')
        
        # ãƒ¬ãƒ³ã‚¸è¡¨è¨˜ã®å ´åˆã¯ä¸‹é™å€¤ã‚’å–å¾—
        if any(separator in text for separator in ['ã€œ', 'ï½', '-', '?']):
            for separator in ['ã€œ', 'ï½', '-', '?']:
                if separator in text:
                    parts = text.split(separator)
                    if len(parts) >= 1:
                        text = parts[0].strip()
                    break
        
        # æ•°å€¤ã¨ãƒãƒ«ãƒãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼ã‚’æŠ½å‡º
        match = re.search(r'([\d\.]+)', text)
        if not match:
            return 0
        
        try:
            value = float(match.group(1))
        except ValueError:
            return 0
        
        # å˜ä½ã«ã‚ˆã‚‹ä¹—ç®—
        multipliers = {
            'å„„å††': 100_000_000,
            'å„„': 100_000_000,
            'åƒä¸‡å††': 10_000_000,
            'åƒä¸‡': 10_000_000,
            'ç™¾ä¸‡å††': 1_000_000,
            'ç™¾ä¸‡': 1_000_000,
            'ä¸‡å††': 10_000,
            'ä¸‡': 10_000
        }
        
        for unit, multiplier in multipliers.items():
            if unit in text:
                value *= multiplier
                break
        
        return int(value)
    
    @staticmethod
    def format_to_million_yen(text: str) -> str:
        """è²¡å‹™ãƒ†ã‚­ã‚¹ãƒˆã‚’ç™¾ä¸‡å††å˜ä½ã«çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not text or any(keyword in text for keyword in ["éå…¬é–‹", "å¿œç›¸è«‡", "èµ¤å­—", "N/A", "å¸Œæœ›ãªã—", "**"]):
            return text or "N/A"
        
        def _convert_to_million(text_part: str) -> str:
            clean_text = text_part.replace(',', '').strip()
            match = re.search(r'([\d\.]+)', clean_text)
            if not match:
                return text_part
            
            try:
                value = float(match.group(1))
            except ValueError:
                return text_part
            
            million_value = 0
            if 'å„„' in text_part:
                million_value = value * 100
            elif 'åƒä¸‡' in text_part:
                million_value = value * 10
            elif 'ç™¾ä¸‡' in text_part:
                million_value = value
            elif 'ä¸‡' in text_part and 'åƒä¸‡' not in text_part and 'ç™¾ä¸‡' not in text_part:
                million_value = value / 100
            else:
                # å˜ä½ãŒæ˜ç¤ºã•ã‚Œã¦ã„ãªã„å ´åˆã€å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                return text_part
            
            if million_value >= 1:
                return f"{int(million_value):,}ç™¾ä¸‡å††" if million_value == int(million_value) else f"{million_value:.1f}ç™¾ä¸‡å††"
            else:
                return text_part
        
        # ãƒ¬ãƒ³ã‚¸è¡¨è¨˜ã®å‡¦ç†
        range_separators = ['ã€œ', 'ï½', '-', '?']
        for separator in range_separators:
            if separator in text:
                parts = text.split(separator)
                if len(parts) == 2:
                    lower = _convert_to_million(parts[0].strip())
                    upper = _convert_to_million(parts[1].strip())
                    return f"{lower}ï½{upper}"
                break
        
        return _convert_to_million(text)

# --- ã‚¢ãƒ³ãƒãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ç®¡ç†ã‚¯ãƒ©ã‚¹ ---
class AntiBlockingManager:
    def __init__(self):
        self.blocked_detected = False
        self.retry_count = 0
        self.max_retries = 1
        
    def get_human_like_delay(self, base_min: int = 3, base_max: int = 8) -> float:
        return random.uniform(base_min, base_max)
    
    def get_recovery_delay(self) -> float:
        return random.uniform(15, 30)
    
    def is_blocked_response(self, html_content: str) -> bool:
        if not html_content:
            return False
            
        blocked_indicators = [
            "403 ERROR", "The request could not be satisfied",
            "Request blocked", "cloudfront", "Access Denied", "Forbidden"
        ]
        
        content_lower = html_content.lower()
        
        for indicator in blocked_indicators:
            if indicator.lower() in content_lower:
                return True
        
        soup = BeautifulSoup(html_content, 'lxml')
        title_tag = soup.find('title')
        if title_tag:
            title_text = title_tag.get_text().lower()
            if 'error' in title_text or 'blocked' in title_text or 'denied' in title_text:
                return True
        
        return False

# --- WebDriverç®¡ç†ã‚¯ãƒ©ã‚¹ ---
class WebDriverManager:
    def __init__(self, headless: bool = True, anti_blocking: AntiBlockingManager = None):
        self.headless = headless
        self.driver: Optional[webdriver.Chrome] = None
        self.anti_blocking = anti_blocking or AntiBlockingManager()

    def __enter__(self) -> webdriver.Chrome:
        logging.info("Initializing Selenium WebDriver for SpeedM&A...")
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920x1080")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        chrome_options.add_argument(f"--user-agent={user_agent}")
        
        try:
            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            logging.info("âœ… WebDriver initialized successfully.")
            return self.driver
        except Exception as e:
            logging.error(f"Failed to initialize WebDriver: {e}")
            raise

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            try:
                self.driver.quit()
                logging.info("WebDriver has been closed.")
            except Exception as e:
                logging.error(f"Error closing WebDriver: {e}")

# --- ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aå°‚ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¯ãƒ©ã‚¹ ---
class SpeedMAParser:
    @staticmethod
    def parse_list_page(html_content: str) -> List[RawDealData]:
        """ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aã®ä¸€è¦§ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚µãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        soup = BeautifulSoup(html_content, 'lxml')
        results = []
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if CONFIG.get('debug', {}).get('save_html_files', False):
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            debug_file = os.path.join("debug", f"debug_speedma_list_{timestamp}.html")
            os.makedirs("debug", exist_ok=True)
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logging.info(f"Debug: HTML saved to {debug_file}")
        
        # ä¿®æ­£ï¼šæ­£ã—ã„æ¡ˆä»¶ã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨
        # å®Ÿéš›ã®HTMLã«åˆã‚ã›ã¦ã‚»ãƒ¬ã‚¯ã‚¿ã‚’èª¿æ•´
        items = soup.select('a.swiper-slide.p_card')  # ä¿®æ­£ã•ã‚ŒãŸã‚»ãƒ¬ã‚¯ã‚¿
        
        if not items:
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
            items = soup.select('a[href*="/projects/"]')
            logging.info(f"Alternative selector found {len(items)} items")
        
        logging.info(f"Found {len(items)} deal items on the page")
        
        for i, item in enumerate(items):
            try:
                logging.info(f"Processing item {i+1}/{len(items)}")
                
                # ãƒªãƒ³ã‚¯ã®æŠ½å‡º
                link = item.get('href', '')
                if not link:
                    logging.warning(f"No link found in item {i+1}")
                    continue
                
                if not link.startswith('http'):
                    link = f"https://speed-ma.com{link}"
                
                # æ¡ˆä»¶IDã®æŠ½å‡ºï¼ˆURLã‹ã‚‰ï¼‰
                deal_id_match = re.search(r'/projects/(\d+)', link)
                if not deal_id_match:
                    logging.warning(f"No deal ID found in URL: {link}")
                    continue
                
                deal_id = deal_id_match.group(1)
                logging.info(f"Found deal ID: {deal_id}")
                
                # å£²ä¸Šé«˜ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ï¼ˆä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
                revenue_elem = item.select_one('.p_sales div')
                if revenue_elem:
                    revenue_text = revenue_elem.get_text(strip=True)
                    revenue_value = SpeedMADataConverter.parse_financial_value(revenue_text)
                    min_revenue = CONFIG.get('speed_ma', {}).get('revenue_threshold', 300000000)
                    
                    logging.info(f"Deal {deal_id}: Revenue from list = {revenue_text} ({revenue_value:,})")
                    
                    # å£²ä¸Šé«˜ãŒåŸºæº–ã‚’æº€ãŸã•ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if revenue_value < min_revenue:
                        logging.info(f"Skipping deal {deal_id}: Revenue {revenue_value:,} < {min_revenue:,}")
                        continue
                else:
                    logging.warning(f"No revenue info found in list for deal {deal_id}")
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
                title_elem = item.select_one('.pcard__title-title-front')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                else:
                    title = f"ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aæ¡ˆä»¶_{deal_id}"
                
                deal_data = RawDealData(
                    site_name="ã‚¹ãƒ”ãƒ¼ãƒ‰M&A",
                    deal_id=deal_id,
                    title=title,
                    link=link,
                    revenue_text="",
                    profit_text="",
                    location_text="",
                    price_text="",
                    features_text=""
                )
                
                results.append(deal_data)
                logging.info(f"Successfully extracted deal: {deal_id} (passed revenue filter)")
                
            except Exception as e:
                logging.error(f"Error parsing item {i+1}: {e}")
                continue
        
        logging.info(f"ã‚¹ãƒ”ãƒ¼ãƒ‰M&A: Successfully extracted {len(results)} deals from list page (after revenue filtering)")
        return results

# --- è©³ç´°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ ---
class SpeedMADetailScraper:
    def __init__(self, driver: webdriver.Chrome, anti_blocking: AntiBlockingManager):
        self.driver = driver
        self.anti_blocking = anti_blocking

    def enhance_deal_with_details(self, deal: RawDealData) -> RawDealData:
        """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦dealã‚’æ‹¡å¼µï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            logging.info(f"    -> Accessing detail page: {deal.link}")
            
            # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
            delay = self.anti_blocking.get_human_like_delay(2, 5)
            logging.info(f"    -> Waiting {delay:.1f} seconds before access...")
            time.sleep(delay)
            
            # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            self.driver.get(deal.link)
            time.sleep(3)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            
            html_content = self.driver.page_source
            
            # 403ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
            if self.anti_blocking.is_blocked_response(html_content):
                logging.warning(f"    -> ğŸš« 403 BLOCK DETECTED for deal: {deal.deal_id}")
                return deal
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            if CONFIG.get('debug', {}).get('save_html_files', False):
                timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
                debug_file = os.path.join("debug", f"debug_speedma_detail_{deal.deal_id}_{timestamp}.html")
                os.makedirs("debug", exist_ok=True)
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                logging.info(f"Debug: Detail HTML saved to {debug_file}")
            
            detail_soup = BeautifulSoup(html_content, 'lxml')
            
            # å„æƒ…å ±ã‚’æŠ½å‡º
            deal.title = self._extract_title(detail_soup, deal.deal_id)
            deal.location_text = self._extract_location(detail_soup)
            deal.revenue_text = self._extract_revenue(detail_soup)
            deal.profit_text = self._extract_profit(detail_soup)
            deal.price_text = self._extract_price(detail_soup)
            
            logging.info(f"    -> Enhanced deal: {deal.deal_id}")
            logging.info(f"    -> Revenue: {deal.revenue_text}, Profit: {deal.profit_text}")
            
            return deal
            
        except Exception as e:
            logging.error(f"    -> Error enhancing deal {deal.deal_id}: {e}")
            return deal

    def _extract_title(self, soup: BeautifulSoup, deal_id: str) -> str:
        """äº‹æ¥­æ¦‚è¦ã‚’æŠ½å‡ºã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹"""
        try:
            # äº‹æ¥­æ¦‚è¦ã‚’æ¢ã™
            items = soup.select('li.single_project_overviewList__item')
            for item in items:
                title_elem = item.select_one('p.single_project_overviewList__item-title')
                if title_elem and 'äº‹æ¥­æ¦‚è¦' in title_elem.get_text(strip=True):
                    text_elem = item.select_one('div.single_project_overviewList__item-text')
                    if text_elem:
                        # brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                        for br in text_elem.find_all('br'):
                            br.replace_with('\n')
                        
                        title_text = text_elem.get_text(strip=True)
                        if title_text and len(title_text) > 10:
                            # é•·ã™ãã‚‹å ´åˆã¯é©åº¦ã«ãƒˆãƒªãƒŸãƒ³ã‚°
                            if len(title_text) > 200:
                                title_text = title_text[:200] + "..."
                            return title_text
            
            return f"ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aæ¡ˆä»¶_{deal_id}"
            
        except Exception as e:
            logging.error(f"Error extracting title: {e}")
            return f"ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aæ¡ˆä»¶_{deal_id}"

    def _extract_location(self, soup: BeautifulSoup) -> str:
        """åœ°åŸŸã‚’æŠ½å‡º"""
        try:
            items = soup.select('li.single_project_overviewList__item')
            for item in items:
                title_elem = item.select_one('p.single_project_overviewList__item-title')
                if title_elem and 'åœ°åŸŸ' in title_elem.get_text(strip=True):
                    text_elem = item.select_one('div.single_project_overviewList__item-text')
                    if text_elem:
                        return text_elem.get_text(strip=True)
            
            return ""
            
        except Exception as e:
            logging.error(f"Error extracting location: {e}")
            return ""

    def _extract_revenue(self, soup: BeautifulSoup) -> str:
        """å£²ä¸Šé«˜ã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            items = soup.select('li.single_project_overviewList__item')
            for item in items:
                title_elem = item.select_one('p.single_project_overviewList__item-title')
                if title_elem and 'å£²ä¸Šé«˜' in title_elem.get_text(strip=True):
                    text_elem = item.select_one('div.single_project_overviewList__item-text')
                    if text_elem:
                        revenue_text = text_elem.get_text(strip=True)
                        # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦
                        if "**" in revenue_text:
                            logging.warning("Revenue is masked (not logged in)")
                            return ""
                        return revenue_text
            
            return ""
            
        except Exception as e:
            logging.error(f"Error extracting revenue: {e}")
            return ""

    def _extract_profit(self, soup: BeautifulSoup) -> str:
        """å–¶æ¥­åˆ©ç›Šã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            # ã¾ãšã€æ¡ˆä»¶æ¦‚è¦ã‹ã‚‰å–¶æ¥­åˆ©ç›Šã‚’æ¢ã™
            items = soup.select('li.single_project_overviewList__item')
            for item in items:
                title_elem = item.select_one('p.single_project_overviewList__item-title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if 'å–¶æ¥­åˆ©ç›Š' in title_text or 'åˆ©ç›Š' in title_text:
                        text_elem = item.select_one('div.single_project_overviewList__item-text')
                        if text_elem:
                            profit_text = text_elem.get_text(strip=True)
                            if "**" in profit_text:
                                logging.warning("Profit is masked (not logged in)")
                                return ""
                            return profit_text
            
            # è²¡å‹™æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚‚æ¢ã™ï¼ˆãƒ­ã‚°ã‚¤ãƒ³æ™‚ç”¨ï¼‰
            financial_items = soup.select('li.single_project_financialList__item')
            for item in financial_items:
                title_elem = item.select_one('p.single_project_financialList__item-title')
                if title_elem and 'å–¶æ¥­åˆ©ç›Š' in title_elem.get_text(strip=True):
                    text_elem = item.select_one('div.single_project_financialList__item-text')
                    if text_elem:
                        profit_text = text_elem.get_text(strip=True)
                        if "**" in profit_text:
                            logging.warning("Profit is masked (not logged in)")
                            return ""
                        return profit_text
            
            return ""
            
        except Exception as e:
            logging.error(f"Error extracting profit: {e}")
            return ""

    def _extract_price(self, soup: BeautifulSoup) -> str:
        """å¸Œæœ›è­²æ¸¡ä¾¡æ ¼ã‚’æŠ½å‡º"""
        try:
            items = soup.select('li.single_project_overviewList__item')
            for item in items:
                title_elem = item.select_one('p.single_project_overviewList__item-title')
                if title_elem and 'å¸Œæœ›è­²æ¸¡ä¾¡æ ¼' in title_elem.get_text(strip=True):
                    text_elem = item.select_one('div.single_project_overviewList__item-text')
                    if text_elem:
                        return text_elem.get_text(strip=True)
            
            return ""
            
        except Exception as e:
            logging.error(f"Error extracting price: {e}")
            return ""

# --- Google Sheetsæ¥ç¶šã‚¯ãƒ©ã‚¹ ---
class GSheetConnector:
    def __init__(self, config: Dict):
        self.config = config['google_sheets']
        self.worksheet = self._connect()

    def _connect(self):
        logging.info("Connecting to Google Sheets...")
        try:
            creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
            if not creds_path:
                raise ValueError("GOOGLE_APPLICATION_CREDENTIALS environment variable is not set.")

            scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
            creds = Credentials.from_service_account_file(creds_path, scopes=scopes)
            client = gspread.authorize(creds)
            spreadsheet = client.open_by_key(self.config['spreadsheet_id'])
            worksheet = spreadsheet.worksheet(self.config['sheet_name'])
            logging.info("âœ… Successfully connected to Google Sheets.")
            return worksheet
        except Exception as e:
            logging.critical(f"âŒ Google Sheets connection error: {e}")
            return None

    def get_existing_ids(self) -> Set[str]:
        """æ—¢å­˜ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’å–å¾—"""
        if not self.worksheet:
            return set()
        logging.info("Fetching existing deal IDs from the sheet...")
        try:
            all_data = self.worksheet.get_all_records()
            if all_data and Constants.FIELD_UNIQUE_ID in all_data[0]:
                ids = {row[Constants.FIELD_UNIQUE_ID] for row in all_data if row.get(Constants.FIELD_UNIQUE_ID)}
                logging.info(f"Found {len(ids)} existing IDs.")
                return ids
            logging.info("No existing data or 'unique_id' column found.")
            return set()
        except Exception as e:
            logging.error(f"Error fetching existing IDs: {e}")
            return set()

    def write_deals(self, new_deals: List[FormattedDealData]) -> None:
        """æ–°ã—ã„æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿"""
        if not self.worksheet or not new_deals:
            return
        logging.info(f"Writing {len(new_deals)} new deals to the spreadsheet...")
        try:
            all_values = self.worksheet.get_all_values()
            headers = [f.name for f in fields(FormattedDealData)]
            if not all_values:
                self.worksheet.append_row(headers, value_input_option='USER_ENTERED')
            existing_headers = self.worksheet.row_values(1) if all_values else headers
            rows_to_append = [[getattr(deal, key, '') for key in existing_headers] for deal in new_deals]
            if rows_to_append:
                self.worksheet.append_rows(rows_to_append, value_input_option='USER_ENTERED')
            logging.info(f"âœ… Successfully appended {len(new_deals)} rows.")
        except Exception as e:
            logging.error(f"Error writing to spreadsheet: {e}")

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def load_config(file_path: str = 'config.yaml') -> None:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    global CONFIG
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        if 'GOOGLE_SHEETS_ID' in os.environ:
            config['google_sheets']['spreadsheet_id'] = os.environ['GOOGLE_SHEETS_ID']
        CONFIG = config
    except Exception as e:
        print(f"âŒ Config file read error: {e}")
        raise

def setup_logging(config: Dict) -> None:
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_config = config.get('logging', {})
    logging.basicConfig(
        level=getattr(logging, log_config.get('level', 'INFO').upper()),
        format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
        handlers=[
            logging.FileHandler(log_config.get('file_name', 'speedma_scraping.log'), encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def retry_on_failure(max_retries: int = 3, delay: int = 1):
    """ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except httpx.RequestError as e:
                    logging.warning(f"Network error. Retrying {attempt + 1}/{max_retries}: {e}")
                    if attempt == max_retries - 1:
                        logging.error("Max retries reached.")
                        raise
                    time.sleep(delay * (attempt + 1))
            return None
        return wrapper
    return decorator

@retry_on_failure()
def fetch_html(url: str) -> Optional[str]:
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—"""
    timeout = CONFIG.get('scraping', {}).get('timeout', 15)
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        with httpx.Client(timeout=timeout, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            response.raise_for_status()
            return response.text
    except httpx.TimeoutException as e:
        raise httpx.RequestError(f"Timeout occurred: {e}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code >= 500:
            raise httpx.RequestError(f"Server error {e.response.status_code}, retrying...")
        else:
            logging.error(f"HTTP client error (no retry): {e.response.status_code} for url {url}")
            return None
    except Exception as e:
        logging.error(f"Unexpected error fetching {url}: {e}")
        return None

def format_deal_data(raw_deals: List[RawDealData], existing_ids: Set[str]) -> List[FormattedDealData]:
    """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã€æ¡ä»¶ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    formatted_deals = []
    extraction_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # è¨­å®šã‹ã‚‰é–¾å€¤ã‚’å–å¾—
    min_revenue = CONFIG.get('speed_ma', {}).get('revenue_threshold', 300000000)  # 3å„„å††
    min_profit = CONFIG.get('speed_ma', {}).get('profit_threshold', 30000000)     # 3,000ä¸‡å††
    
    for raw_deal in raw_deals:
        try:
            unique_id = hashlib.md5(f"{raw_deal.site_name}_{raw_deal.deal_id}".encode()).hexdigest()[:12]
            
            if unique_id in existing_ids:
                logging.info(f"    -> Skipping duplicate deal: {raw_deal.deal_id}")
                continue
            
            # è²¡å‹™æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            revenue_value = SpeedMADataConverter.parse_financial_value(raw_deal.revenue_text)
            profit_value = SpeedMADataConverter.parse_financial_value(raw_deal.profit_text)
            
            logging.info(f"    -> Deal {raw_deal.deal_id}: Revenue={revenue_value:,}, Profit={profit_value:,}")
            
            # å£²ä¸Šé«˜ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢ã«ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã ãŒå†ç¢ºèªï¼‰
            if revenue_value > 0 and revenue_value < min_revenue:
                logging.info(f"    -> Skipping deal {raw_deal.deal_id}: Revenue {revenue_value:,} < {min_revenue:,}")
                continue
            
            # å–¶æ¥­åˆ©ç›Šãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
            if profit_value > 0 and profit_value < min_profit:
                logging.info(f"    -> Skipping deal {raw_deal.deal_id}: Profit {profit_value:,} < {min_profit:,}")
                continue
            
            # å–¶æ¥­åˆ©ç›Šãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯è­¦å‘Šã ã‘ã—ã¦é€šã™
            if not raw_deal.profit_text or "**" in raw_deal.profit_text:
                logging.warning(f"    -> Deal {raw_deal.deal_id}: Profit data not available (may require login)")
            
            formatted_deal = FormattedDealData(
                extraction_time=extraction_time,
                site_name=raw_deal.site_name,
                deal_id=raw_deal.deal_id,
                title=raw_deal.title,
                location=raw_deal.location_text or "-",
                revenue=SpeedMADataConverter.format_to_million_yen(raw_deal.revenue_text),
                profit=SpeedMADataConverter.format_to_million_yen(raw_deal.profit_text),
                price=SpeedMADataConverter.format_to_million_yen(raw_deal.price_text),
                features=raw_deal.features_text or "-",
                link=raw_deal.link,
                unique_id=unique_id
            )
            
            formatted_deals.append(formatted_deal)
            logging.info(f"    -> Formatted deal: {raw_deal.deal_id} - {raw_deal.title[:50]}...")
            
        except Exception as e:
            logging.error(f"    -> Error formatting deal {raw_deal.deal_id}: {e}")
            continue
    
    return formatted_deals

def scrape_speed_ma() -> List[RawDealData]:
    """ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    logging.info("ğŸ” Starting scraping for: ã‚¹ãƒ”ãƒ¼ãƒ‰M&A")
    all_deals = []
    
    try:
        max_pages = CONFIG.get('speed_ma', {}).get('max_pages', 2)
        base_url = CONFIG.get('speed_ma', {}).get('projects_url', 'https://speed-ma.com/projects')
        
        for page_num in range(1, max_pages + 1):
            if page_num == 1:
                url = base_url
            else:
                url = f"{base_url}?p={page_num}"
            
            logging.info(f"  ğŸ“„ Scraping page {page_num}: {url}")
            
            html_content = fetch_html(url)
            if not html_content:
                logging.error(f"  âŒ Failed to fetch page {page_num}")
                continue
            
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆå£²ä¸Šé«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰
            deals = SpeedMAParser.parse_list_page(html_content)
            all_deals.extend(deals)
            
            # 1ãƒšãƒ¼ã‚¸ç›®ã§æ¡ˆä»¶ãŒ0ä»¶ã®å ´åˆã¯è­¦å‘Š
            if page_num == 1 and len(deals) == 0:
                logging.critical(f"ğŸš¨ CRITICAL - ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aã®1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰æ¡ˆä»¶ãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                logging.critical(f"   ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            time.sleep(2)
    
    except Exception as e:
        logging.error(f"âŒ Error scraping ã‚¹ãƒ”ãƒ¼ãƒ‰M&A: {e}")
        logging.debug(traceback.format_exc())
    
    logging.info(f"ğŸ¯ Total deals found from ã‚¹ãƒ”ãƒ¼ãƒ‰M&A (after revenue filtering): {len(all_deals)}")
    return all_deals

def enhance_deals_with_details(raw_deals: List[RawDealData]) -> List[RawDealData]:
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µ"""
    logging.info(f"ğŸ”— Fetching details for {len(raw_deals)} deals from ã‚¹ãƒ”ãƒ¼ãƒ‰M&A")
    enhanced_deals = []
    
    try:
        anti_blocking = AntiBlockingManager()
        with WebDriverManager(headless=CONFIG.get('debug', {}).get('headless_mode', True), anti_blocking=anti_blocking) as driver:
            scraper = SpeedMADetailScraper(driver, anti_blocking)
            
            for i, deal in enumerate(raw_deals, 1):
                try:
                    logging.info(f"  ğŸ“– Processing deal {i}/{len(raw_deals)}: {deal.deal_id}")
                    
                    # 403ãƒ–ãƒ­ãƒƒã‚¯ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å‡¦ç†ã‚’åœæ­¢
                    if anti_blocking.blocked_detected:
                        logging.warning(f"  ğŸš« Blocked state detected. Skipping remaining {len(raw_deals) - i + 1} deals.")
                        enhanced_deals.extend(raw_deals[i-1:])
                        break
                    
                    enhanced_deal = scraper.enhance_deal_with_details(deal)
                    enhanced_deals.append(enhanced_deal)
                    
                    # äººé–“ã‚‰ã—ã„å¾…æ©Ÿæ™‚é–“
                    delay = anti_blocking.get_human_like_delay(3, 6)
                    logging.info(f"    -> Waiting {delay:.1f} seconds before next request...")
                    time.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ Error processing deal {deal.deal_id}: {e}")
                    enhanced_deals.append(deal)
                    continue
    
    except Exception as e:
        logging.error(f"âŒ Error initializing WebDriver: {e}")
        return raw_deals
    
    logging.info(f"âœ… Enhanced {len(enhanced_deals)} deals with detail information")
    return enhanced_deals

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        load_config()
        setup_logging(CONFIG)
        
        logging.info("ğŸš€ Starting SpeedM&A deal scraping (FIXED VERSION)")
        logging.info(f"ğŸ“Š Target criteria: Revenue â‰¥ {CONFIG.get('speed_ma', {}).get('revenue_threshold', 300000000):,} yen, Profit â‰¥ {CONFIG.get('speed_ma', {}).get('profit_threshold', 30000000):,} yen")
        
        sheet_connector = GSheetConnector(CONFIG)
        if not sheet_connector.worksheet:
            logging.critical("âŒ Cannot proceed without Google Sheets connection")
            return
        
        existing_ids = sheet_connector.get_existing_ids()
        logging.info(f"ğŸ“‹ Found {len(existing_ids)} existing deals in spreadsheet")
        
        # ã‚¹ãƒ”ãƒ¼ãƒ‰M&Aã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå£²ä¸Šé«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ï¼‰
        raw_deals = scrape_speed_ma()
        
        if not raw_deals:
            logging.warning("âš ï¸ ã‚¹ãƒ”ãƒ¼ãƒ‰M&A: No deals extracted (after revenue filtering)")
            return
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        enhanced_deals = enhance_deals_with_details(raw_deals)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€æœ€çµ‚æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        formatted_deals = format_deal_data(enhanced_deals, existing_ids)
        
        logging.info(f"âœ… ã‚¹ãƒ”ãƒ¼ãƒ‰M&A: {len(formatted_deals)} new deals after all filtering")
        
        if formatted_deals:
            sheet_connector.write_deals(formatted_deals)
            logging.info(f"ğŸ‰ Successfully added {len(formatted_deals)} new deals to spreadsheet")
        else:
            logging.warning("ğŸ“ No new deals found that meet all criteria")
        
        logging.info("âœ¨ SpeedM&A scraping process completed successfully")
        
    except Exception as e:
        logging.critical(f"ğŸ’¥ Critical error in main process: {e}")
        logging.debug(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()